{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explainability and Regulation: SHAP and LIME\n",
    "\n",
    "This hands-on lab demonstrates key concepts in machine learning model interpretability and explainability using real-world data. We'll cover:\n",
    "\n",
    "1. **Introduction to Model Interpretability** - Understanding the need for explainable AI\n",
    "2. **SHAP (SHapley Additive exPlanations)** - Theory and practical implementation\n",
    "3. **LIME (Local Interpretable Model-agnostic Explanations)** - Local explanations for individual predictions\n",
    "4. **Regulatory Considerations for AI Models** - Compliance, fairness, and transparency\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We'll use the **Adult Income Dataset** (also known as Census Income dataset) from UCI Machine Learning Repository. This dataset is perfect for demonstrating explainability in a regulatory context as it involves sensitive attributes and predictions that affect people's lives.\n",
    "\n",
    "**Task**: Predict whether a person makes over $50K a year based on census data.\n",
    "\n",
    "**Why this dataset?**\n",
    "- Real-world use case (credit scoring, hiring decisions)\n",
    "- Contains sensitive attributes (age, sex, race, education)\n",
    "- Regulatory implications for fairness and non-discrimination\n",
    "- Interpretable features for clear explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "First, let's import all necessary libraries and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Explainability libraries\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"SHAP version: {shap.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Explore the Adult Income Dataset\n",
    "\n",
    "We'll load the dataset directly from UCI's repository. The dataset contains demographic and employment information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for the Adult dataset\n",
    "column_names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "    'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "\n",
    "# Load data from UCI repository\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "df = pd.read_csv(url, names=column_names, na_values=' ?', skipinitialspace=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Features: {len(df.columns) - 1}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['income'].value_counts())\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Model Interpretability\n",
    "\n",
    "### Why Do We Need Explainable AI?\n",
    "\n",
    "**Model interpretability** refers to the degree to which a human can understand the cause of a decision made by a machine learning model.\n",
    "\n",
    "**Key Reasons:**\n",
    "1. **Trust**: Stakeholders need to trust model predictions\n",
    "2. **Debugging**: Understanding why a model makes certain predictions helps identify errors\n",
    "3. **Regulatory Compliance**: Laws like GDPR, ECOA require explanations for automated decisions\n",
    "4. **Fairness**: Detecting and mitigating bias in model predictions\n",
    "5. **Scientific Understanding**: Learning new insights from model behavior\n",
    "\n",
    "### Types of Interpretability\n",
    "\n",
    "1. **Global Interpretability**: Understanding the overall model behavior\n",
    "   - Which features are most important?\n",
    "   - How do features interact?\n",
    "\n",
    "2. **Local Interpretability**: Understanding individual predictions\n",
    "   - Why did the model make this specific prediction?\n",
    "   - Which features contributed most to this decision?\n",
    "\n",
    "### The Tradeoff: Accuracy vs. Interpretability\n",
    "\n",
    "- **Simple models** (Linear Regression, Decision Trees): Highly interpretable but may lack accuracy\n",
    "- **Complex models** (Random Forests, XGBoost, Neural Networks): High accuracy but \"black box\"\n",
    "- **Solution**: Use explainability tools (SHAP, LIME) to interpret complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before building our model, we need to prepare the data by handling missing values and encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values (simple approach for this demo)\n",
    "df_clean = df.dropna()\n",
    "print(f\"Samples after removing missing values: {len(df_clean):,}\")\n",
    "\n",
    "# Create binary target variable\n",
    "df_clean['income_binary'] = (df_clean['income'] == '>50K').astype(int)\n",
    "\n",
    "# Select features for modeling\n",
    "# We'll use a mix of numerical and categorical features\n",
    "numerical_features = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
    "\n",
    "# Create a copy for modeling\n",
    "df_model = df_clean.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_model[col] = le.fit_transform(df_model[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Prepare feature matrix and target\n",
    "feature_columns = numerical_features + categorical_features\n",
    "X = df_model[feature_columns]\n",
    "y = df_model['income_binary']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nFeatures used: {len(feature_columns)}\")\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Machine Learning Model\n",
    "\n",
    "We'll train a Random Forest classifier - a powerful but \"black box\" model that will benefit from explainability tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Model Performance\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['<=50K', '>50K']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['<=50K', '>50K'], yticklabels=['<=50K', '>50K'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "### What is SHAP?\n",
    "\n",
    "SHAP is based on **Shapley values** from cooperative game theory. It provides a unified measure of feature importance that:\n",
    "\n",
    "- **Is theoretically grounded**: Based on solid mathematical foundations\n",
    "- **Is consistent**: If a model changes so a feature has a larger impact, the importance should increase\n",
    "- **Is locally accurate**: The sum of feature attributions equals the model output\n",
    "- **Works globally and locally**: Provides both instance-level and global explanations\n",
    "\n",
    "### How SHAP Works\n",
    "\n",
    "SHAP values represent the contribution of each feature to the prediction, compared to the average prediction:\n",
    "\n",
    "```\n",
    "prediction = base_value + Σ(SHAP_value_i)\n",
    "```\n",
    "\n",
    "For each feature:\n",
    "- **Positive SHAP value**: Feature pushes prediction higher\n",
    "- **Negative SHAP value**: Feature pushes prediction lower\n",
    "- **Magnitude**: How much the feature contributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer for our Random Forest model\n",
    "print(\"Creating SHAP TreeExplainer...\")\n",
    "print(\"(This may take a moment for large datasets)\")\n",
    "\n",
    "# Use a subset for faster computation in this demo\n",
    "X_test_sample = X_test.sample(n=min(1000, len(X_test)), random_state=42)\n",
    "\n",
    "# TreeExplainer is optimized for tree-based models\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(\"✓ SHAP values computed\")\n",
    "print(f\"Shape: {np.array(shap_values).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Summary Plot: Global Feature Importance\n",
    "\n",
    "The summary plot shows:\n",
    "- Features ranked by importance (top to bottom)\n",
    "- Distribution of SHAP values for each feature\n",
    "- Color indicates feature value (red = high, blue = low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global feature importance visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values[1], X_test_sample, feature_names=feature_columns, show=False)\n",
    "plt.title('SHAP Summary Plot - Global Feature Importance', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Each dot is a person in the dataset\")\n",
    "print(\"- X-axis shows SHAP value (impact on prediction)\")\n",
    "print(\"- Color shows feature value (red=high, blue=low)\")\n",
    "print(\"- Features sorted by importance (top = most important)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Bar Plot: Mean Absolute Feature Importance\n",
    "\n",
    "This shows the average magnitude of each feature's impact across all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of mean absolute SHAP values\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values[1], X_test_sample, plot_type=\"bar\", \n",
    "                  feature_names=feature_columns, show=False)\n",
    "plt.title('Mean Absolute SHAP Values - Feature Importance', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display numerical importance\n",
    "mean_abs_shap = np.abs(shap_values[1]).mean(axis=0)\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Mean |SHAP|': mean_abs_shap\n",
    "}).sort_values('Mean |SHAP|', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Force Plot: Explaining Individual Predictions\n",
    "\n",
    "Force plots show how each feature contributes to pushing a specific prediction away from the base value (average prediction).\n",
    "\n",
    "- **Red arrows**: Push prediction higher (toward >50K)\n",
    "- **Blue arrows**: Push prediction lower (toward <=50K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP JavaScript visualization\n",
    "shap.initjs()\n",
    "\n",
    "# Explain a specific prediction\n",
    "sample_idx = 0\n",
    "sample_prediction = model.predict_proba(X_test_sample.iloc[[sample_idx]])[0]\n",
    "\n",
    "print(f\"Explaining prediction for sample {sample_idx}\")\n",
    "print(f\"Predicted probability of >50K: {sample_prediction[1]:.4f}\")\n",
    "print(f\"Actual label: {'Positive' if y_test.iloc[X_test_sample.index[sample_idx]] == 1 else 'Negative'}\")\n",
    "\n",
    "# Display the force plot\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[1],\n",
    "    shap_values[1][sample_idx],\n",
    "    X_test_sample.iloc[sample_idx],\n",
    "    feature_names=feature_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Local Explanation\n",
    "\n",
    "Let's create a detailed breakdown of what contributes to a specific prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed explanation for a sample\n",
    "def explain_prediction_detailed(model, X_sample, shap_values, feature_names, sample_idx=0):\n",
    "    \"\"\"\n",
    "    Provide a detailed textual explanation of a prediction.\n",
    "    \"\"\"\n",
    "    prediction = model.predict_proba(X_sample.iloc[[sample_idx]])[0]\n",
    "    shap_vals = shap_values[sample_idx]\n",
    "    feature_vals = X_sample.iloc[sample_idx]\n",
    "    \n",
    "    # Create explanation dataframe\n",
    "    explanation = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Value': feature_vals.values,\n",
    "        'SHAP Value': shap_vals,\n",
    "        'Abs SHAP': np.abs(shap_vals)\n",
    "    }).sort_values('Abs SHAP', ascending=False)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"DETAILED EXPLANATION FOR PREDICTION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nPredicted Probabilities:\")\n",
    "    print(f\"  Income <=50K: {prediction[0]:.2%}\")\n",
    "    print(f\"  Income >50K:  {prediction[1]:.2%}\")\n",
    "    print(f\"\\nPrediction: {'Income >50K' if prediction[1] > 0.5 else 'Income <=50K'}\")\n",
    "    \n",
    "    print(f\"\\n{'Feature':<20} {'Value':<15} {'SHAP Impact':<15} {'Direction'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for idx, row in explanation.head(10).iterrows():\n",
    "        direction = \"↑ (Higher income)\" if row['SHAP Value'] > 0 else \"↓ (Lower income)\"\n",
    "        print(f\"{row['Feature']:<20} {str(row['Value']):<15} {row['SHAP Value']:>10.4f}     {direction}\")\n",
    "    \n",
    "    return explanation\n",
    "\n",
    "# Explain the first sample\n",
    "explanation_df = explain_prediction_detailed(\n",
    "    model, X_test_sample, shap_values[1], feature_columns, sample_idx=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Dependence Plots: Feature Interactions\n",
    "\n",
    "Dependence plots show how a single feature's value affects predictions, potentially revealing interactions with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plot for age (most important feature)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Age dependence\n",
    "plt.sca(axes[0])\n",
    "shap.dependence_plot(\n",
    "    'age',\n",
    "    shap_values[1],\n",
    "    X_test_sample,\n",
    "    feature_names=feature_columns,\n",
    "    interaction_index='education-num',\n",
    "    show=False\n",
    ")\n",
    "axes[0].set_title('SHAP Dependence Plot: Age', fontsize=12)\n",
    "\n",
    "# Education dependence\n",
    "plt.sca(axes[1])\n",
    "shap.dependence_plot(\n",
    "    'education-num',\n",
    "    shap_values[1],\n",
    "    X_test_sample,\n",
    "    feature_names=feature_columns,\n",
    "    interaction_index='age',\n",
    "    show=False\n",
    ")\n",
    "axes[1].set_title('SHAP Dependence Plot: Education Level', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- X-axis: Feature value\")\n",
    "print(\"- Y-axis: SHAP value (impact on prediction)\")\n",
    "print(\"- Color: Interaction feature value\")\n",
    "print(\"- Shows how feature effect changes with its value and interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LIME (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "### What is LIME?\n",
    "\n",
    "LIME explains individual predictions by:\n",
    "1. **Perturbing** the input (creating similar samples)\n",
    "2. **Getting predictions** from the complex model for these samples\n",
    "3. **Training a simple model** (like linear regression) on these perturbed samples\n",
    "4. **Using the simple model** to explain the prediction\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Model-agnostic**: Works with any ML model (black box)\n",
    "- **Local fidelity**: Approximates the model locally around the instance\n",
    "- **Interpretable**: Uses interpretable representations (linear models)\n",
    "\n",
    "### LIME vs SHAP\n",
    "\n",
    "| Aspect | LIME | SHAP |\n",
    "|--------|------|------|\n",
    "| Foundation | Perturbation-based | Game theory (Shapley values) |\n",
    "| Scope | Local only | Local + Global |\n",
    "| Speed | Fast | Slower (especially KernelSHAP) |\n",
    "| Consistency | May vary | Consistent and unique |\n",
    "| Use Case | Quick explanations | Thorough analysis |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LIME explainer\n",
    "print(\"Creating LIME explainer...\")\n",
    "\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train.values,\n",
    "    feature_names=feature_columns,\n",
    "    class_names=['<=50K', '>50K'],\n",
    "    mode='classification',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"✓ LIME explainer created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME: Explaining Individual Predictions\n",
    "\n",
    "Let's explain the same prediction we analyzed with SHAP to compare the approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single instance with LIME\n",
    "sample_idx = 0\n",
    "instance = X_test_sample.iloc[sample_idx].values\n",
    "\n",
    "print(\"Generating LIME explanation...\")\n",
    "lime_exp = lime_explainer.explain_instance(\n",
    "    data_row=instance,\n",
    "    predict_fn=model.predict_proba,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "# Display explanation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LIME EXPLANATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get prediction\n",
    "prediction = model.predict_proba(instance.reshape(1, -1))[0]\n",
    "print(f\"\\nPredicted Probabilities:\")\n",
    "print(f\"  Income <=50K: {prediction[0]:.2%}\")\n",
    "print(f\"  Income >50K:  {prediction[1]:.2%}\")\n",
    "\n",
    "# Show the explanation as a list\n",
    "print(f\"\\nTop Contributing Features:\")\n",
    "print(f\"{'Feature Rule':<40} {'Weight':<15} {'Direction'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for feature, weight in lime_exp.as_list()[:10]:\n",
    "    direction = \"→ Higher income\" if weight > 0 else \"→ Lower income\"\n",
    "    print(f\"{feature:<40} {weight:>10.4f}     {direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LIME explanation\n",
    "fig = lime_exp.as_pyplot_figure()\n",
    "plt.title('LIME Explanation - Feature Contributions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Green bars: Features pushing toward >50K income\")\n",
    "print(\"- Red bars: Features pushing toward <=50K income\")\n",
    "print(\"- Bar length: Magnitude of contribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Multiple LIME Explanations\n",
    "\n",
    "Let's examine how LIME explains different types of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain multiple instances\n",
    "def compare_lime_explanations(model, X_data, lime_explainer, n_samples=3):\n",
    "    \"\"\"\n",
    "    Compare LIME explanations for multiple predictions.\n",
    "    \"\"\"\n",
    "    for i in range(n_samples):\n",
    "        instance = X_data.iloc[i].values\n",
    "        prediction = model.predict_proba(instance.reshape(1, -1))[0]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"SAMPLE {i+1}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Predicted: {'Income >50K' if prediction[1] > 0.5 else 'Income <=50K'}\")\n",
    "        print(f\"Confidence: {max(prediction):.2%}\")\n",
    "        \n",
    "        # Generate explanation\n",
    "        exp = lime_explainer.explain_instance(\n",
    "            data_row=instance,\n",
    "            predict_fn=model.predict_proba,\n",
    "            num_features=5\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTop 5 Features:\")\n",
    "        for feature, weight in exp.as_list()[:5]:\n",
    "            print(f\"  {feature}: {weight:+.4f}\")\n",
    "\n",
    "# Compare explanations\n",
    "compare_lime_explanations(model, X_test_sample, lime_explainer, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP vs LIME: Side-by-Side Comparison\n",
    "\n",
    "Let's compare both methods on the same prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SHAP and LIME for the same instance\n",
    "def compare_shap_lime(model, X_sample, shap_vals, lime_explainer, feature_names, idx=0):\n",
    "    \"\"\"\n",
    "    Compare SHAP and LIME explanations side by side.\n",
    "    \"\"\"\n",
    "    instance = X_sample.iloc[idx].values\n",
    "    prediction = model.predict_proba(instance.reshape(1, -1))[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SHAP vs LIME COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nPrediction: {'Income >50K' if prediction[1] > 0.5 else 'Income <=50K'}\")\n",
    "    print(f\"Probability: {prediction[1]:.2%}\")\n",
    "    \n",
    "    # SHAP explanation\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'SHAP': shap_vals[idx],\n",
    "        'Abs_SHAP': np.abs(shap_vals[idx])\n",
    "    }).sort_values('Abs_SHAP', ascending=False)\n",
    "    \n",
    "    # LIME explanation\n",
    "    lime_exp = lime_explainer.explain_instance(\n",
    "        data_row=instance,\n",
    "        predict_fn=model.predict_proba,\n",
    "        num_features=len(feature_names)\n",
    "    )\n",
    "    \n",
    "    lime_dict = dict(lime_exp.as_list())\n",
    "    \n",
    "    print(f\"\\n{'Feature':<20} {'SHAP Value':<15} {'LIME Weight':<15} {'Agreement'}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for _, row in shap_importance.head(8).iterrows():\n",
    "        feature = row['Feature']\n",
    "        shap_val = row['SHAP']\n",
    "        \n",
    "        # Find corresponding LIME value (approximate match)\n",
    "        lime_val = 0\n",
    "        for lime_feature, lime_weight in lime_dict.items():\n",
    "            if feature in lime_feature:\n",
    "                lime_val = lime_weight\n",
    "                break\n",
    "        \n",
    "        # Check if both agree on direction\n",
    "        agreement = \"✓\" if (shap_val * lime_val) > 0 else \"✗\"\n",
    "        \n",
    "        print(f\"{feature:<20} {shap_val:>10.4f}      {lime_val:>10.4f}      {agreement}\")\n",
    "    \n",
    "    print(\"\\nKey Observations:\")\n",
    "    print(\"✓ = Both methods agree on direction (positive or negative impact)\")\n",
    "    print(\"✗ = Methods disagree on direction\")\n",
    "\n",
    "# Run comparison\n",
    "compare_shap_lime(model, X_test_sample, shap_values[1], lime_explainer, feature_columns, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regulatory Considerations for AI Models\n",
    "\n",
    "### Why Regulation Matters\n",
    "\n",
    "Machine learning models increasingly make decisions that significantly impact people's lives:\n",
    "- Credit approval/denial\n",
    "- Hiring decisions\n",
    "- Insurance pricing\n",
    "- Healthcare diagnoses\n",
    "- Criminal justice (bail, sentencing)\n",
    "\n",
    "### Key Regulatory Frameworks\n",
    "\n",
    "1. **GDPR (General Data Protection Regulation) - EU**\n",
    "   - Right to explanation for automated decisions\n",
    "   - Data protection and privacy requirements\n",
    "   - Affects any organization handling EU citizen data\n",
    "\n",
    "2. **ECOA (Equal Credit Opportunity Act) - US**\n",
    "   - Requires explanation for credit denials\n",
    "   - Adverse action notices\n",
    "   - Prohibits discrimination\n",
    "\n",
    "3. **Fair Lending Laws**\n",
    "   - Prohibit discrimination based on protected attributes\n",
    "   - Require fair treatment across demographics\n",
    "\n",
    "4. **Proposed AI Regulations**\n",
    "   - EU AI Act (risk-based approach)\n",
    "   - US Algorithmic Accountability Act\n",
    "   - Various state-level regulations\n",
    "\n",
    "### Protected Attributes\n",
    "\n",
    "These attributes cannot be used for discriminatory decisions:\n",
    "- Race/Ethnicity\n",
    "- Sex/Gender\n",
    "- Age\n",
    "- Religion\n",
    "- National Origin\n",
    "- Disability Status\n",
    "- Marital Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Analysis: Detecting Bias\n",
    "\n",
    "Let's examine if our model treats different demographic groups fairly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model fairness across protected attributes\n",
    "def analyze_fairness(model, X_data, y_true, df_original, feature_columns):\n",
    "    \"\"\"\n",
    "    Analyze model predictions across demographic groups.\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_data)\n",
    "    y_pred_proba = model.predict_proba(X_data)[:, 1]\n",
    "    \n",
    "    # Create analysis dataframe\n",
    "    analysis_df = pd.DataFrame({\n",
    "        'prediction': y_pred,\n",
    "        'probability': y_pred_proba,\n",
    "        'actual': y_true.values\n",
    "    })\n",
    "    \n",
    "    # Add demographic information\n",
    "    # Get sex information (assuming it's encoded)\n",
    "    sex_idx = feature_columns.index('sex')\n",
    "    analysis_df['sex'] = X_data.iloc[:, sex_idx].values\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FAIRNESS ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analyze by sex\n",
    "    print(\"\\nPredictions by Sex:\")\n",
    "    sex_analysis = analysis_df.groupby('sex').agg({\n",
    "        'prediction': ['count', 'mean'],\n",
    "        'probability': 'mean',\n",
    "        'actual': 'mean'\n",
    "    })\n",
    "    sex_analysis.columns = ['Count', 'Predicted >50K Rate', 'Avg Probability', 'Actual >50K Rate']\n",
    "    print(sex_analysis)\n",
    "    \n",
    "    # Statistical parity difference\n",
    "    rates = analysis_df.groupby('sex')['prediction'].mean()\n",
    "    if len(rates) == 2:\n",
    "        spd = rates.iloc[1] - rates.iloc[0]\n",
    "        print(f\"\\nStatistical Parity Difference: {spd:.4f}\")\n",
    "        print(\"(Difference in positive prediction rates between groups)\")\n",
    "        print(\"Ideally should be close to 0 for fairness\")\n",
    "    \n",
    "    return analysis_df\n",
    "\n",
    "# Run fairness analysis\n",
    "fairness_results = analyze_fairness(model, X_test_sample, \n",
    "                                    y_test.loc[X_test_sample.index], \n",
    "                                    df_clean, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SHAP for Bias Detection\n",
    "\n",
    "SHAP can help identify if protected attributes unfairly influence predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze SHAP values for protected attributes\n",
    "protected_features = ['sex', 'race', 'age']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PROTECTED ATTRIBUTE IMPACT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate mean absolute SHAP values for protected features\n",
    "mean_abs_shap = np.abs(shap_values[1]).mean(axis=0)\n",
    "all_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Mean |SHAP|': mean_abs_shap,\n",
    "    'Rank': range(1, len(feature_columns) + 1)\n",
    "}).sort_values('Mean |SHAP|', ascending=False)\n",
    "all_importance_df['Rank'] = range(1, len(all_importance_df) + 1)\n",
    "\n",
    "print(\"\\nProtected Attribute Importance:\")\n",
    "print(f\"{'Feature':<20} {'Mean |SHAP|':<15} {'Rank':<10} {'Status'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for feature in protected_features:\n",
    "    if feature in all_importance_df['Feature'].values:\n",
    "        row = all_importance_df[all_importance_df['Feature'] == feature].iloc[0]\n",
    "        importance = row['Mean |SHAP|']\n",
    "        rank = row['Rank']\n",
    "        status = \"⚠ HIGH\" if rank <= 5 else \"✓ Low\" if rank > 10 else \"~ Medium\"\n",
    "        print(f\"{feature:<20} {importance:<15.4f} {rank:<10} {status}\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"1. High importance for protected attributes may indicate potential bias\")\n",
    "print(\"2. Consider if this importance is justified by legitimate business needs\")\n",
    "print(\"3. May need to implement fairness constraints or preprocessing\")\n",
    "print(\"4. Document decision-making process for regulatory compliance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Regulatory Compliance\n",
    "\n",
    "#### 1. Documentation Requirements\n",
    "\n",
    "Maintain comprehensive documentation:\n",
    "- Model development process\n",
    "- Training data sources and characteristics\n",
    "- Feature selection rationale\n",
    "- Validation and testing procedures\n",
    "- Fairness assessments\n",
    "- Ongoing monitoring procedures\n",
    "\n",
    "#### 2. Explainability Requirements\n",
    "\n",
    "Be able to explain:\n",
    "- **Why**: Why did the model make this decision?\n",
    "- **What**: What factors were most important?\n",
    "- **How**: How do we ensure fairness?\n",
    "- **When**: When should we update the model?\n",
    "\n",
    "#### 3. Adverse Action Notices\n",
    "\n",
    "For negative decisions (e.g., loan denial), provide:\n",
    "- Primary reasons for the decision\n",
    "- Specific factors that adversely affected the decision\n",
    "- Clear, understandable language (not technical jargon)\n",
    "\n",
    "#### 4. Human Oversight\n",
    "\n",
    "- Ensure human review for high-stakes decisions\n",
    "- Ability to override automated decisions\n",
    "- Appeal process for affected individuals\n",
    "\n",
    "#### 5. Monitoring and Auditing\n",
    "\n",
    "- Regular fairness audits\n",
    "- Performance monitoring across demographics\n",
    "- Drift detection (data and concept drift)\n",
    "- Incident response procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Adverse Action Explanations\n",
    "\n",
    "Let's create a function that generates regulatory-compliant explanations for negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adverse_action_notice(model, instance, shap_values, feature_names, top_n=4):\n",
    "    \"\"\"\n",
    "    Generate a regulatory-compliant adverse action notice.\n",
    "    \n",
    "    This simulates what would be required under regulations like ECOA.\n",
    "    \"\"\"\n",
    "    prediction = model.predict_proba(instance.reshape(1, -1))[0]\n",
    "    \n",
    "    # Only generate for negative predictions\n",
    "    if prediction[1] >= 0.5:\n",
    "        return \"Approved - No adverse action notice required\"\n",
    "    \n",
    "    # Get SHAP values for this instance\n",
    "    # Negative SHAP values push toward negative prediction (<=50K)\n",
    "    shap_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'SHAP': shap_values,\n",
    "        'Value': instance\n",
    "    })\n",
    "    \n",
    "    # Sort by SHAP value (most negative = most responsible for denial)\n",
    "    shap_df['Abs_SHAP'] = np.abs(shap_df['SHAP'])\n",
    "    top_factors = shap_df.nsmallest(top_n, 'SHAP')\n",
    "    \n",
    "    # Generate notice\n",
    "    notice = []\n",
    "    notice.append(\"=\"*80)\n",
    "    notice.append(\"ADVERSE ACTION NOTICE\")\n",
    "    notice.append(\"=\"*80)\n",
    "    notice.append(\"\")\n",
    "    notice.append(\"Your application for income classification has been DENIED.\")\n",
    "    notice.append(\"\")\n",
    "    notice.append(\"The primary factors that adversely affected your decision were:\")\n",
    "    notice.append(\"\")\n",
    "    \n",
    "    # Map technical features to user-friendly descriptions\n",
    "    feature_descriptions = {\n",
    "        'age': 'Your age',\n",
    "        'education-num': 'Your education level',\n",
    "        'capital-gain': 'Your capital gains',\n",
    "        'capital-loss': 'Your capital losses',\n",
    "        'hours-per-week': 'Your hours worked per week',\n",
    "        'marital-status': 'Your marital status',\n",
    "        'occupation': 'Your occupation',\n",
    "        'relationship': 'Your relationship status',\n",
    "        'workclass': 'Your work class',\n",
    "        'education': 'Your education'\n",
    "    }\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_factors.iterrows(), 1):\n",
    "        feature = row['Feature']\n",
    "        description = feature_descriptions.get(feature, feature)\n",
    "        notice.append(f\"{i}. {description}\")\n",
    "    \n",
    "    notice.append(\"\")\n",
    "    notice.append(\"You have the right to:\")\n",
    "    notice.append(\"- Request additional information about this decision\")\n",
    "    notice.append(\"- Appeal this decision within 60 days\")\n",
    "    notice.append(\"- Request a free copy of any report used in this decision\")\n",
    "    notice.append(\"\")\n",
    "    notice.append(\"This decision was made using an automated system with human oversight.\")\n",
    "    notice.append(\"=\"*80)\n",
    "    \n",
    "    return \"\\n\".join(notice)\n",
    "\n",
    "# Example: Generate notice for a denied application\n",
    "# Find a sample with negative prediction\n",
    "for i in range(len(X_test_sample)):\n",
    "    instance = X_test_sample.iloc[i].values\n",
    "    prediction = model.predict_proba(instance.reshape(1, -1))[0]\n",
    "    \n",
    "    if prediction[1] < 0.5:  # Negative prediction\n",
    "        notice = generate_adverse_action_notice(\n",
    "            model, instance, shap_values[1][i], feature_columns\n",
    "        )\n",
    "        print(notice)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### 1. Model Interpretability is Essential\n",
    "\n",
    "- Not just a nice-to-have, but increasingly **required by law**\n",
    "- Critical for trust, debugging, and fairness\n",
    "- Must balance with model performance\n",
    "\n",
    "### 2. SHAP Advantages\n",
    "\n",
    "✅ **Theoretically grounded** (Shapley values)\n",
    "✅ **Consistent and unique** explanations\n",
    "✅ **Both global and local** interpretability\n",
    "✅ **Handles feature interactions**\n",
    "✅ **Fast for tree-based models** (TreeExplainer)\n",
    "\n",
    "❌ Can be computationally expensive (KernelSHAP)\n",
    "❌ May be complex to communicate to non-technical audiences\n",
    "\n",
    "### 3. LIME Advantages\n",
    "\n",
    "✅ **Model-agnostic** - works with any model\n",
    "✅ **Fast and intuitive**\n",
    "✅ **Simple to understand** (linear approximation)\n",
    "✅ **Great for text and images**\n",
    "\n",
    "❌ Local only (no global importance)\n",
    "❌ Can vary across runs\n",
    "❌ Limited feature interaction support\n",
    "\n",
    "### 4. When to Use Each\n",
    "\n",
    "**Use SHAP when:**\n",
    "- You need theoretical guarantees\n",
    "- Global feature importance is important\n",
    "- Working with tree-based models\n",
    "- Regulatory compliance requires rigorous explanations\n",
    "\n",
    "**Use LIME when:**\n",
    "- Need quick local explanations\n",
    "- Working with text or image data\n",
    "- Computational resources are limited\n",
    "- Want simple, intuitive explanations\n",
    "\n",
    "**Best Practice:** Use both for critical applications!\n",
    "\n",
    "### 5. Regulatory Compliance Checklist\n",
    "\n",
    "- [ ] Document model development process\n",
    "- [ ] Assess and mitigate bias across protected groups\n",
    "- [ ] Implement explainability (SHAP/LIME)\n",
    "- [ ] Create adverse action notice process\n",
    "- [ ] Establish human oversight procedures\n",
    "- [ ] Set up monitoring and auditing\n",
    "- [ ] Create appeal process\n",
    "- [ ] Train staff on AI ethics and fairness\n",
    "\n",
    "### 6. Future Considerations\n",
    "\n",
    "As AI regulation evolves:\n",
    "- Stay informed about new regulations (EU AI Act, etc.)\n",
    "- Invest in fairness-aware ML techniques\n",
    "- Build interpretability into your ML pipeline from day one\n",
    "- Consider ethical implications beyond legal requirements\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [SHAP Documentation](https://shap.readthedocs.io/)\n",
    "- [LIME Documentation](https://lime-ml.readthedocs.io/)\n",
    "- [Interpretable ML Book](https://christophm.github.io/interpretable-ml-book/)\n",
    "- [Fairlearn Library](https://fairlearn.org/)\n",
    "- [EU AI Act](https://artificialintelligenceact.eu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises for Practice\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Compare different models**: Train a Gradient Boosting model and compare its SHAP explanations with the Random Forest\n",
    "\n",
    "2. **Feature engineering**: Create interaction features (e.g., age × education) and see how they affect explanations\n",
    "\n",
    "3. **Fairness improvement**: Try removing or down-weighting protected attributes and measure the impact on fairness metrics\n",
    "\n",
    "4. **Custom explanations**: Modify the adverse action notice generator to be more user-friendly\n",
    "\n",
    "5. **Different datasets**: Apply SHAP and LIME to:\n",
    "   - Healthcare data (e.g., diabetes prediction)\n",
    "   - Credit scoring data\n",
    "   - Your own domain-specific dataset\n",
    "\n",
    "6. **Interaction analysis**: Use SHAP interaction values to find important feature pairs\n",
    "\n",
    "7. **Stability analysis**: Run LIME multiple times on the same instance and measure explanation stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for exercises here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
