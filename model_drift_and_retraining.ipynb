{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Retraining and Drift Detection: A Hands-On Lab\n",
    "\n",
    "This notebook demonstrates core concepts in maintaining machine learning models in production:\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. **Understanding Model Drift**\n",
    "   - Concept Drift: Changes in the relationship between features and target\n",
    "   - Data Drift: Changes in the input data distribution\n",
    "   \n",
    "2. **Detecting Model Drift**\n",
    "   - Statistical tests for data drift (KS-test, Chi-square)\n",
    "   - Population Stability Index (PSI)\n",
    "   - Performance monitoring for concept drift\n",
    "   \n",
    "3. **Strategies for Model Retraining**\n",
    "   - Periodic retraining\n",
    "   - Trigger-based retraining\n",
    "   - Incremental learning\n",
    "   \n",
    "4. **Automating Retraining Pipelines**\n",
    "   - Building automated monitoring systems\n",
    "   - Implementing retraining triggers\n",
    "   - Version control for models\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We'll use the **Credit Card Fraud Detection** dataset from Kaggle (simulated version), which is ideal for demonstrating drift as fraud patterns change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"\u2713 Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Model Drift\n",
    "\n",
    "### What is Model Drift?\n",
    "\n",
    "Model drift occurs when the statistical properties of the target variable or input features change over time, causing model performance to degrade.\n",
    "\n",
    "### Types of Drift:\n",
    "\n",
    "**1. Data Drift (Covariate Shift)**\n",
    "- The distribution of input features P(X) changes\n",
    "- Example: Customer demographics shift as your business expands to new markets\n",
    "- The relationship between features and target remains the same\n",
    "\n",
    "**2. Concept Drift (Prior Probability Shift)**\n",
    "- The relationship between features and target P(Y|X) changes\n",
    "- Example: Fraud patterns evolve as criminals adapt to detection systems\n",
    "- Model predictions become less accurate even if data distribution stays constant\n",
    "\n",
    "**3. Label Drift**\n",
    "- The distribution of the target variable P(Y) changes\n",
    "- Example: Seasonal changes in product demand\n",
    "\n",
    "Let's visualize these concepts with simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell demonstrates the difference between data drift and concept drift\n",
    "\n",
    "# Generate baseline data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Original data distribution\n",
    "X_original = np.random.randn(n_samples, 2)\n",
    "y_original = (X_original[:, 0] + X_original[:, 1] > 0).astype(int)\n",
    "\n",
    "# Data drift: shift in feature distribution but same relationship\n",
    "X_data_drift = np.random.randn(n_samples, 2) + np.array([2, 1])  # Shifted distribution\n",
    "y_data_drift = (X_data_drift[:, 0] + X_data_drift[:, 1] > 2).astype(int)  # Same decision boundary, shifted\n",
    "\n",
    "# Concept drift: same feature distribution but different relationship\n",
    "X_concept_drift = np.random.randn(n_samples, 2)\n",
    "y_concept_drift = (X_concept_drift[:, 0] - X_concept_drift[:, 1] > 0).astype(int)  # Different decision boundary\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Original data\n",
    "axes[0].scatter(X_original[y_original==0, 0], X_original[y_original==0, 1], \n",
    "                alpha=0.5, label='Class 0', s=20)\n",
    "axes[0].scatter(X_original[y_original==1, 0], X_original[y_original==1, 1], \n",
    "                alpha=0.5, label='Class 1', s=20)\n",
    "axes[0].set_title('Original Data Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Data drift\n",
    "axes[1].scatter(X_data_drift[y_data_drift==0, 0], X_data_drift[y_data_drift==0, 1], \n",
    "                alpha=0.5, label='Class 0', s=20)\n",
    "axes[1].scatter(X_data_drift[y_data_drift==1, 0], X_data_drift[y_data_drift==1, 1], \n",
    "                alpha=0.5, label='Class 1', s=20)\n",
    "axes[1].set_title('Data Drift\\n(Features shifted, same relationship)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Concept drift\n",
    "axes[2].scatter(X_concept_drift[y_concept_drift==0, 0], X_concept_drift[y_concept_drift==0, 1], \n",
    "                alpha=0.5, label='Class 0', s=20)\n",
    "axes[2].scatter(X_concept_drift[y_concept_drift==1, 0], X_concept_drift[y_concept_drift==1, 1], \n",
    "                alpha=0.5, label='Class 1', s=20)\n",
    "axes[2].set_title('Concept Drift\\n(Same features, different relationship)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Visualization complete\")\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Original: Baseline data distribution\")\n",
    "print(\"- Data Drift: Data has shifted in feature space, but decision boundary relationship is preserved\")\n",
    "print(\"- Concept Drift: Feature distribution similar, but the decision boundary has rotated (relationship changed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing a Real-World Dataset\n",
    "\n",
    "We'll create a simulated credit card fraud detection dataset that mimics real-world patterns:\n",
    "- Highly imbalanced (fraud is rare)\n",
    "- Multiple features representing transaction characteristics\n",
    "- Temporal component to simulate drift over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fraud_detection_dataset(n_samples=10000, fraud_rate=0.02, drift_type=None, drift_magnitude=0.0):\n",
    "    \"\"\"\n",
    "    Create a simulated fraud detection dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    fraud_rate : float\n",
    "        Proportion of fraudulent transactions\n",
    "    drift_type : str or None\n",
    "        Type of drift to introduce ('data', 'concept', or None)\n",
    "    drift_magnitude : float\n",
    "        Magnitude of drift (0-1)\n",
    "    \"\"\"\n",
    "    np.random.seed(42 + int(drift_magnitude * 100))\n",
    "    \n",
    "    # Generate features\n",
    "    # Feature 1: Transaction amount (log-normal distribution)\n",
    "    amount = np.random.lognormal(mean=4, sigma=1.5, size=n_samples)\n",
    "    \n",
    "    # Feature 2: Time of day (0-24 hours)\n",
    "    hour_of_day = np.random.uniform(0, 24, n_samples)\n",
    "    \n",
    "    # Feature 3: Distance from home\n",
    "    distance_from_home = np.random.exponential(scale=50, size=n_samples)\n",
    "    \n",
    "    # Feature 4: Number of transactions in last 24h\n",
    "    transactions_last_24h = np.random.poisson(lam=5, size=n_samples)\n",
    "    \n",
    "    # Feature 5: Average transaction amount\n",
    "    avg_transaction_amount = amount * np.random.uniform(0.8, 1.2, n_samples)\n",
    "    \n",
    "    # Apply data drift if specified\n",
    "    if drift_type == 'data':\n",
    "        amount = amount * (1 + drift_magnitude)\n",
    "        distance_from_home = distance_from_home * (1 + drift_magnitude * 0.5)\n",
    "        hour_of_day = (hour_of_day + drift_magnitude * 6) % 24\n",
    "    \n",
    "    # Create fraud labels\n",
    "    n_fraud = int(n_samples * fraud_rate)\n",
    "    is_fraud = np.zeros(n_samples)\n",
    "    is_fraud[:n_fraud] = 1\n",
    "    np.random.shuffle(is_fraud)\n",
    "    \n",
    "    # Fraud patterns (concept)\n",
    "    if drift_type == 'concept':\n",
    "        # Original pattern: High amount + late hours + far from home = fraud\n",
    "        # Drifted pattern: Moderate amount + unusual times + many transactions = fraud\n",
    "        fraud_score = (\n",
    "            (1 - drift_magnitude) * (amount > np.percentile(amount, 80)) * \n",
    "            (hour_of_day > 22) * (distance_from_home > 100) +\n",
    "            drift_magnitude * (amount > np.percentile(amount, 60)) * \n",
    "            (transactions_last_24h > 8) * ((hour_of_day < 4) | (hour_of_day > 20))\n",
    "        )\n",
    "    else:\n",
    "        # Standard fraud pattern\n",
    "        fraud_score = (\n",
    "            (amount > np.percentile(amount, 80)) * \n",
    "            (hour_of_day > 22) * \n",
    "            (distance_from_home > 100)\n",
    "        )\n",
    "    \n",
    "    # Make actual frauds more likely to have high fraud scores\n",
    "    fraud_indices = np.where(is_fraud == 1)[0]\n",
    "    fraud_score[fraud_indices] += np.random.uniform(0.5, 1.0, len(fraud_indices))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'amount': amount,\n",
    "        'hour_of_day': hour_of_day,\n",
    "        'distance_from_home': distance_from_home,\n",
    "        'transactions_last_24h': transactions_last_24h,\n",
    "        'avg_transaction_amount': avg_transaction_amount,\n",
    "        'fraud_score_latent': fraud_score,\n",
    "        'is_fraud': is_fraud.astype(int)\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate baseline dataset (training data)\n",
    "print(\"Generating datasets...\")\n",
    "df_train = create_fraud_detection_dataset(n_samples=10000, fraud_rate=0.02)\n",
    "df_test = create_fraud_detection_dataset(n_samples=3000, fraud_rate=0.02)\n",
    "\n",
    "print(f\"\\n\u2713 Training data: {df_train.shape}\")\n",
    "print(f\"\u2713 Test data: {df_test.shape}\")\n",
    "print(f\"\\nFraud rate in training: {df_train['is_fraud'].mean():.2%}\")\n",
    "print(f\"Fraud rate in test: {df_test['is_fraud'].mean():.2%}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nSample of training data:\")\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell performs exploratory data analysis on the fraud detection dataset\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Feature distributions by fraud status\n",
    "features = ['amount', 'hour_of_day', 'distance_from_home', 'transactions_last_24h', 'avg_transaction_amount']\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    axes[idx].hist(df_train[df_train['is_fraud']==0][feature], bins=50, alpha=0.6, label='Normal', density=True)\n",
    "    axes[idx].hist(df_train[df_train['is_fraud']==1][feature], bins=50, alpha=0.6, label='Fraud', density=True)\n",
    "    axes[idx].set_xlabel(feature.replace('_', ' ').title(), fontsize=11)\n",
    "    axes[idx].set_ylabel('Density', fontsize=11)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_title(f'{feature.replace(\"_\", \" \").title()} Distribution', fontweight='bold')\n",
    "\n",
    "# Class balance\n",
    "class_counts = df_train['is_fraud'].value_counts()\n",
    "axes[5].bar(['Normal', 'Fraud'], class_counts.values, color=['lightblue', 'salmon'])\n",
    "axes[5].set_ylabel('Count', fontsize=11)\n",
    "axes[5].set_title('Class Balance', fontweight='bold')\n",
    "axes[5].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    axes[5].text(i, v + 100, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 EDA complete\")\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Dataset is highly imbalanced (typical for fraud detection)\")\n",
    "print(\"- Fraudulent transactions tend to have higher amounts\")\n",
    "print(\"- Fraud is more common during late night hours\")\n",
    "print(\"- Greater distance from home correlates with fraud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Baseline Model\n",
    "\n",
    "Let's train a baseline fraud detection model that we'll use to demonstrate drift detection and retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = ['amount', 'hour_of_day', 'distance_from_home', 'transactions_last_24h', 'avg_transaction_amount']\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train['is_fraud']\n",
    "X_test = df_test[feature_cols]\n",
    "y_test = df_test['is_fraud']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train baseline model\n",
    "print(\"Training baseline Random Forest model...\")\n",
    "baseline_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate baseline model\n",
    "y_pred_train = baseline_model.predict(X_train_scaled)\n",
    "y_pred_test = baseline_model.predict(X_test_scaled)\n",
    "y_pred_proba_test = baseline_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTraining Set:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"  F1-Score:  {f1_score(y_train, y_pred_train):.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  F1-Score:  {f1_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_pred_proba_test):.4f}\")\n",
    "\n",
    "# Store baseline performance for comparison\n",
    "baseline_performance = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_test),\n",
    "    'precision': precision_score(y_test, y_pred_test),\n",
    "    'recall': recall_score(y_test, y_pred_test),\n",
    "    'f1': f1_score(y_test, y_pred_test),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba_test)\n",
    "}\n",
    "\n",
    "print(\"\\n\u2713 Baseline model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detecting Data Drift\n",
    "\n",
    "Data drift occurs when the distribution of input features changes over time. We'll implement several methods to detect this:\n",
    "\n",
    "### Methods:\n",
    "1. **Kolmogorov-Smirnov (KS) Test**: Statistical test comparing two distributions\n",
    "2. **Population Stability Index (PSI)**: Industry standard for monitoring feature drift\n",
    "3. **Chi-Square Test**: For categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDriftDetector:\n",
    "    \"\"\"\n",
    "    Detect data drift using statistical methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data, feature_names=None):\n",
    "        \"\"\"\n",
    "        Initialize with reference (training) data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        reference_data : array-like or DataFrame\n",
    "            Reference dataset for comparison\n",
    "        feature_names : list, optional\n",
    "            Names of features\n",
    "        \"\"\"\n",
    "        self.reference_data = reference_data\n",
    "        self.feature_names = feature_names\n",
    "        if hasattr(reference_data, 'columns'):\n",
    "            self.feature_names = reference_data.columns.tolist()\n",
    "    \n",
    "    def ks_test(self, new_data, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Perform Kolmogorov-Smirnov test for each feature.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        new_data : array-like\n",
    "            New data to test\n",
    "        alpha : float\n",
    "            Significance level\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Test results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        n_features = self.reference_data.shape[1]\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            # Extract feature\n",
    "            ref_feature = self.reference_data.iloc[:, i] if hasattr(self.reference_data, 'iloc') else self.reference_data[:, i]\n",
    "            new_feature = new_data.iloc[:, i] if hasattr(new_data, 'iloc') else new_data[:, i]\n",
    "            \n",
    "            # Perform KS test\n",
    "            statistic, p_value = stats.ks_2samp(ref_feature, new_feature)\n",
    "            \n",
    "            feature_name = self.feature_names[i] if self.feature_names else f\"Feature_{i}\"\n",
    "            \n",
    "            results.append({\n",
    "                'feature': feature_name,\n",
    "                'ks_statistic': statistic,\n",
    "                'p_value': p_value,\n",
    "                'drift_detected': p_value < alpha\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        return {\n",
    "            'overall_drift': results_df['drift_detected'].any(),\n",
    "            'n_drifted_features': results_df['drift_detected'].sum(),\n",
    "            'results': results_df\n",
    "        }\n",
    "    \n",
    "    def psi(self, new_data, n_bins=10, threshold=0.1):\n",
    "        \"\"\"\n",
    "        Calculate Population Stability Index.\n",
    "        \n",
    "        PSI < 0.1: No significant change\n",
    "        0.1 <= PSI < 0.25: Small change\n",
    "        PSI >= 0.25: Large change (retraining recommended)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        new_data : array-like\n",
    "            New data to test\n",
    "        n_bins : int\n",
    "            Number of bins for discretization\n",
    "        threshold : float\n",
    "            PSI threshold for drift detection\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : PSI results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        n_features = self.reference_data.shape[1]\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            # Extract feature\n",
    "            ref_feature = self.reference_data.iloc[:, i] if hasattr(self.reference_data, 'iloc') else self.reference_data[:, i]\n",
    "            new_feature = new_data.iloc[:, i] if hasattr(new_data, 'iloc') else new_data[:, i]\n",
    "            \n",
    "            # Create bins based on reference data\n",
    "            _, bin_edges = np.histogram(ref_feature, bins=n_bins)\n",
    "            \n",
    "            # Bin both datasets\n",
    "            ref_binned = np.histogram(ref_feature, bins=bin_edges)[0]\n",
    "            new_binned = np.histogram(new_feature, bins=bin_edges)[0]\n",
    "            \n",
    "            # Convert to percentages\n",
    "            ref_pct = ref_binned / len(ref_feature)\n",
    "            new_pct = new_binned / len(new_feature)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            ref_pct = np.where(ref_pct == 0, 0.0001, ref_pct)\n",
    "            new_pct = np.where(new_pct == 0, 0.0001, new_pct)\n",
    "            \n",
    "            # Calculate PSI\n",
    "            psi_value = np.sum((new_pct - ref_pct) * np.log(new_pct / ref_pct))\n",
    "            \n",
    "            feature_name = self.feature_names[i] if self.feature_names else f\"Feature_{i}\"\n",
    "            \n",
    "            # Interpret PSI\n",
    "            if psi_value < 0.1:\n",
    "                interpretation = \"No significant change\"\n",
    "            elif psi_value < 0.25:\n",
    "                interpretation = \"Small change\"\n",
    "            else:\n",
    "                interpretation = \"Large change - retraining recommended\"\n",
    "            \n",
    "            results.append({\n",
    "                'feature': feature_name,\n",
    "                'psi': psi_value,\n",
    "                'interpretation': interpretation,\n",
    "                'drift_detected': psi_value > threshold\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        return {\n",
    "            'overall_drift': results_df['drift_detected'].any(),\n",
    "            'mean_psi': results_df['psi'].mean(),\n",
    "            'max_psi': results_df['psi'].max(),\n",
    "            'results': results_df\n",
    "        }\n",
    "\n",
    "print(\"\u2713 DataDriftDetector class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with drift\n",
    "print(\"Generating datasets with varying levels of drift...\\n\")\n",
    "\n",
    "# No drift\n",
    "df_no_drift = create_fraud_detection_dataset(n_samples=3000, fraud_rate=0.02, drift_type=None)\n",
    "\n",
    "# Small data drift\n",
    "df_small_drift = create_fraud_detection_dataset(n_samples=3000, fraud_rate=0.02, drift_type='data', drift_magnitude=0.2)\n",
    "\n",
    "# Large data drift\n",
    "df_large_drift = create_fraud_detection_dataset(n_samples=3000, fraud_rate=0.02, drift_type='data', drift_magnitude=0.8)\n",
    "\n",
    "# Initialize drift detector\n",
    "drift_detector = DataDriftDetector(df_train[feature_cols], feature_names=feature_cols)\n",
    "\n",
    "# Test different scenarios\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA DRIFT DETECTION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# No drift\n",
    "print(\"\\n1. Testing with NO DRIFT:\")\n",
    "result_no_drift = drift_detector.ks_test(df_no_drift[feature_cols])\n",
    "print(f\"   Overall drift detected: {result_no_drift['overall_drift']}\")\n",
    "print(f\"   Drifted features: {result_no_drift['n_drifted_features']}/{len(feature_cols)}\")\n",
    "\n",
    "psi_no_drift = drift_detector.psi(df_no_drift[feature_cols])\n",
    "print(f\"   Mean PSI: {psi_no_drift['mean_psi']:.4f}\")\n",
    "\n",
    "# Small drift\n",
    "print(\"\\n2. Testing with SMALL DATA DRIFT:\")\n",
    "result_small_drift = drift_detector.ks_test(df_small_drift[feature_cols])\n",
    "print(f\"   Overall drift detected: {result_small_drift['overall_drift']}\")\n",
    "print(f\"   Drifted features: {result_small_drift['n_drifted_features']}/{len(feature_cols)}\")\n",
    "\n",
    "psi_small_drift = drift_detector.psi(df_small_drift[feature_cols])\n",
    "print(f\"   Mean PSI: {psi_small_drift['mean_psi']:.4f}\")\n",
    "print(\"\\n   PSI by feature:\")\n",
    "print(psi_small_drift['results'][['feature', 'psi', 'interpretation']])\n",
    "\n",
    "# Large drift\n",
    "print(\"\\n3. Testing with LARGE DATA DRIFT:\")\n",
    "result_large_drift = drift_detector.ks_test(df_large_drift[feature_cols])\n",
    "print(f\"   Overall drift detected: {result_large_drift['overall_drift']}\")\n",
    "print(f\"   Drifted features: {result_large_drift['n_drifted_features']}/{len(feature_cols)}\")\n",
    "\n",
    "psi_large_drift = drift_detector.psi(df_large_drift[feature_cols])\n",
    "print(f\"   Mean PSI: {psi_large_drift['mean_psi']:.4f}\")\n",
    "print(\"\\n   PSI by feature:\")\n",
    "print(psi_large_drift['results'][['feature', 'psi', 'interpretation']])\n",
    "\n",
    "print(\"\\n\u2713 Drift detection tests complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data drift for one feature\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "feature_to_plot = 'amount'\n",
    "\n",
    "# No drift\n",
    "axes[0].hist(df_train[feature_to_plot], bins=50, alpha=0.6, label='Training (Reference)', density=True)\n",
    "axes[0].hist(df_no_drift[feature_to_plot], bins=50, alpha=0.6, label='New Data', density=True)\n",
    "axes[0].set_title(f'No Drift\\nPSI: {psi_no_drift[\"results\"][psi_no_drift[\"results\"][\"feature\"]==feature_to_plot][\"psi\"].values[0]:.4f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel(feature_to_plot.replace('_', ' ').title())\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Small drift\n",
    "axes[1].hist(df_train[feature_to_plot], bins=50, alpha=0.6, label='Training (Reference)', density=True)\n",
    "axes[1].hist(df_small_drift[feature_to_plot], bins=50, alpha=0.6, label='New Data (Small Drift)', density=True)\n",
    "axes[1].set_title(f'Small Data Drift\\nPSI: {psi_small_drift[\"results\"][psi_small_drift[\"results\"][\"feature\"]==feature_to_plot][\"psi\"].values[0]:.4f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel(feature_to_plot.replace('_', ' ').title())\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Large drift\n",
    "axes[2].hist(df_train[feature_to_plot], bins=50, alpha=0.6, label='Training (Reference)', density=True)\n",
    "axes[2].hist(df_large_drift[feature_to_plot], bins=50, alpha=0.6, label='New Data (Large Drift)', density=True)\n",
    "axes[2].set_title(f'Large Data Drift\\nPSI: {psi_large_drift[\"results\"][psi_large_drift[\"results\"][\"feature\"]==feature_to_plot][\"psi\"].values[0]:.4f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel(feature_to_plot.replace('_', ' ').title())\n",
    "axes[2].set_ylabel('Density')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Visualization complete\")\n",
    "print(\"\\nObservation: As drift magnitude increases, the distribution of 'amount' shifts significantly,\")\n",
    "print(\"and the PSI value increases, indicating the need for model retraining.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detecting Concept Drift\n",
    "\n",
    "Concept drift occurs when the relationship between features and target changes. This is harder to detect than data drift because it requires monitoring model performance over time.\n",
    "\n",
    "### Detection Methods:\n",
    "1. **Performance Monitoring**: Track accuracy, precision, recall over time\n",
    "2. **Prediction Distribution Shifts**: Monitor changes in prediction distributions\n",
    "3. **Error Rate Analysis**: Detect increases in error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptDriftDetector:\n",
    "    \"\"\"\n",
    "    Detect concept drift through performance monitoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, baseline_performance):\n",
    "        \"\"\"\n",
    "        Initialize concept drift detector.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : estimator\n",
    "            Trained model\n",
    "        baseline_performance : dict\n",
    "            Baseline performance metrics\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.baseline_performance = baseline_performance\n",
    "        self.performance_history = []\n",
    "    \n",
    "    def monitor_performance(self, X, y, window_name='Current'):\n",
    "        \"\"\"\n",
    "        Monitor model performance on new data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Features\n",
    "        y : array-like\n",
    "            True labels\n",
    "        window_name : str\n",
    "            Name/identifier for this time window\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        y_pred = self.model.predict(X)\n",
    "        y_pred_proba = self.model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        performance = {\n",
    "            'window': window_name,\n",
    "            'timestamp': datetime.now(),\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'precision': precision_score(y, y_pred),\n",
    "            'recall': recall_score(y, y_pred),\n",
    "            'f1': f1_score(y, y_pred),\n",
    "            'roc_auc': roc_auc_score(y, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        self.performance_history.append(performance)\n",
    "        return performance\n",
    "    \n",
    "    def detect_performance_degradation(self, threshold=0.05):\n",
    "        \"\"\"\n",
    "        Detect significant performance degradation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        threshold : float\n",
    "            Performance drop threshold\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Drift detection results\n",
    "        \"\"\"\n",
    "        if not self.performance_history:\n",
    "            return {'drift_detected': False, 'message': 'No performance history'}\n",
    "        \n",
    "        current_performance = self.performance_history[-1]\n",
    "        \n",
    "        degradation_metrics = {}\n",
    "        drift_detected = False\n",
    "        \n",
    "        for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "            baseline_value = self.baseline_performance.get(metric, 0)\n",
    "            current_value = current_performance.get(metric, 0)\n",
    "            drop = baseline_value - current_value\n",
    "            \n",
    "            degradation_metrics[metric] = {\n",
    "                'baseline': baseline_value,\n",
    "                'current': current_value,\n",
    "                'drop': drop,\n",
    "                'significant': drop > threshold\n",
    "            }\n",
    "            \n",
    "            if drop > threshold:\n",
    "                drift_detected = True\n",
    "        \n",
    "        return {\n",
    "            'drift_detected': drift_detected,\n",
    "            'metrics': degradation_metrics\n",
    "        }\n",
    "    \n",
    "    def detect_prediction_drift(self, reference_predictions, new_predictions, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Detect drift in prediction distributions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        reference_predictions : array-like\n",
    "            Baseline predictions\n",
    "        new_predictions : array-like\n",
    "            New predictions\n",
    "        alpha : float\n",
    "            Significance level\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Drift detection results\n",
    "        \"\"\"\n",
    "        # For binary classification, use Chi-square test\n",
    "        ref_dist = np.bincount(reference_predictions.astype(int), minlength=2)\n",
    "        new_dist = np.bincount(new_predictions.astype(int), minlength=2)\n",
    "        \n",
    "        # Normalize to same total\n",
    "        ref_dist = ref_dist / ref_dist.sum()\n",
    "        new_dist = new_dist * len(reference_predictions)\n",
    "        \n",
    "        statistic, p_value = stats.chisquare(new_dist, ref_dist * len(new_predictions))\n",
    "        \n",
    "        return {\n",
    "            'drift_detected': p_value < alpha,\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'reference_dist': ref_dist,\n",
    "            'new_dist': new_dist / len(new_predictions)\n",
    "        }\n",
    "\n",
    "print(\"\u2713 ConceptDriftDetector class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with concept drift\n",
    "print(\"Generating datasets with concept drift...\\n\")\n",
    "\n",
    "# Small concept drift\n",
    "df_concept_drift_small = create_fraud_detection_dataset(\n",
    "    n_samples=3000, fraud_rate=0.02, drift_type='concept', drift_magnitude=0.3\n",
    ")\n",
    "\n",
    "# Large concept drift\n",
    "df_concept_drift_large = create_fraud_detection_dataset(\n",
    "    n_samples=3000, fraud_rate=0.02, drift_type='concept', drift_magnitude=0.8\n",
    ")\n",
    "\n",
    "# Initialize concept drift detector\n",
    "concept_detector = ConceptDriftDetector(baseline_model, baseline_performance)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONCEPT DRIFT DETECTION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Monitor performance on different datasets\n",
    "print(\"\\n1. Baseline (No Drift):\")\n",
    "X_test_no_drift = scaler.transform(df_test[feature_cols])\n",
    "perf_no_drift = concept_detector.monitor_performance(X_test_no_drift, df_test['is_fraud'], 'No Drift')\n",
    "print(f\"   Accuracy: {perf_no_drift['accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {perf_no_drift['f1']:.4f}\")\n",
    "print(f\"   ROC-AUC:  {perf_no_drift['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n2. Small Concept Drift:\")\n",
    "X_concept_drift_small = scaler.transform(df_concept_drift_small[feature_cols])\n",
    "perf_small = concept_detector.monitor_performance(\n",
    "    X_concept_drift_small, df_concept_drift_small['is_fraud'], 'Small Concept Drift'\n",
    ")\n",
    "print(f\"   Accuracy: {perf_small['accuracy']:.4f} (\u0394: {perf_no_drift['accuracy'] - perf_small['accuracy']:+.4f})\")\n",
    "print(f\"   F1-Score: {perf_small['f1']:.4f} (\u0394: {perf_no_drift['f1'] - perf_small['f1']:+.4f})\")\n",
    "print(f\"   ROC-AUC:  {perf_small['roc_auc']:.4f} (\u0394: {perf_no_drift['roc_auc'] - perf_small['roc_auc']:+.4f})\")\n",
    "\n",
    "print(\"\\n3. Large Concept Drift:\")\n",
    "X_concept_drift_large = scaler.transform(df_concept_drift_large[feature_cols])\n",
    "perf_large = concept_detector.monitor_performance(\n",
    "    X_concept_drift_large, df_concept_drift_large['is_fraud'], 'Large Concept Drift'\n",
    ")\n",
    "print(f\"   Accuracy: {perf_large['accuracy']:.4f} (\u0394: {perf_no_drift['accuracy'] - perf_large['accuracy']:+.4f})\")\n",
    "print(f\"   F1-Score: {perf_large['f1']:.4f} (\u0394: {perf_no_drift['f1'] - perf_large['f1']:+.4f})\")\n",
    "print(f\"   ROC-AUC:  {perf_large['roc_auc']:.4f} (\u0394: {perf_no_drift['roc_auc'] - perf_large['roc_auc']:+.4f})\")\n",
    "\n",
    "# Detect degradation\n",
    "print(\"\\n4. Drift Detection (Threshold = 5%):\")\n",
    "degradation_result = concept_detector.detect_performance_degradation(threshold=0.05)\n",
    "print(f\"   Drift detected: {degradation_result['drift_detected']}\")\n",
    "\n",
    "if degradation_result['drift_detected']:\n",
    "    print(\"\\n   Degraded metrics:\")\n",
    "    for metric, values in degradation_result['metrics'].items():\n",
    "        if values['significant']:\n",
    "            print(f\"     {metric.upper()}: {values['baseline']:.4f} \u2192 {values['current']:.4f} (\u0394: {values['drop']:.4f})\")\n",
    "\n",
    "print(\"\\n\u2713 Concept drift detection complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance degradation\n",
    "performance_df = pd.DataFrame(concept_detector.performance_history)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'f1', 'precision', 'recall']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    # Plot metric over time windows\n",
    "    axes[idx].plot(range(len(performance_df)), performance_df[metric], \n",
    "                  marker='o', markersize=8, linewidth=2, color=colors[idx])\n",
    "    \n",
    "    # Add baseline line\n",
    "    baseline_value = baseline_performance[metric]\n",
    "    axes[idx].axhline(y=baseline_value, color='green', linestyle='--', \n",
    "                     linewidth=2, label=f'Baseline: {baseline_value:.3f}', alpha=0.7)\n",
    "    \n",
    "    # Add threshold line (5% degradation)\n",
    "    threshold_value = baseline_value - 0.05\n",
    "    axes[idx].axhline(y=threshold_value, color='red', linestyle='--', \n",
    "                     linewidth=2, label=f'Threshold: {threshold_value:.3f}', alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    axes[idx].set_xlabel('Time Window', fontsize=11)\n",
    "    axes[idx].set_ylabel(metric.upper(), fontsize=11)\n",
    "    axes[idx].set_title(f'{metric.upper()} Over Time', fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_xticks(range(len(performance_df)))\n",
    "    axes[idx].set_xticklabels(performance_df['window'], rotation=45, ha='right')\n",
    "    axes[idx].legend(loc='best')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight degraded values\n",
    "    for i, value in enumerate(performance_df[metric]):\n",
    "        if value < threshold_value:\n",
    "            axes[idx].scatter(i, value, color='red', s=200, zorder=5, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Performance visualization complete\")\n",
    "print(\"\\nRed circles indicate performance below the 5% degradation threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Strategies for Model Retraining\n",
    "\n",
    "When drift is detected, we need to retrain our model. There are several strategies:\n",
    "\n",
    "### 1. Periodic Retraining\n",
    "- Retrain on a fixed schedule (daily, weekly, monthly)\n",
    "- **Pros**: Simple, predictable\n",
    "- **Cons**: May retrain when not needed, or miss sudden drift\n",
    "\n",
    "### 2. Trigger-Based Retraining\n",
    "- Retrain when drift is detected or performance drops\n",
    "- **Pros**: Resource-efficient, responds to actual drift\n",
    "- **Cons**: Requires monitoring infrastructure\n",
    "\n",
    "### 3. Incremental Learning\n",
    "- Continuously update model with new data\n",
    "- **Pros**: Always up-to-date, smooth adaptation\n",
    "- **Cons**: Requires models that support partial_fit, may forget old patterns\n",
    "\n",
    "### 4. Ensemble Approach\n",
    "- Maintain multiple models trained on different time windows\n",
    "- **Pros**: Robust to different types of drift\n",
    "- **Cons**: More complex, higher resource requirements\n",
    "\n",
    "Let's implement trigger-based retraining with an automated pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatedRetrainingPipeline:\n",
    "    \"\"\"\n",
    "    Automated pipeline for model monitoring and retraining.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, model_params, scaler=None, \n",
    "                 performance_threshold=0.05, psi_threshold=0.1):\n",
    "        \"\"\"\n",
    "        Initialize retraining pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_class : class\n",
    "            Model class (e.g., RandomForestClassifier)\n",
    "        model_params : dict\n",
    "            Model hyperparameters\n",
    "        scaler : transformer, optional\n",
    "            Feature scaler\n",
    "        performance_threshold : float\n",
    "            Performance drop threshold to trigger retraining\n",
    "        psi_threshold : float\n",
    "            PSI threshold to trigger retraining\n",
    "        \"\"\"\n",
    "        self.model_class = model_class\n",
    "        self.model_params = model_params\n",
    "        self.scaler = scaler\n",
    "        self.performance_threshold = performance_threshold\n",
    "        self.psi_threshold = psi_threshold\n",
    "        \n",
    "        self.current_model = None\n",
    "        self.baseline_performance = None\n",
    "        self.reference_data = None\n",
    "        self.retraining_history = []\n",
    "    \n",
    "    def initial_training(self, X_train, y_train, X_val, y_val, feature_names=None):\n",
    "        \"\"\"\n",
    "        Perform initial model training.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : array-like\n",
    "            Training features\n",
    "        y_train : array-like\n",
    "            Training labels\n",
    "        X_val : array-like\n",
    "            Validation features\n",
    "        y_val : array-like\n",
    "            Validation labels\n",
    "        feature_names : list, optional\n",
    "            Feature names\n",
    "        \"\"\"\n",
    "        print(\"Performing initial training...\")\n",
    "        \n",
    "        # Scale features if scaler provided\n",
    "        if self.scaler:\n",
    "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "            X_val_scaled = self.scaler.transform(X_val)\n",
    "        else:\n",
    "            X_train_scaled = X_train\n",
    "            X_val_scaled = X_val\n",
    "        \n",
    "        # Train model\n",
    "        self.current_model = self.model_class(**self.model_params)\n",
    "        self.current_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate and store baseline\n",
    "        y_pred = self.current_model.predict(X_val_scaled)\n",
    "        y_pred_proba = self.current_model.predict_proba(X_val_scaled)[:, 1]\n",
    "        \n",
    "        self.baseline_performance = {\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'precision': precision_score(y_val, y_pred),\n",
    "            'recall': recall_score(y_val, y_pred),\n",
    "            'f1': f1_score(y_val, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_val, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        # Store reference data for drift detection\n",
    "        self.reference_data = X_train if not self.scaler else X_train_scaled\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Initialize detectors\n",
    "        self.data_drift_detector = DataDriftDetector(X_train, feature_names)\n",
    "        self.concept_drift_detector = ConceptDriftDetector(self.current_model, self.baseline_performance)\n",
    "        \n",
    "        print(f\"\u2713 Initial training complete\")\n",
    "        print(f\"  Baseline F1-Score: {self.baseline_performance['f1']:.4f}\")\n",
    "        print(f\"  Baseline ROC-AUC: {self.baseline_performance['roc_auc']:.4f}\")\n",
    "    \n",
    "    def check_and_retrain(self, X_new, y_new, window_name='Current'):\n",
    "        \"\"\"\n",
    "        Check for drift and retrain if necessary.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_new : array-like\n",
    "            New data features\n",
    "        y_new : array-like\n",
    "            New data labels\n",
    "        window_name : str\n",
    "            Name for this time window\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Retraining decision and results\n",
    "        \"\"\"\n",
    "        print(f\"\\nChecking drift for window: {window_name}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Scale if needed\n",
    "        X_new_scaled = self.scaler.transform(X_new) if self.scaler else X_new\n",
    "        \n",
    "        # Check data drift\n",
    "        psi_result = self.data_drift_detector.psi(X_new, threshold=self.psi_threshold)\n",
    "        data_drift_detected = psi_result['overall_drift']\n",
    "        mean_psi = psi_result['mean_psi']\n",
    "        \n",
    "        print(f\"Data Drift: {'\u26a0\ufe0f  DETECTED' if data_drift_detected else '\u2713 None'}\")\n",
    "        print(f\"  Mean PSI: {mean_psi:.4f}\")\n",
    "        \n",
    "        # Check concept drift\n",
    "        current_perf = self.concept_drift_detector.monitor_performance(X_new_scaled, y_new, window_name)\n",
    "        degradation = self.concept_drift_detector.detect_performance_degradation(self.performance_threshold)\n",
    "        concept_drift_detected = degradation['drift_detected']\n",
    "        \n",
    "        print(f\"Concept Drift: {'\u26a0\ufe0f  DETECTED' if concept_drift_detected else '\u2713 None'}\")\n",
    "        print(f\"  Current F1: {current_perf['f1']:.4f} (Baseline: {self.baseline_performance['f1']:.4f})\")\n",
    "        \n",
    "        # Decide on retraining\n",
    "        should_retrain = data_drift_detected or concept_drift_detected\n",
    "        \n",
    "        result = {\n",
    "            'window': window_name,\n",
    "            'timestamp': datetime.now(),\n",
    "            'data_drift': data_drift_detected,\n",
    "            'concept_drift': concept_drift_detected,\n",
    "            'should_retrain': should_retrain,\n",
    "            'mean_psi': mean_psi,\n",
    "            'current_performance': current_perf,\n",
    "            'retrained': False\n",
    "        }\n",
    "        \n",
    "        if should_retrain:\n",
    "            print(\"\\n\ud83d\udd04 Retraining triggered!\")\n",
    "            result['retrained'] = True\n",
    "            result['retraining_result'] = self._retrain(X_new, y_new, X_new_scaled)\n",
    "        else:\n",
    "            print(\"\\n\u2713 No retraining needed\")\n",
    "        \n",
    "        self.retraining_history.append(result)\n",
    "        return result\n",
    "    \n",
    "    def _retrain(self, X_new, y_new, X_new_scaled):\n",
    "        \"\"\"\n",
    "        Perform model retraining.\n",
    "        \"\"\"\n",
    "        # Split new data for training/validation\n",
    "        X_retrain, X_val, y_retrain, y_val = train_test_split(\n",
    "            X_new_scaled, y_new, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train new model\n",
    "        new_model = self.model_class(**self.model_params)\n",
    "        new_model.fit(X_retrain, y_retrain)\n",
    "        \n",
    "        # Evaluate new model\n",
    "        y_pred = new_model.predict(X_val)\n",
    "        y_pred_proba = new_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        new_performance = {\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'precision': precision_score(y_val, y_pred),\n",
    "            'recall': recall_score(y_val, y_pred),\n",
    "            'f1': f1_score(y_val, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_val, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        # Compare with current model\n",
    "        y_pred_current = self.current_model.predict(X_val)\n",
    "        current_f1 = f1_score(y_val, y_pred_current)\n",
    "        \n",
    "        # Update model if new one is better\n",
    "        if new_performance['f1'] >= current_f1:\n",
    "            self.current_model = new_model\n",
    "            self.baseline_performance = new_performance\n",
    "            \n",
    "            # Update reference data for drift detection\n",
    "            if self.scaler:\n",
    "                X_new_unscaled = X_new\n",
    "            else:\n",
    "                X_new_unscaled = X_new_scaled\n",
    "            self.data_drift_detector = DataDriftDetector(X_new_unscaled, self.feature_names)\n",
    "            self.concept_drift_detector = ConceptDriftDetector(self.current_model, new_performance)\n",
    "            \n",
    "            print(f\"  \u2713 Model updated!\")\n",
    "            print(f\"  New F1: {new_performance['f1']:.4f} (Previous: {current_f1:.4f})\")\n",
    "            model_updated = True\n",
    "        else:\n",
    "            print(f\"  \u26a0\ufe0f  New model not better, keeping current model\")\n",
    "            print(f\"  New F1: {new_performance['f1']:.4f} (Current: {current_f1:.4f})\")\n",
    "            model_updated = False\n",
    "        \n",
    "        return {\n",
    "            'new_performance': new_performance,\n",
    "            'previous_performance': {'f1': current_f1},\n",
    "            'model_updated': model_updated\n",
    "        }\n",
    "    \n",
    "    def get_retraining_summary(self):\n",
    "        \"\"\"\n",
    "        Get summary of retraining history.\n",
    "        \"\"\"\n",
    "        if not self.retraining_history:\n",
    "            return \"No retraining history available\"\n",
    "        \n",
    "        summary_df = pd.DataFrame([\n",
    "            {\n",
    "                'window': h['window'],\n",
    "                'data_drift': h['data_drift'],\n",
    "                'concept_drift': h['concept_drift'],\n",
    "                'retrained': h['retrained'],\n",
    "                'f1_score': h['current_performance']['f1'],\n",
    "                'mean_psi': h['mean_psi']\n",
    "            }\n",
    "            for h in self.retraining_history\n",
    "        ])\n",
    "        \n",
    "        return summary_df\n",
    "\n",
    "print(\"\u2713 AutomatedRetrainingPipeline class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize automated retraining pipeline\n",
    "pipeline = AutomatedRetrainingPipeline(\n",
    "    model_class=RandomForestClassifier,\n",
    "    model_params={\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 20,\n",
    "        'class_weight': 'balanced',\n",
    "        'random_state': 42\n",
    "    },\n",
    "    scaler=StandardScaler(),\n",
    "    performance_threshold=0.05,\n",
    "    psi_threshold=0.1\n",
    ")\n",
    "\n",
    "# Initial training\n",
    "print(\"=\"*80)\n",
    "print(\"AUTOMATED RETRAINING PIPELINE DEMONSTRATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStep 1: Initial Training\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "X_init_train, X_init_val, y_init_train, y_init_val = train_test_split(\n",
    "    df_train[feature_cols], df_train['is_fraud'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "pipeline.initial_training(X_init_train, y_init_train, X_init_val, y_init_val, feature_names=feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multiple time windows with varying drift\n",
    "print(\"\\n\\nStep 2: Monitoring Over Time (Simulating 6 time windows)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Window 1: No drift\n",
    "print(\"\\nWindow 1: Normal operations (no drift)\")\n",
    "df_window1 = create_fraud_detection_dataset(n_samples=2000, fraud_rate=0.02)\n",
    "result1 = pipeline.check_and_retrain(df_window1[feature_cols], df_window1['is_fraud'], 'Window 1')\n",
    "\n",
    "# Window 2: Small data drift\n",
    "print(\"\\n\\nWindow 2: Small data drift detected\")\n",
    "df_window2 = create_fraud_detection_dataset(n_samples=2000, fraud_rate=0.02, drift_type='data', drift_magnitude=0.15)\n",
    "result2 = pipeline.check_and_retrain(df_window2[feature_cols], df_window2['is_fraud'], 'Window 2')\n",
    "\n",
    "# Window 3: No significant drift\n",
    "print(\"\\n\\nWindow 3: Minor fluctuation (no significant drift)\")\n",
    "df_window3 = create_fraud_detection_dataset(n_samples=2000, fraud_rate=0.02, drift_type='data', drift_magnitude=0.05)\n",
    "result3 = pipeline.check_and_retrain(df_window3[feature_cols], df_window3['is_fraud'], 'Window 3')\n",
    "\n",
    "# Window 4: Concept drift\n",
    "print(\"\\n\\nWindow 4: Concept drift detected (fraud patterns changing)\")\n",
    "df_window4 = create_fraud_detection_dataset(n_samples=2000, fraud_rate=0.02, drift_type='concept', drift_magnitude=0.5)\n",
    "result4 = pipeline.check_and_retrain(df_window4[feature_cols], df_window4['is_fraud'], 'Window 4')\n",
    "\n",
    "# Window 5: Both drifts\n",
    "print(\"\\n\\nWindow 5: Both data and concept drift\")\n",
    "df_window5 = create_fraud_detection_dataset(n_samples=2000, fraud_rate=0.02, drift_type='data', drift_magnitude=0.4)\n",
    "# Add some concept drift by modifying patterns\n",
    "result5 = pipeline.check_and_retrain(df_window5[feature_cols], df_window5['is_fraud'], 'Window 5')\n",
    "\n",
    "# Window 6: Return to normal\n",
    "print(\"\\n\\nWindow 6: Stabilization (minimal drift after retraining)\")\n",
    "df_window6 = create_fraud_detection_dataset(n_samples=2000, fraud_rate=0.02, drift_type='data', drift_magnitude=0.1)\n",
    "result6 = pipeline.check_and_retrain(df_window6[feature_cols], df_window6['is_fraud'], 'Window 6')\n",
    "\n",
    "print(\"\\n\\n\u2713 Monitoring complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get retraining summary\n",
    "summary_df = pipeline.get_retraining_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RETRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: F1-Score over time with retraining events\n",
    "x_positions = range(len(summary_df))\n",
    "axes[0].plot(x_positions, summary_df['f1_score'], marker='o', markersize=10, \n",
    "             linewidth=2, color='#1f77b4', label='F1-Score')\n",
    "\n",
    "# Highlight retraining events\n",
    "retrain_positions = summary_df[summary_df['retrained']].index\n",
    "retrain_f1 = summary_df.loc[retrain_positions, 'f1_score']\n",
    "axes[0].scatter(retrain_positions, retrain_f1, color='red', s=300, marker='*', \n",
    "               zorder=5, label='Retraining Event', edgecolors='black', linewidths=2)\n",
    "\n",
    "# Add threshold line\n",
    "initial_f1 = summary_df.iloc[0]['f1_score']\n",
    "threshold_line = initial_f1 - 0.05\n",
    "axes[0].axhline(y=threshold_line, color='red', linestyle='--', \n",
    "               linewidth=2, alpha=0.5, label=f'Threshold: {threshold_line:.3f}')\n",
    "\n",
    "axes[0].set_xlabel('Time Window', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Model Performance Over Time with Automated Retraining', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x_positions)\n",
    "axes[0].set_xticklabels(summary_df['window'], rotation=45, ha='right')\n",
    "axes[0].legend(loc='best', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: PSI over time with drift events\n",
    "axes[1].plot(x_positions, summary_df['mean_psi'], marker='s', markersize=10, \n",
    "            linewidth=2, color='#ff7f0e', label='Mean PSI')\n",
    "\n",
    "# Highlight data drift events\n",
    "drift_positions = summary_df[summary_df['data_drift']].index\n",
    "drift_psi = summary_df.loc[drift_positions, 'mean_psi']\n",
    "axes[1].scatter(drift_positions, drift_psi, color='orange', s=200, marker='v', \n",
    "               zorder=5, label='Data Drift Detected', alpha=0.7)\n",
    "\n",
    "# Add PSI threshold\n",
    "axes[1].axhline(y=0.1, color='orange', linestyle='--', \n",
    "               linewidth=2, alpha=0.5, label='PSI Threshold: 0.1')\n",
    "axes[1].axhline(y=0.25, color='red', linestyle='--', \n",
    "               linewidth=2, alpha=0.5, label='Critical PSI: 0.25')\n",
    "\n",
    "axes[1].set_xlabel('Time Window', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Mean PSI', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Data Drift Monitoring (Population Stability Index)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x_positions)\n",
    "axes[1].set_xticklabels(summary_df['window'], rotation=45, ha='right')\n",
    "axes[1].legend(loc='best', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print insights\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total windows monitored: {len(summary_df)}\")\n",
    "print(f\"Data drift events: {summary_df['data_drift'].sum()}\")\n",
    "print(f\"Concept drift events: {summary_df['concept_drift'].sum()}\")\n",
    "print(f\"Retraining events: {summary_df['retrained'].sum()}\")\n",
    "print(f\"\\nInitial F1-Score: {summary_df.iloc[0]['f1_score']:.4f}\")\n",
    "print(f\"Final F1-Score: {summary_df.iloc[-1]['f1_score']:.4f}\")\n",
    "print(f\"Performance change: {(summary_df.iloc[-1]['f1_score'] - summary_df.iloc[0]['f1_score']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices for Production ML\n",
    "\n",
    "### Model Monitoring\n",
    "1. **Track Multiple Metrics**: Don't rely on a single metric; monitor accuracy, precision, recall, F1, and business-specific KPIs\n",
    "2. **Set Appropriate Thresholds**: Balance sensitivity vs. false alarms based on your use case\n",
    "3. **Use Multiple Drift Detection Methods**: Combine statistical tests (KS-test) with domain-specific metrics (PSI)\n",
    "\n",
    "### Retraining Strategy\n",
    "1. **Validate Before Deployment**: Always validate retrained models before replacing production models\n",
    "2. **Keep Model Versions**: Maintain a registry of model versions for rollback capability\n",
    "3. **A/B Testing**: Test new models on a subset of traffic before full deployment\n",
    "4. **Gradual Rollout**: Use canary deployments to minimize risk\n",
    "\n",
    "### Data Management\n",
    "1. **Log Everything**: Capture features, predictions, labels, and timestamps\n",
    "2. **Maintain Training Data**: Keep historical data for periodic full retraining\n",
    "3. **Handle Data Quality**: Implement data validation pipelines\n",
    "\n",
    "### Automation\n",
    "1. **Automated Monitoring**: Set up dashboards and alerts\n",
    "2. **CI/CD for ML**: Automate testing, training, and deployment\n",
    "3. **Human-in-the-Loop**: Critical decisions should still involve human review\n",
    "\n",
    "### Documentation\n",
    "1. **Model Cards**: Document model purpose, performance, limitations\n",
    "2. **Drift Reports**: Keep records of drift events and retraining decisions\n",
    "3. **Incident Response**: Have a plan for when models fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Understanding Drift**\n",
    "   - Data drift affects input distributions P(X)\n",
    "   - Concept drift affects the relationship P(Y|X)\n",
    "   - Both can degrade model performance\n",
    "\n",
    "2. **Detection Methods**\n",
    "   - Statistical tests (KS-test, Chi-square) for data drift\n",
    "   - Population Stability Index (PSI) for monitoring feature shifts\n",
    "   - Performance monitoring for concept drift\n",
    "\n",
    "3. **Retraining Strategies**\n",
    "   - Trigger-based retraining is efficient and responsive\n",
    "   - Automated pipelines reduce manual intervention\n",
    "   - Always validate before deploying retrained models\n",
    "\n",
    "4. **Production Best Practices**\n",
    "   - Monitor continuously with appropriate thresholds\n",
    "   - Maintain model versioning and rollback capability\n",
    "   - Document everything for auditability\n",
    "\n",
    "### Practical Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Modify Drift Detection Thresholds**\n",
    "   - Experiment with different PSI and performance thresholds\n",
    "   - Observe how it affects retraining frequency\n",
    "\n",
    "2. **Implement Different Retraining Strategies**\n",
    "   - Add periodic retraining (every N windows)\n",
    "   - Implement incremental learning using models that support partial_fit\n",
    "\n",
    "3. **Add More Drift Detection Methods**\n",
    "   - Implement ADWIN (Adaptive Windowing)\n",
    "   - Try Evidently AI or Alibi Detect libraries\n",
    "\n",
    "4. **Enhance the Pipeline**\n",
    "   - Add model versioning with timestamps\n",
    "   - Implement A/B testing capability\n",
    "   - Create a dashboard for monitoring\n",
    "\n",
    "5. **Apply to Your Domain**\n",
    "   - Use your own dataset\n",
    "   - Define domain-specific drift metrics\n",
    "   - Set business-relevant performance thresholds\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Evidently AI Documentation](https://docs.evidentlyai.com/)\n",
    "- [Alibi Detect Documentation](https://docs.seldon.io/projects/alibi-detect/)\n",
    "- [MLOps Best Practices](https://ml-ops.org/)\n",
    "- [Google's Rules of Machine Learning](https://developers.google.com/machine-learning/guides/rules-of-ml)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Explore the existing code in `day3/04_production_ml.py` for more advanced implementations\n",
    "2. Learn about feature stores for better data management\n",
    "3. Study model serving frameworks (TensorFlow Serving, MLflow, KFServing)\n",
    "4. Understand experiment tracking (Weights & Biases, MLflow, Comet)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now have a solid foundation in model drift detection and automated retraining. These skills are essential for maintaining robust ML systems in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}