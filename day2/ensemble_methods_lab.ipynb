{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods Lab\n",
    "\n",
    "This hands-on lab demonstrates key ensemble learning concepts using real-world data. We'll explore:\n",
    "\n",
    "1. **Using Multiple Models Together** - Combining different algorithms\n",
    "2. **Random Forests and GBTs** - Tree-based ensemble methods\n",
    "3. **Understanding Bootstrap Aggregation** - The bagging technique\n",
    "4. **Combining Heterogeneous Models** - Stacking and blending\n",
    "5. **Evaluating Ensembles of Methods** - Comprehensive performance analysis\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We'll use the **Wine Quality Dataset** from UCI Machine Learning Repository. This dataset contains physicochemical properties of Portuguese \"Vinho Verde\" wine samples, along with sensory quality ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Individual models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Ensemble methods\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier\n",
    ")\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\n",
    "\n",
    "We'll load the Wine dataset from scikit-learn, which is a well-known classification dataset perfect for demonstrating ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=wine.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(pd.Series(y).value_counts().sort_index())\n",
    "\n",
    "print(\"\\nFeature names:\")\n",
    "print(wine.feature_names)\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "pd.Series(y).value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Wine Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df.iloc[:, :4].boxplot()\n",
    "plt.title('Feature Distributions (First 4 Features)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Split the data into training and test sets, and standardize the features for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features (important for some algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Multiple Models Together\n",
    "\n",
    "Before diving into ensemble methods, let's first train several individual models to establish baselines. We'll compare their performance and then see how combining them improves results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple individual models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Store results\n",
    "individual_results = {}\n",
    "\n",
    "print(\"Training individual models...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    individual_results[name] = {\n",
    "        'model': model,\n",
    "        'test_accuracy': accuracy,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  CV Score: {cv_mean:.4f} (+/- {cv_std:.4f})\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual model performance\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(individual_results.keys()),\n",
    "    'Test Accuracy': [r['test_accuracy'] for r in individual_results.values()],\n",
    "    'CV Mean': [r['cv_mean'] for r in individual_results.values()],\n",
    "    'CV Std': [r['cv_std'] for r in individual_results.values()]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(results_df['Model'], results_df['Test Accuracy'])\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Individual Model Performance')\n",
    "plt.xlim(0.8, 1.0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.errorbar(results_df['CV Mean'], results_df['Model'], \n",
    "             xerr=results_df['CV Std'], fmt='o', markersize=8)\n",
    "plt.xlabel('Cross-Validation Score')\n",
    "plt.title('CV Performance with Standard Deviation')\n",
    "plt.xlim(0.8, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Voting Ensemble\n",
    "\n",
    "Now let's combine these models using a **Voting Classifier**. This ensemble method combines predictions from multiple models using either:\n",
    "- **Hard voting**: Each model votes for a class, and the majority wins\n",
    "- **Soft voting**: Predictions are weighted by class probabilities (usually performs better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create voting classifiers\n",
    "estimators = [(name, model) for name, model in models.items()]\n",
    "\n",
    "# Hard voting\n",
    "voting_hard = VotingClassifier(estimators=estimators, voting='hard')\n",
    "voting_hard.fit(X_train_scaled, y_train)\n",
    "y_pred_hard = voting_hard.predict(X_test_scaled)\n",
    "acc_hard = accuracy_score(y_test, y_pred_hard)\n",
    "\n",
    "# Soft voting\n",
    "voting_soft = VotingClassifier(estimators=estimators, voting='soft')\n",
    "voting_soft.fit(X_train_scaled, y_train)\n",
    "y_pred_soft = voting_soft.predict(X_test_scaled)\n",
    "acc_soft = accuracy_score(y_test, y_pred_soft)\n",
    "\n",
    "print(\"\\nVoting Ensemble Results:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hard Voting Accuracy: {acc_hard:.4f}\")\n",
    "print(f\"Soft Voting Accuracy: {acc_soft:.4f}\")\n",
    "print(\"\\nComparison with best individual model:\")\n",
    "best_individual = max(individual_results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "print(f\"Best Individual Model: {best_individual[0]}\")\n",
    "print(f\"Best Individual Accuracy: {best_individual[1]['test_accuracy']:.4f}\")\n",
    "print(f\"\\nImprovement (Soft Voting): {(acc_soft - best_individual[1]['test_accuracy']):.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Forests and Gradient Boosted Trees\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "Random Forests use **Bootstrap Aggregation (Bagging)** combined with random feature selection. Each tree is trained on a different bootstrap sample, and at each split, only a random subset of features is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest with different numbers of trees\n",
    "n_trees_list = [10, 50, 100, 200, 500]\n",
    "rf_results = []\n",
    "\n",
    "print(\"Training Random Forests with different number of trees...\\n\")\n",
    "\n",
    "for n_trees in n_trees_list:\n",
    "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = rf.score(X_train, y_train)\n",
    "    test_acc = rf.score(X_test, y_test)\n",
    "    \n",
    "    rf_results.append({\n",
    "        'n_trees': n_trees,\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"n_estimators={n_trees:3d} | Train: {train_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "\n",
    "rf_results_df = pd.DataFrame(rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Random Forest performance vs number of trees\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(rf_results_df['n_trees'], rf_results_df['train_accuracy'], \n",
    "         marker='o', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(rf_results_df['n_trees'], rf_results_df['test_accuracy'], \n",
    "         marker='s', label='Test Accuracy', linewidth=2)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest Performance vs Number of Trees')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.85, 1.05)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Performance improves with more trees initially\")\n",
    "print(\"- Returns diminish after a certain point\")\n",
    "print(\"- Random Forests are resistant to overfitting due to averaging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from Random Forest\n",
    "rf_final = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_final.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': wine.feature_names,\n",
    "    'importance': rf_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees (GBTs)\n",
    "\n",
    "Unlike Random Forests which build trees independently, Gradient Boosting builds trees **sequentially**. Each tree corrects the errors of the previous trees.\n",
    "\n",
    "We'll compare:\n",
    "- **Scikit-learn GradientBoosting**\n",
    "- **XGBoost** (eXtreme Gradient Boosting)\n",
    "- **LightGBM** (Light Gradient Boosting Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different gradient boosting implementations\n",
    "print(\"Training Gradient Boosting Models...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Scikit-learn Gradient Boosting\n",
    "gb_sklearn = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                        max_depth=3, random_state=42)\n",
    "gb_sklearn.fit(X_train, y_train)\n",
    "gb_sklearn_acc = gb_sklearn.score(X_test, y_test)\n",
    "print(f\"Scikit-learn GradientBoosting: {gb_sklearn_acc:.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                              max_depth=3, random_state=42, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_acc = xgb_model.score(X_test, y_test)\n",
    "print(f\"XGBoost:                       {xgb_acc:.4f}\")\n",
    "\n",
    "# LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                               max_depth=3, random_state=42, verbose=-1)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "lgb_acc = lgb_model.score(X_test, y_test)\n",
    "print(f\"LightGBM:                      {lgb_acc:.4f}\")\n",
    "\n",
    "# AdaBoost (another boosting variant)\n",
    "ada_model = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "ada_acc = ada_model.score(X_test, y_test)\n",
    "print(f\"AdaBoost:                      {ada_acc:.4f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Random Forest vs Gradient Boosting\n",
    "comparison_data = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Sklearn GB', 'XGBoost', 'LightGBM', 'AdaBoost'],\n",
    "    'Accuracy': [rf_final.score(X_test, y_test), gb_sklearn_acc, xgb_acc, lgb_acc, ada_acc],\n",
    "    'Type': ['Bagging', 'Boosting', 'Boosting', 'Boosting', 'Boosting']\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['blue' if t == 'Bagging' else 'orange' for t in comparison_data['Type']]\n",
    "plt.barh(comparison_data['Model'], comparison_data['Accuracy'], color=colors)\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Random Forest (Bagging) vs Gradient Boosting Methods')\n",
    "plt.xlim(0.85, 1.0)\n",
    "plt.axvline(x=0.95, color='red', linestyle='--', alpha=0.5, label='95% threshold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Understanding Bootstrap Aggregation (Bagging)\n",
    "\n",
    "Let's dive deeper into how **Bootstrap Aggregation** works:\n",
    "\n",
    "1. Create multiple bootstrap samples (random sampling with replacement)\n",
    "2. Train a model on each bootstrap sample\n",
    "3. Aggregate predictions (majority vote for classification, average for regression)\n",
    "\n",
    "We'll demonstrate this manually and compare it to scikit-learn's BaggingClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate bootstrap sampling\n",
    "n_samples = len(X_train)\n",
    "n_bootstrap = 3\n",
    "\n",
    "print(\"Demonstrating Bootstrap Sampling:\\n\")\n",
    "print(f\"Original training set size: {n_samples}\")\n",
    "print(f\"\\nCreating {n_bootstrap} bootstrap samples...\\n\")\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Create bootstrap sample (sampling with replacement)\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    unique_indices = len(np.unique(indices))\n",
    "    \n",
    "    # Out-of-bag samples (samples not selected)\n",
    "    oob_indices = set(range(n_samples)) - set(indices)\n",
    "    \n",
    "    print(f\"Bootstrap Sample {i+1}:\")\n",
    "    print(f\"  Total samples: {len(indices)}\")\n",
    "    print(f\"  Unique samples: {unique_indices} ({unique_indices/n_samples*100:.1f}%)\")\n",
    "    print(f\"  Out-of-bag samples: {len(oob_indices)} ({len(oob_indices)/n_samples*100:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "print(\"Key Insight: Each bootstrap sample uses ~63.2% unique samples\")\n",
    "print(\"The remaining ~36.8% are out-of-bag (OOB) samples used for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base model vs Bagging ensemble\n",
    "print(\"\\nComparing Single Model vs Bagging Ensemble:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Single Decision Tree\n",
    "single_tree = DecisionTreeClassifier(random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "single_tree_acc = single_tree.score(X_test, y_test)\n",
    "print(f\"Single Decision Tree: {single_tree_acc:.4f}\")\n",
    "\n",
    "# Bagging with different numbers of estimators\n",
    "n_estimators_list = [10, 50, 100, 200]\n",
    "bagging_results = []\n",
    "\n",
    "for n_est in n_estimators_list:\n",
    "    bagging = BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(),\n",
    "        n_estimators=n_est,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    bagging.fit(X_train, y_train)\n",
    "    bagging_acc = bagging.score(X_test, y_test)\n",
    "    bagging_results.append({'n_estimators': n_est, 'accuracy': bagging_acc})\n",
    "    print(f\"Bagging ({n_est:3d} trees): {bagging_acc:.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "improvement = bagging_results[-1]['accuracy'] - single_tree_acc\n",
    "print(f\"\\nImprovement from Bagging: {improvement:.4f} ({improvement/single_tree_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of ensemble size\n",
    "bagging_df = pd.DataFrame(bagging_results)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(bagging_df['n_estimators'], bagging_df['accuracy'], \n",
    "         marker='o', linewidth=2, markersize=8, label='Bagging')\n",
    "plt.axhline(y=single_tree_acc, color='red', linestyle='--', \n",
    "            label='Single Tree', linewidth=2)\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Bagging Performance vs Ensemble Size')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag (OOB) Evaluation\n",
    "\n",
    "One of the benefits of bagging is **OOB evaluation** - we can estimate model performance without a separate validation set using the samples that weren't selected in each bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate OOB scoring\n",
    "bagging_oob = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagging_oob.fit(X_train, y_train)\n",
    "\n",
    "print(\"Out-of-Bag Evaluation:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"OOB Score (internal validation): {bagging_oob.oob_score_:.4f}\")\n",
    "print(f\"Test Score:                      {bagging_oob.score(X_test, y_test):.4f}\")\n",
    "print(\"\\nThe OOB score provides an unbiased estimate without needing a separate\")\n",
    "print(\"validation set, saving data for training!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Combining Heterogeneous Models (Stacking)\n",
    "\n",
    "**Stacking** is an advanced ensemble technique that combines different types of models:\n",
    "\n",
    "1. Train multiple diverse base models (level 0)\n",
    "2. Use their predictions as features for a meta-model (level 1)\n",
    "3. The meta-model learns how to best combine the base model predictions\n",
    "\n",
    "This is different from voting, which uses a fixed combination rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models (diverse set of algorithms)\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', SVC(probability=True, random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "]\n",
    "\n",
    "# Define meta-model (final estimator)\n",
    "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5  # Use cross-validation to generate meta-features\n",
    ")\n",
    "\n",
    "print(\"Training Stacking Ensemble...\\n\")\n",
    "stacking.fit(X_train_scaled, y_train)\n",
    "stacking_acc = stacking.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Stacking Results:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nBase Models Performance:\")\n",
    "for name, model in base_models:\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    acc = model.score(X_test_scaled, y_test)\n",
    "    print(f\"  {name:3s}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nStacking Ensemble: {stacking_acc:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different ensemble strategies\n",
    "ensemble_comparison = pd.DataFrame({\n",
    "    'Method': ['Voting (Hard)', 'Voting (Soft)', 'Bagging', 'Random Forest', \n",
    "               'Gradient Boosting', 'Stacking'],\n",
    "    'Accuracy': [\n",
    "        acc_hard,\n",
    "        acc_soft,\n",
    "        bagging_oob.score(X_test, y_test),\n",
    "        rf_final.score(X_test, y_test),\n",
    "        xgb_acc,\n",
    "        stacking_acc\n",
    "    ],\n",
    "    'Strategy': ['Voting', 'Voting', 'Bagging', 'Bagging', 'Boosting', 'Stacking']\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = {'Voting': 'skyblue', 'Bagging': 'lightgreen', \n",
    "          'Boosting': 'orange', 'Stacking': 'purple'}\n",
    "bar_colors = [colors[s] for s in ensemble_comparison['Strategy']]\n",
    "\n",
    "plt.barh(ensemble_comparison['Method'], ensemble_comparison['Accuracy'], color=bar_colors)\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Comparison of Different Ensemble Strategies')\n",
    "plt.xlim(0.85, 1.0)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=strategy) \n",
    "                   for strategy, color in colors.items()]\n",
    "plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEnsemble Method Comparison:\")\n",
    "print(ensemble_comparison.sort_values('Accuracy', ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluating Ensembles of Methods\n",
    "\n",
    "Let's perform a comprehensive evaluation of our best ensemble models using multiple metrics:\n",
    "- Accuracy\n",
    "- Precision, Recall, F1-score\n",
    "- Confusion Matrix\n",
    "- Cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best models for detailed evaluation\n",
    "best_models = {\n",
    "    'Random Forest': rf_final,\n",
    "    'XGBoost': xgb_model,\n",
    "    'Stacking': stacking,\n",
    "    'Voting (Soft)': voting_soft\n",
    "}\n",
    "\n",
    "# Generate predictions for each model\n",
    "print(\"Detailed Classification Reports:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Use scaled or unscaled data based on model type\n",
    "    if name in ['Stacking', 'Voting (Soft)']:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=wine.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, model) in enumerate(best_models.items()):\n",
    "    # Use scaled or unscaled data based on model type\n",
    "    if name in ['Stacking', 'Voting (Soft)']:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=wine.target_names,\n",
    "                yticklabels=wine.target_names)\n",
    "    axes[idx].set_title(f'{name} Confusion Matrix')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation comparison\n",
    "print(\"\\nCross-Validation Performance Comparison:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    # Use scaled or unscaled data based on model type\n",
    "    if name in ['Stacking', 'Voting (Soft)']:\n",
    "        X_cv = X_train_scaled\n",
    "    else:\n",
    "        X_cv = X_train\n",
    "    \n",
    "    scores = cross_val_score(model, X_cv, y_train, cv=5, scoring='accuracy')\n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'Mean CV Score': scores.mean(),\n",
    "        'Std CV Score': scores.std(),\n",
    "        'Min Score': scores.min(),\n",
    "        'Max Score': scores.max()\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "    print(f\"  Range: [{scores.min():.4f}, {scores.max():.4f}]\")\n",
    "    print()\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV performance with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(cv_results_df['Mean CV Score'], cv_results_df['Model'],\n",
    "             xerr=cv_results_df['Std CV Score'], fmt='o', markersize=10,\n",
    "             capsize=5, capthick=2, linewidth=2)\n",
    "plt.xlabel('Cross-Validation Score')\n",
    "plt.title('Cross-Validation Performance Comparison (with Standard Deviation)')\n",
    "plt.xlim(0.90, 1.0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Performance Summary\n",
    "\n",
    "Let's create a comprehensive summary of all ensemble methods we've explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary table\n",
    "final_summary = pd.DataFrame({\n",
    "    'Ensemble Method': [\n",
    "        'Single Decision Tree (Baseline)',\n",
    "        'Voting - Hard',\n",
    "        'Voting - Soft',\n",
    "        'Bagging (100 trees)',\n",
    "        'Random Forest (200 trees)',\n",
    "        'Gradient Boosting (sklearn)',\n",
    "        'XGBoost',\n",
    "        'LightGBM',\n",
    "        'AdaBoost',\n",
    "        'Stacking'\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        single_tree_acc,\n",
    "        acc_hard,\n",
    "        acc_soft,\n",
    "        bagging_oob.score(X_test, y_test),\n",
    "        rf_final.score(X_test, y_test),\n",
    "        gb_sklearn_acc,\n",
    "        xgb_acc,\n",
    "        lgb_acc,\n",
    "        ada_acc,\n",
    "        stacking_acc\n",
    "    ],\n",
    "    'Category': [\n",
    "        'Baseline',\n",
    "        'Voting',\n",
    "        'Voting',\n",
    "        'Bagging',\n",
    "        'Bagging',\n",
    "        'Boosting',\n",
    "        'Boosting',\n",
    "        'Boosting',\n",
    "        'Boosting',\n",
    "        'Stacking'\n",
    "    ]\n",
    "})\n",
    "\n",
    "final_summary = final_summary.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(final_summary.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "best_model = final_summary.iloc[0]\n",
    "improvement = (best_model['Test Accuracy'] - single_tree_acc) / single_tree_acc * 100\n",
    "print(f\"\\nBest Model: {best_model['Ensemble Method']}\")\n",
    "print(f\"Improvement over baseline: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "category_colors = {\n",
    "    'Baseline': 'gray',\n",
    "    'Voting': 'skyblue',\n",
    "    'Bagging': 'lightgreen',\n",
    "    'Boosting': 'orange',\n",
    "    'Stacking': 'purple'\n",
    "}\n",
    "\n",
    "colors = [category_colors[cat] for cat in final_summary['Category']]\n",
    "\n",
    "plt.barh(final_summary['Ensemble Method'], final_summary['Test Accuracy'], color=colors)\n",
    "plt.xlabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Comprehensive Ensemble Methods Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlim(0.85, 1.0)\n",
    "plt.axvline(x=single_tree_acc, color='red', linestyle='--', linewidth=2, label='Baseline', alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for idx, row in final_summary.iterrows():\n",
    "    plt.text(row['Test Accuracy'], row['Ensemble Method'], \n",
    "             f\" {row['Test Accuracy']:.4f}\", \n",
    "             va='center', fontsize=9)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=category) \n",
    "                   for category, color in category_colors.items()]\n",
    "plt.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Key Takeaways\n",
    "\n",
    "## Ensemble Methods Overview\n",
    "\n",
    "### 1. **Using Multiple Models Together**\n",
    "- Combining diverse models often performs better than individual models\n",
    "- Voting ensembles are simple but effective\n",
    "- Soft voting (using probabilities) typically outperforms hard voting\n",
    "\n",
    "### 2. **Random Forests and GBTs**\n",
    "- **Random Forests** (Bagging):\n",
    "  - Build trees independently in parallel\n",
    "  - Reduce variance through averaging\n",
    "  - Resistant to overfitting\n",
    "  - Good default choice for many problems\n",
    "  \n",
    "- **Gradient Boosted Trees** (Boosting):\n",
    "  - Build trees sequentially\n",
    "  - Each tree corrects previous errors\n",
    "  - Often achieve highest accuracy\n",
    "  - More prone to overfitting than Random Forests\n",
    "  - XGBoost and LightGBM offer optimized implementations\n",
    "\n",
    "### 3. **Bootstrap Aggregation (Bagging)**\n",
    "- Creates diverse training sets through random sampling with replacement\n",
    "- Each bootstrap sample uses ~63.2% unique samples\n",
    "- Out-of-bag (OOB) samples provide free validation\n",
    "- Reduces variance and improves stability\n",
    "\n",
    "### 4. **Combining Heterogeneous Models**\n",
    "- **Stacking** uses a meta-model to learn optimal combinations\n",
    "- More sophisticated than simple voting\n",
    "- Can capture complementary strengths of different algorithms\n",
    "- Requires more computational resources\n",
    "\n",
    "### 5. **Evaluating Ensembles**\n",
    "- Always use cross-validation for robust performance estimates\n",
    "- Consider multiple metrics (accuracy, precision, recall, F1)\n",
    "- Examine confusion matrices for detailed error analysis\n",
    "- Balance performance with computational cost\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Start Simple**: Begin with Random Forest as a strong baseline\n",
    "2. **Try Boosting**: Gradient boosting often gives the best performance\n",
    "3. **Diversify**: Use different types of models in voting/stacking\n",
    "4. **Validate Properly**: Use cross-validation to avoid overfitting\n",
    "5. **Monitor Complexity**: More complex ensembles aren't always better\n",
    "\n",
    "## When to Use Each Method\n",
    "\n",
    "- **Random Forest**: Good default choice, handles non-linear relationships well\n",
    "- **Gradient Boosting**: When you need maximum accuracy and have time to tune\n",
    "- **Bagging**: When you have a high-variance model to stabilize\n",
    "- **Voting**: Quick ensemble of pre-trained models\n",
    "- **Stacking**: When you have diverse models and want optimal combination"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
