{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning a Language Model for Healthcare Question Answering\n",
        "\n",
        "**A practical guide for students using Ollama and Hugging Face.**\n",
        "\n",
        "This notebook provides a comprehensive, step-by-step tutorial on fine-tuning a smaller language model for a specific domain. We will focus on the healthcare sector and create a question-answering model capable of responding to medical queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Fine-Tuning\n",
        "\n",
        "### What is Fine-Tuning?\n",
        "\n",
        "Large Language Models (LLMs) are pre-trained on vast amounts of general text data, giving them a broad understanding of language. However, for specialized tasks or domains, their performance can be significantly improved by **fine-tuning**. Fine-tuning is the process of taking a pre-trained model and continuing its training on a smaller, task-specific dataset. This adapts the model's knowledge and capabilities to the new domain, leading to more accurate and relevant outputs.\n",
        "\n",
        "### Why is Fine-Tuning Important?\n",
        "\n",
        "- **Domain Adaptation:** Fine-tuning allows a general-purpose model to learn the specific jargon, entities, and relationships of a particular field, such as medicine, law, or finance.\n",
        "- **Improved Performance:** A fine-tuned model will outperform a general model on tasks within its specialized domain.\n",
        "- **Cost and Time Efficiency:** Training a model from scratch is computationally expensive and time-consuming. Fine-tuning offers a more efficient way to achieve high performance on specific tasks.\n",
        "\n",
        "### Parameter-Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "Fine-tuning a full LLM can still be resource-intensive. **Parameter-Efficient Fine-Tuning (PEFT)** methods address this by only updating a small subset of the model's parameters. This significantly reduces the computational and storage costs while achieving performance comparable to full fine-tuning. We will be using **Low-Rank Adaptation (LoRA)**, a popular PEFT technique, in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding LoRA (Low-Rank Adaptation)\n",
        "\n",
        "### How LoRA Works\n",
        "\n",
        "LoRA works by adding small, trainable rank decomposition matrices to the existing model weights. Instead of updating all the weights in the model, LoRA only trains these small matrices, which are then added to the frozen pre-trained weights during inference.\n",
        "\n",
        "**Key LoRA Parameters:**\n",
        "\n",
        "- **r (rank):** The dimension of the low-rank matrices. Lower values mean fewer trainable parameters. Typical values: 8, 16, 32.\n",
        "- **lora_alpha:** Scaling factor for the LoRA weights. Usually set to 2x the rank value.\n",
        "- **lora_dropout:** Dropout probability for LoRA layers to prevent overfitting.\n",
        "\n",
        "**Benefits:**\n",
        "- Only 0.1-1% of parameters need to be trained\n",
        "- Significantly reduced memory requirements\n",
        "- Faster training times\n",
        "- Small adapter files (often just a few MB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setup and Installation\n",
        "\n",
        "First, let's install the necessary Python libraries. We will need `transformers` for loading the model, `datasets` for loading our data, `peft` for the LoRA implementation, `trl` for the training loop, and `bitsandbytes` for quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Loading the Dataset\n",
        "\n",
        "We will use the `MedQuad` dataset from Hugging Face, which contains medical question-answer pairs. This dataset has 16,407 examples covering various medical topics.\n",
        "\n",
        "**Dataset Structure:**\n",
        "- `qtype`: Type of question (symptoms, treatment, prevention, etc.)\n",
        "- `Question`: The medical question\n",
        "- `Answer`: The detailed answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the MedQuad dataset\n",
        "dataset_name = \"keivalya/MedQuad-MedicalQnADataset\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)} examples\")\n",
        "print(\"\\nFirst example:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Preprocessing\n",
        "\n",
        "To prepare the data for fine-tuning, we need to format it into a prompt template that the model can understand. A good prompt helps the model learn the desired input-output structure. We will create a simple prompt that clearly separates the question from the answer.\n",
        "\n",
        "We'll also take a smaller subset of the data for this demonstration to speed up training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt(sample):\n",
        "    \"\"\"Format a sample into a training prompt.\"\"\"\n",
        "    prompt = f\"\"\"### Question:\n",
        "{sample['Question']}\n",
        "\n",
        "### Answer:\n",
        "{sample['Answer']}\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Let's see what a formatted prompt looks like\n",
        "print(\"Example formatted prompt:\")\n",
        "print(create_prompt(dataset[0]))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# For this demo, we'll use a subset of the data (first 1000 examples)\n",
        "# In practice, you would use the full dataset\n",
        "train_dataset = dataset.select(range(min(1000, len(dataset))))\n",
        "print(f\"Training on {len(train_dataset)} examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Loading and Configuration\n",
        "\n",
        "Now, let's load our base model. We will use `TinyLlama`, a small but powerful model perfect for demonstrations on consumer hardware. We will also use 4-bit quantization to further reduce the memory footprint.\n",
        "\n",
        "### Quantization\n",
        "\n",
        "**4-bit quantization** reduces the precision of model weights from 32-bit or 16-bit floating point to 4-bit integers. This dramatically reduces memory usage (up to 75% reduction) with minimal impact on model quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load the base model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(\"Base model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Configuring LoRA\n",
        "\n",
        "Now we'll configure the LoRA parameters and apply them to our model. This creates the trainable adapter layers that will be fine-tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                      # Rank of the low-rank matrices\n",
        "    lora_alpha=32,             # Scaling factor (usually 2x rank)\n",
        "    lora_dropout=0.05,         # Dropout for regularization\n",
        "    bias=\"none\",               # Don't train bias parameters\n",
        "    task_type=\"CAUSAL_LM\",     # Task type: Causal Language Modeling\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers to apply LoRA to\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that only a small percentage of parameters are trainable! This is the power of PEFT - we're only training a tiny fraction of the model while still achieving good results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training the Model\n",
        "\n",
        "We will use the `SFTTrainer` (Supervised Fine-Tuning Trainer) from the `trl` library to perform the fine-tuning. This trainer simplifies the training process and is optimized for supervised fine-tuning tasks.\n",
        "\n",
        "### Training Parameters:\n",
        "\n",
        "- **num_train_epochs:** Number of times to iterate over the entire dataset\n",
        "- **per_device_train_batch_size:** Number of samples processed at once\n",
        "- **learning_rate:** How quickly the model updates its weights\n",
        "- **max_seq_length:** Maximum length of input sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        ")\n",
        "\n",
        "# Format the dataset for training\n",
        "def formatting_func(example):\n",
        "    return create_prompt(example)\n",
        "\n",
        "# Create the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=lora_config,\n",
        "    formatting_func=formatting_func,\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Testing the Fine-Tuned Model\n",
        "\n",
        "Let's test our fine-tuned model with a medical question to see how it performs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_question(question):\n",
        "    \"\"\"Ask the fine-tuned model a medical question.\"\"\"\n",
        "    prompt = f\"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\n",
        "\"\"\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the answer part\n",
        "    answer = response.split(\"### Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "# Test with a sample question\n",
        "test_question = \"What are the symptoms of diabetes?\"\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"\\nAnswer: {ask_question(test_question)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Saving the Model\n",
        "\n",
        "After training, we save the LoRA adapters. These are small files (typically just a few MB) that contain only the trained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the LoRA adapters\n",
        "output_dir = \"tinyllama-medical-qa-lora\"\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model saved to {output_dir}\")\n",
        "print(\"\\nYou can now load this model using:\")\n",
        "print(\"from peft import PeftModel\")\n",
        "print(f\"model = PeftModel.from_pretrained(base_model, '{output_dir}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Merging and Exporting for Ollama\n",
        "\n",
        "To use the model with Ollama, we need to merge the LoRA adapters with the base model and convert it to GGUF format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Load base model without quantization for merging\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load and merge the LoRA weights\n",
        "merged_model = PeftModel.from_pretrained(base_model, output_dir)\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "# Save the merged model\n",
        "merged_output_dir = \"tinyllama-medical-qa-merged\"\n",
        "merged_model.save_pretrained(merged_output_dir)\n",
        "tokenizer.save_pretrained(merged_output_dir)\n",
        "\n",
        "print(f\"Merged model saved to {merged_output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Converting to GGUF and Using with Ollama\n",
        "\n",
        "To use the model with Ollama, follow these steps in your terminal:\n",
        "\n",
        "### Step 1: Install llama.cpp\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/ggerganov/llama.cpp.git\n",
        "cd llama.cpp\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "### Step 2: Convert to GGUF\n",
        "\n",
        "```bash\n",
        "python convert.py /path/to/tinyllama-medical-qa-merged \\\n",
        "  --outfile tinyllama-medical-qa.gguf \\\n",
        "  --outtype f16\n",
        "```\n",
        "\n",
        "### Step 3: Create a Modelfile\n",
        "\n",
        "Create a file named `Modelfile` with this content:\n",
        "\n",
        "```\n",
        "FROM ./tinyllama-medical-qa.gguf\n",
        "\n",
        "TEMPLATE \"\"\"### Question:\n",
        "{{ .Prompt }}\n",
        "\n",
        "### Answer:\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM \"You are a helpful medical assistant trained to answer healthcare questions.\"\n",
        "\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER stop \"### Question:\"\n",
        "```\n",
        "\n",
        "### Step 4: Create and Run in Ollama\n",
        "\n",
        "```bash\n",
        "# Create the model in Ollama\n",
        "ollama create tinyllama-medical-qa -f Modelfile\n",
        "\n",
        "# Run the model\n",
        "ollama run tinyllama-medical-qa\n",
        "```\n",
        "\n",
        "### Step 5: Test Your Model\n",
        "\n",
        "```bash\n",
        "ollama run tinyllama-medical-qa \"What are the symptoms of diabetes?\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Summary and Key Takeaways\n",
        "\n",
        "In this notebook, we successfully fine-tuned a small language model for healthcare question answering. Here are the key concepts we covered:\n",
        "\n",
        "### What We Learned:\n",
        "\n",
        "1. **Fine-Tuning Fundamentals:** Fine-tuning adapts a pre-trained model to a specific domain or task, improving performance without training from scratch.\n",
        "\n",
        "2. **Parameter-Efficient Fine-Tuning (PEFT):** Methods like LoRA allow us to fine-tune models by training only a small fraction of parameters, making it feasible on consumer hardware.\n",
        "\n",
        "3. **LoRA (Low-Rank Adaptation):** Adds small trainable matrices to frozen model weights, achieving comparable performance to full fine-tuning with minimal resources.\n",
        "\n",
        "4. **Quantization:** 4-bit quantization reduces memory requirements by up to 75%, enabling larger models to run on limited hardware.\n",
        "\n",
        "5. **Practical Workflow:**\n",
        "   - Load dataset and preprocess with prompt templates\n",
        "   - Load base model with quantization\n",
        "   - Configure and apply LoRA\n",
        "   - Train using SFTTrainer\n",
        "   - Save, merge, and export for deployment\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- **Experiment with different datasets:** Try fine-tuning on other domains like legal, financial, or technical documentation.\n",
        "- **Adjust hyperparameters:** Experiment with different LoRA ranks, learning rates, and batch sizes.\n",
        "- **Use larger models:** Apply the same techniques to larger models like Llama 2 7B or Mistral 7B.\n",
        "- **Evaluate performance:** Create test sets and measure accuracy, relevance, and factual correctness.\n",
        "\n",
        "### Resources:\n",
        "\n",
        "- [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft)\n",
        "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
        "- [Ollama Documentation](https://ollama.ai)\n",
        "- [TRL Library](https://huggingface.co/docs/trl)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}