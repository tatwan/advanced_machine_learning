{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-engine: Essential Transformations for ML Students\n",
    "\n",
    "This notebook demonstrates 8 important feature-engine transformations that every machine learning student should know. We use the Titanic dataset (via seaborn) to illustrate transformations on both numerical and categorical variables. Each transformation includes a short explanation and a runnable example.\n",
    "\n",
    "Transformations covered:\n",
    "1. Mean/Median imputation for numerical variables (MeanMedianImputer)\n",
    "2. Categorical imputation for missing categorical values (CategoricalImputer)\n",
    "3. Rare label encoding to group infrequent categories (RareLabelEncoder)\n",
    "4. One-hot encoding for categorical expansion (OneHotEncoder)\n",
    "5. Equal-frequency discretisation / binning (EqualFrequencyDiscretiser)\n",
    "6. Log transformation to reduce skew (LogTransformer)\n",
    "7. Scaling numerical features (StandardScaler)\n",
    "8. Outlier capping/winsorization (Winsorizer)\n",
    "\n",
    "All transformers are from the feature-engine library and are sklearn-compatible, so they can be included inside sklearn Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!uv pip install -q feature-engine seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
    "from feature_engine.encoding import RareLabelEncoder, OneHotEncoder\n",
    "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
    "from feature_engine.transformation import LogTransformer\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>deck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age     fare embarked deck\n",
       "0         0       3    male  22.0   7.2500        S  NaN\n",
       "1         1       1  female  38.0  71.2833        C    C\n",
       "2         1       3  female  26.0   7.9250        S  NaN\n",
       "3         1       1  female  35.0  53.1000        S    C\n",
       "4         0       3    male  35.0   8.0500        S  NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Titanic dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "df = df[['survived','pclass','sex','age','fare','embarked','deck']].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look: the dataset contains numerical columns (age, fare), categorical columns (sex, embarked, deck), and missing values. We'll demonstrate transformations on a subset of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Mean/Median imputation for numerical variables (MeanMedianImputer)\n",
    "\n",
    "When numerical features have missing values, replacing them with the mean or median is a simple baseline strategy. Median is robust to outliers.\n",
    "\n",
    "**Key differences from sklearn.impute.SimpleImputer:**\n",
    "- **Input**: Accepts pandas DataFrame (not numpy arrays)\n",
    "- **Output**: Returns pandas DataFrame (preserves column names and index)\n",
    "- **Variables parameter**: Specify which columns to impute; other columns pass through unchanged\n",
    "- **Flexibility**: Can impute different columns with different strategies in same pipeline\n",
    "\n",
    "**How it works:**\n",
    "1. `.fit()` - Calculates and stores the median/mean from training data\n",
    "2. `.transform()` - Replaces missing values using stored statistics\n",
    "3. Stored in `.imputer_dict_` attribute (e.g., `{'age': 28.0}`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>714.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.526497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age\n",
       "count  714.000000\n",
       "mean    29.699118\n",
       "std     14.526497\n",
       "min      0.420000\n",
       "50%     28.000000\n",
       "max     80.000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create imputer - specify which columns to impute\n",
    "num_imputer = MeanMedianImputer(\n",
    "    imputation_method='median',  # Can be 'mean' or 'median'\n",
    "    variables=['age']            # Only impute 'age', leave other columns as-is\n",
    ")\n",
    "\n",
    "# Fit on data (learns median=28.0) and transform in one step\n",
    "df_imputed = num_imputer.fit_transform(df)\n",
    "\n",
    "# Show statistics - note that 'count' increases after imputation\n",
    "df[['age']].describe().loc[['count','mean','std','min','50%','max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that missing values in 'age' were replaced by the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0), np.int64(177))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imputed['age'].isna().sum(), df['age'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Categorical imputation (CategoricalImputer)\n",
    "\n",
    "Categorical features sometimes have missing values. One option is to replace them with a string such as 'Missing' so that the model can learn from the presence of missingness.\n",
    "\n",
    "**Key points:**\n",
    "- **Input/Output**: DataFrame → DataFrame (like all feature-engine transformers)\n",
    "- **fill_value**: The string/value to replace NaN with (default='Missing')\n",
    "- **variables**: List of categorical columns to impute\n",
    "- **Why this matters**: Treating missing as a separate category can capture patterns (e.g., \"missing deck info correlates with lower fare\")\n",
    "\n",
    "**Compared to sklearn:** sklearn's SimpleImputer can do this with `strategy='constant'`, but feature-engine's version is more explicit and DataFrame-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embarked    0\n",
       "deck        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create categorical imputer\n",
    "cat_imputer = CategoricalImputer(\n",
    "    fill_value='Missing',              # Replace NaN with this string\n",
    "    variables=['embarked','deck']      # Apply to these columns only\n",
    ")\n",
    "\n",
    "# Transform data - replaces NaN with 'Missing'\n",
    "df_cat_imputed = cat_imputer.fit_transform(df)\n",
    "\n",
    "# Verify: should show 0 missing values\n",
    "df_cat_imputed[['embarked','deck']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Rare label encoding (RareLabelEncoder)\n",
    "\n",
    "Rare categories (very infrequent) may create noisy features. RareLabelEncoder groups categories that appear in a small fraction of observations into a single label (e.g., 'Rare').\n",
    "\n",
    "**Important parameters:**\n",
    "- **tol**: Minimum frequency threshold (0.05 = 5%). Categories below this → 'Rare'\n",
    "- **n_categories**: Minimum number of categories to keep. If tol would create fewer categories, keep top n_categories\n",
    "- **replace_with**: String to replace rare categories with (default='Rare')\n",
    "\n",
    "**Why use this?**\n",
    "- Reduces high-cardinality categorical features\n",
    "- Prevents overfitting to rare categories with few samples\n",
    "- Reduces dimensionality after one-hot encoding\n",
    "\n",
    "**How it learns:**\n",
    "- `.fit()` - Identifies which categories are rare based on training data frequency\n",
    "- `.transform()` - Replaces rare categories with 'Rare'\n",
    "- Stored in `.encoder_dict_` (e.g., `{'deck': ['D', 'E', 'F', 'G'], 'embarked': []}`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deck\n",
       "Missing    0.772166\n",
       "Rare       0.108866\n",
       "C          0.066218\n",
       "B          0.052750\n",
       "A          0.000000\n",
       "D          0.000000\n",
       "E          0.000000\n",
       "F          0.000000\n",
       "G          0.000000\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create rare label encoder\n",
    "rare_enc = RareLabelEncoder(\n",
    "    tol=0.05,                          # Categories appearing in <5% of rows → 'Rare'\n",
    "    n_categories=1,                    # Keep at least 1 category (plus 'Rare')\n",
    "    variables=['deck','embarked']      # Apply to these categorical columns\n",
    ")\n",
    "\n",
    "# Fit and transform - groups infrequent deck values into 'Rare'\n",
    "df_rare = rare_enc.fit_transform(df_cat_imputed)\n",
    "\n",
    "# Show frequency distribution - notice 'Rare' category consolidates infrequent values\n",
    "df_rare['deck'].value_counts(normalize=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) One-hot encoding (OneHotEncoder)\n",
    "\n",
    "Convert categorical variables into numerical columns via one-hot encoding. feature-engine's OneHotEncoder returns a DataFrame and allows dropping the last level to avoid collinearity.\n",
    "\n",
    "**Key differences from sklearn.preprocessing.OneHotEncoder:**\n",
    "- **Input/Output**: DataFrame → DataFrame (sklearn uses arrays)\n",
    "- **Column names**: Automatically creates readable names like 'sex_male', 'embarked_S'\n",
    "- **drop_last**: If True, drops one category per variable to avoid multicollinearity (important for linear models)\n",
    "- **variables**: Specify which columns to encode; unspecified columns pass through unchanged\n",
    "\n",
    "**How it works:**\n",
    "1. `.fit()` - Learns all unique categories in specified columns\n",
    "2. `.transform()` - Creates binary columns for each category, drops original categorical columns\n",
    "3. New columns: 'variable_category' format (e.g., 'sex_male', 'sex_female')\n",
    "\n",
    "**Important**: Always fit on training data only! If test data has unseen categories, they'll be ignored (all zeros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>deck</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Missing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Missing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass   age     fare     deck  sex_male  embarked_S  embarked_C  \\\n",
       "0         0       3  22.0   7.2500  Missing         1           1           0   \n",
       "1         1       1  38.0  71.2833        C         0           0           1   \n",
       "2         1       3  26.0   7.9250  Missing         0           1           0   \n",
       "3         1       1  35.0  53.1000        C         0           1           0   \n",
       "4         0       3  35.0   8.0500  Missing         1           1           0   \n",
       "\n",
       "   embarked_Q  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create one-hot encoder\n",
    "ohe = OneHotEncoder(\n",
    "    drop_last=True,                    # Drop last category to avoid dummy variable trap\n",
    "    variables=['sex','embarked']       # Encode these categorical columns\n",
    ")\n",
    "\n",
    "# Transform - creates new binary columns, removes original categorical columns\n",
    "df_ohe = ohe.fit_transform(df_rare)\n",
    "\n",
    "# Notice: 'sex' becomes 'sex_male' (female dropped), 'embarked' becomes 3 columns (one dropped)\n",
    "df_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Equal-frequency discretisation (EqualFrequencyDiscretiser)\n",
    "\n",
    "Discretisation (binning) transforms continuous variables into ordinal bins. Equal-frequency bins try to make bins with (roughly) the same number of observations.\n",
    "Binning can help models that benefit from ordinal categories or to capture non-linear relationships.\n",
    "\n",
    "**Key parameters:**\n",
    "- **q**: Number of quantiles/bins to create (e.g., q=4 creates quartiles)\n",
    "- **return_object**: \n",
    "  - `False` → Returns integers (0, 1, 2, 3) representing bin numbers\n",
    "  - `True` → Returns interval strings like '(0.0, 7.91]'\n",
    "- **variables**: Columns to discretize\n",
    "\n",
    "**Compared to sklearn.preprocessing.KBinsDiscretizer:**\n",
    "- feature-engine is DataFrame-friendly and allows specifying columns\n",
    "- sklearn works with arrays and discretizes all features\n",
    "\n",
    "**Why use discretization?**\n",
    "- Captures non-linear patterns (e.g., fare 0-10, 10-30, 30-100, 100+)\n",
    "- Makes models more robust to outliers\n",
    "- Can improve tree-based models and linear models\n",
    "\n",
    "**Fit behavior:** Learns bin edges from training data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare</th>\n",
       "      <th>fare_binned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.2833</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.1000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.4583</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51.8625</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21.0750</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.1333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30.0708</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fare  fare_binned\n",
       "0   7.2500            0\n",
       "1  71.2833            3\n",
       "2   7.9250            1\n",
       "3  53.1000            3\n",
       "4   8.0500            1\n",
       "5   8.4583            1\n",
       "6  51.8625            3\n",
       "7  21.0750            2\n",
       "8  11.1333            1\n",
       "9  30.0708            2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create equal-frequency discretiser\n",
    "disc = EqualFrequencyDiscretiser(\n",
    "    q=4,                               # Create 4 bins (quartiles)\n",
    "    variables=['fare'],                # Discretize 'fare' column\n",
    "    return_object=False                # Return bin numbers (0,1,2,3) not interval strings\n",
    ")\n",
    "\n",
    "# Transform - replaces continuous values with bin numbers\n",
    "df_disc = disc.fit_transform(df_ohe)\n",
    "\n",
    "# Show original and binned values side by side\n",
    "df_temp = df_ohe[['fare']].copy()\n",
    "df_temp['fare_binned'] = df_disc['fare']\n",
    "df_temp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: return_object=False returns bins as integers (0, 1, 2, 3) representing quartiles. Set to True if you want string representations like '(0.0, 7.91]'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Log transformation (LogTransformer)\n",
    "\n",
    "Apply a log transform to reduce skew in positive-valued variables (e.g., fare). LogTransformer requires strictly positive values. For data with zeros, use `YeoJohnsonTransformer` instead, or add a small constant before transforming.\n",
    "\n",
    "**Important constraints:**\n",
    "- **Requires**: All values must be > 0 (raises ValueError if zeros/negatives found)\n",
    "- **Input/Output**: DataFrame → DataFrame\n",
    "- **Effect**: Compresses large values, expands small values → reduces right skew\n",
    "\n",
    "**Compared to sklearn:**\n",
    "- sklearn doesn't have a direct LogTransformer\n",
    "- Use sklearn's `FunctionTransformer(np.log1p)` or feature-engine's version\n",
    "- feature-engine is more explicit and checks for invalid values\n",
    "\n",
    "**Mathematical transformation:**\n",
    "```\n",
    "new_value = log(original_value)\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Right-skewed distributions (long tail to the right)\n",
    "- Variables with exponential growth (prices, populations, web traffic)\n",
    "- To stabilize variance in regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fare has 15 zero or negative values\n",
      "\n",
      "Original fare stats:\n",
      "            fare\n",
      "mean   32.204208\n",
      "std    49.693429\n",
      "min     0.000000\n",
      "max   512.329200\n",
      "\n",
      "Log-transformed fare stats:\n",
      "          fare\n",
      "mean  2.817036\n",
      "std   1.343804\n",
      "min  -4.605170\n",
      "max   6.238987\n"
     ]
    }
   ],
   "source": [
    "# Check for zeros/negatives and handle them\n",
    "print(f\"Fare has {(df['fare'] <= 0).sum()} zero or negative values\")\n",
    "\n",
    "# Create a copy and add small constant to avoid log(0)\n",
    "df_for_log = df.copy()\n",
    "df_for_log['fare'] = df_for_log['fare'] + 0.01  # Add small constant\n",
    "\n",
    "log_t = LogTransformer(variables=['fare'])\n",
    "df_log = log_t.fit_transform(df_for_log)\n",
    "\n",
    "# Show the transformed data statistics\n",
    "print(\"\\nOriginal fare stats:\")\n",
    "print(df[['fare']].describe().loc[['mean','std','min','max']])\n",
    "print(\"\\nLog-transformed fare stats:\")\n",
    "print(df_log[['fare']].describe().loc[['mean','std','min','max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show original vs log transformed distribution (basic comparison using summary stats):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_mean</th>\n",
       "      <th>original_std</th>\n",
       "      <th>log_mean</th>\n",
       "      <th>log_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.204208</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>2.817036</td>\n",
       "      <td>1.343804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_mean  original_std  log_mean   log_std\n",
       "0      32.204208     49.693429  2.817036  1.343804"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare original vs transformed using the same data the transformer was fitted on\n",
    "orig = df_for_log['fare'].dropna()\n",
    "trans = df_log['fare'].dropna()\n",
    "\n",
    "pd.DataFrame({\n",
    "    'original_mean': df['fare'].mean(), \n",
    "    'original_std': df['fare'].std(),\n",
    "    'log_mean': trans.mean(), \n",
    "    'log_std': trans.std()\n",
    "}, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Scaling numerical features (StandardScaler)\n",
    "\n",
    "Standardisation (zero mean, unit variance) is a common preprocessing step, especially for algorithms that are distance-based or use regularisation. We use sklearn's StandardScaler which works well with feature-engine pipelines.\n",
    "\n",
    "**sklearn.preprocessing.StandardScaler:**\n",
    "- **Input**: Numpy array or DataFrame\n",
    "- **Output**: Numpy array (loses column names!)\n",
    "- **Formula**: `z = (x - mean) / std`\n",
    "- **Result**: Mean ≈ 0, Standard Deviation ≈ 1\n",
    "\n",
    "**Important for pipelines:**\n",
    "- StandardScaler returns numpy arrays, breaking DataFrame structure\n",
    "- In production pipelines, place StandardScaler as the last step\n",
    "- Or use feature-engine's `StandardScaler` wrapper to maintain DataFrames\n",
    "\n",
    "**When to use scaling:**\n",
    "- Linear regression, Logistic regression (with regularization)\n",
    "- KNN, SVM, Neural Networks (distance-based algorithms)\n",
    "- Gradient descent optimization\n",
    "- **Not needed** for tree-based models (Random Forest, XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.272780e-16</td>\n",
       "      <td>3.987333e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000562e+00</td>\n",
       "      <td>1.000562e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age          fare\n",
       "mean  2.272780e-16  3.987333e-18\n",
       "std   1.000562e+00  1.000562e+00"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First impute age (scaler can't handle NaN values)\n",
    "df_for_scaling = df.copy()\n",
    "num_imputer = MeanMedianImputer(imputation_method='median', variables=['age'])\n",
    "df_for_scaling = num_imputer.fit_transform(df_for_scaling)\n",
    "\n",
    "# Create and apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Note: StandardScaler requires numeric data without NaN\n",
    "# It learns mean and std from the data, then transforms: (x - mean) / std\n",
    "df_for_scaling[['age','fare']] = scaler.fit_transform(df_for_scaling[['age','fare']])\n",
    "\n",
    "# Verify: scaled features should have mean≈0, std≈1\n",
    "df_for_scaling[['age','fare']].describe().loc[['mean','std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34e1b5",
   "metadata": {},
   "source": [
    "## 8) Outlier capping with Winsorizer\n",
    "\n",
    "Outliers can negatively impact model performance. The Winsorizer caps extreme values at specified quantiles (e.g., 5th and 95th percentiles), which is less aggressive than removing outliers entirely.\n",
    "\n",
    "**Key parameters:**\n",
    "- **capping_method**: \n",
    "  - `'quantiles'` - Cap at percentiles (e.g., 5th and 95th)\n",
    "  - `'iqr'` - Cap using IQR method (Q1 - 1.5×IQR, Q3 + 1.5×IQR)\n",
    "  - `'gaussian'` - Cap at mean ± n×std\n",
    "- **tail**: `'both'`, `'left'`, or `'right'` - which tail(s) to cap\n",
    "- **fold**: For quantiles, the percentile (0.05 = 5th/95th percentiles)\n",
    "\n",
    "**How it works:**\n",
    "1. `.fit()` - Calculates capping values from training data (e.g., 5th and 95th percentiles)\n",
    "2. `.transform()` - Replaces values below 5th with 5th percentile, above 95th with 95th percentile\n",
    "3. Stored in `.left_tail_caps_` and `.right_tail_caps_` attributes\n",
    "\n",
    "**Why winsorize instead of removing outliers?**\n",
    "- Preserves all data points (no sample loss)\n",
    "- Less sensitive to extreme values than deletion\n",
    "- Better for production: handles outliers in new data gracefully\n",
    "\n",
    "**Bonus:** Also removes zeros/negatives from lower tail, making data safe for log transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd79de64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original fare range: 0.0 to 512.3292\n",
      "Winsorized fare range: 7.225 to 112.07915\n",
      "\n",
      "Original 95th percentile: 112.07915\n",
      "Winsorized max (capped at 95th): 112.07915\n"
     ]
    }
   ],
   "source": [
    "# Create winsorizer\n",
    "winsorizer = Winsorizer(\n",
    "    capping_method='quantiles',        # Cap at percentiles\n",
    "    tail='both',                       # Cap both high and low values\n",
    "    fold=0.05,                         # 5th and 95th percentiles (keeps middle 90%)\n",
    "    variables=['fare']                 # Apply to 'fare' column\n",
    ")\n",
    "\n",
    "# Transform - caps extreme values at 5th and 95th percentiles\n",
    "df_winsorized = winsorizer.fit_transform(df)\n",
    "\n",
    "# Compare original vs winsorized - notice min/max are now bounded\n",
    "print(\"Original fare range:\", df['fare'].min(), \"to\", df['fare'].max())\n",
    "print(\"Winsorized fare range:\", df_winsorized['fare'].min(), \"to\", df_winsorized['fare'].max())\n",
    "print(\"\\nOriginal 95th percentile:\", df['fare'].quantile(0.95))\n",
    "print(\"Winsorized max (capped at 95th):\", df_winsorized['fare'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together: a simple end-to-end Pipeline\n",
    "\n",
    "Below is a single sklearn Pipeline that chains several feature-engine transformers. Feature-engine transformers are compatible with sklearn Pipelines and operate on pandas DataFrames (returning DataFrames), which makes it convenient to build sequential transformations that act on different variables.\n",
    "\n",
    "Pipeline steps (example):\n",
    "- Impute numeric missing values (age)\n",
    "- Cap outliers in fare (removes zeros/negatives too)\n",
    "- Log transform fare (now safe after winsorization)\n",
    "- Impute categorical missing values\n",
    "- Rare label encode\n",
    "- One-hot encode selected categoricals\n",
    "- Discretise fare into equal-frequency bins (optional)\n",
    "\n",
    "Note: The Winsorizer before LogTransformer is important because it ensures fare > 0, which is required for log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>deck</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.981001</td>\n",
       "      <td>Missing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4.266662</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.070022</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.972177</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.085672</td>\n",
       "      <td>Missing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass   age      fare     deck  sex_male  embarked_S  \\\n",
       "0         0       3  22.0  1.981001  Missing         1           1   \n",
       "1         1       1  38.0  4.266662        C         0           0   \n",
       "2         1       3  26.0  2.070022  Missing         0           1   \n",
       "3         1       1  35.0  3.972177        C         0           1   \n",
       "4         0       3  35.0  2.085672  Missing         1           1   \n",
       "\n",
       "   embarked_C  embarked_Q  \n",
       "0           0           0  \n",
       "1           1           0  \n",
       "2           0           0  \n",
       "3           0           0  \n",
       "4           0           0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an end-to-end preprocessing pipeline\n",
    "# Pipeline executes transformers in sequence, passing output of each to the next\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Step 1: Impute missing numerical values (age has ~20% missing)\n",
    "    ('num_imputer', MeanMedianImputer(imputation_method='median', variables=['age'])),\n",
    "    \n",
    "    # Step 2: Cap outliers in fare (removes extreme values AND zeros/negatives)\n",
    "    ('winsorizer', Winsorizer(capping_method='quantiles', tail='both', fold=0.05, variables=['fare'])),\n",
    "    \n",
    "    # Step 3: Log transform fare (safe now - winsorizer ensured fare > 0)\n",
    "    ('log_fare', LogTransformer(variables=['fare'])),\n",
    "    \n",
    "    # Step 4: Impute missing categorical values (embarked, deck)\n",
    "    ('cat_imputer', CategoricalImputer(fill_value='Missing', variables=['embarked','deck'])),\n",
    "    \n",
    "    # Step 5: Group rare categories (reduces cardinality of 'deck')\n",
    "    ('rare_encoder', RareLabelEncoder(tol=0.05, n_categories=1, variables=['deck','embarked'])),\n",
    "    \n",
    "    # Step 6: One-hot encode categorical variables (creates binary columns)\n",
    "    ('ohe', OneHotEncoder(drop_last=True, variables=['sex','embarked'])),\n",
    "    \n",
    "    # Optional: Discretise fare to bins (uncomment if desired)\n",
    "    # ('discretiser', EqualFrequencyDiscretiser(q=4, variables=['fare'], return_object=False)),\n",
    "])\n",
    "\n",
    "# Fit the entire pipeline on training data\n",
    "# Each transformer learns its parameters sequentially\n",
    "pipeline.fit(df)\n",
    "\n",
    "# Transform data - applies all transformations in order\n",
    "df_transformed = pipeline.transform(df)\n",
    "\n",
    "# Result: Clean, encoded, ready-for-modeling DataFrame\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and teaching tips\n",
    "\n",
    "**Understanding transformers (sklearn API):**\n",
    "- All transformers follow the same pattern: `.fit()`, `.transform()`, `.fit_transform()`\n",
    "- `.fit()` - Learn parameters from training data (e.g., mean, median, categories)\n",
    "- `.transform()` - Apply learned parameters to new data\n",
    "- `.fit_transform()` - Shortcut to fit and transform in one step\n",
    "\n",
    "**Critical concept: Fit on training data only!**\n",
    "```python\n",
    "# ✅ CORRECT\n",
    "transformer.fit(X_train)           # Learn from training data\n",
    "X_train_transformed = transformer.transform(X_train)\n",
    "X_test_transformed = transformer.transform(X_test)   # Apply to test\n",
    "\n",
    "# ❌ WRONG - causes data leakage!\n",
    "transformer.fit(X)                  # Don't fit on all data\n",
    "```\n",
    "\n",
    "**feature-engine advantages over sklearn:**\n",
    "1. **DataFrame-in, DataFrame-out** - Preserves column names and structure\n",
    "2. **Selective transformation** - `variables` parameter lets you transform specific columns\n",
    "3. **Explicit and readable** - Clear what each transformer does\n",
    "4. **Pipeline-friendly** - All transformers work seamlessly in sklearn Pipelines\n",
    "\n",
    "**Inspecting learned parameters:**\n",
    "- Show students how each transformer stores state (e.g., median used for imputation, categories identified as rare)\n",
    "- Examine transformer attributes: `.imputer_dict_`, `.encoder_dict_`, `.right_tail_caps_`, etc.\n",
    "- This helps debug and understand what the model learned\n",
    "\n",
    "**Additional feature-engine transformers to explore:**\n",
    "- `ArbitraryOutlierCapper` - Custom outlier thresholds\n",
    "- `DecisionTreeDiscretiser` - Supervised binning using decision trees\n",
    "- `YeoJohnsonTransformer` - Handles both positive and negative values (unlike log)\n",
    "- `DropFeatures` - Systematic feature removal\n",
    "- `MathFeatures` - Create interaction features (sum, product, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 28.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: inspect what median was used for 'age'\n",
    "pipeline.named_steps['num_imputer'].imputer_dict_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deck': ['Missing', 'C', 'B'], 'embarked': ['S', 'C', 'Q']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: show the categories mapped to 'Rare' by the RareLabelEncoder\n",
    "pipeline.named_steps['rare_encoder'].encoder_dict_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
