{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models: Optimization and Evaluation\n",
    "\n",
    "This hands-on lab demonstrates comprehensive machine learning concepts for optimizing and evaluating linear models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will explore:\n",
    "\n",
    "### Part 1: Optimizing and Training Linear Models\n",
    "1. **Understanding Gradient Descent Optimization** - How gradient descent works and its variants\n",
    "2. **L1 and L2/ElasticNet Regularization** - Preventing overfitting with regularization techniques\n",
    "3. **Support Vector Machines** - Linear and non-linear classification with SVMs\n",
    "4. **k-Folds Cross Validation** - Robust model evaluation through cross-validation\n",
    "\n",
    "### Part 2: Evaluating Models\n",
    "5. **Metrics for Model Evaluation** - Comprehensive evaluation metrics\n",
    "6. **Hyperparameter Tuning** - Grid search and randomized search\n",
    "7. **Threshold Tuning** - Adjusting decision thresholds for business needs\n",
    "8. **Class Imbalance** - Handling imbalanced datasets\n",
    "9. **Advanced Metrics** - ROC-AUC, PR-AUC, and more\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We'll use the **Breast Cancer Wisconsin** dataset, a real-world medical dataset available in scikit-learn. This dataset contains features computed from digitized images of breast mass and is used to predict whether the mass is malignant or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, auc,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Dataset\n",
    "\n",
    "Let's load the Breast Cancer Wisconsin dataset and explore its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFeature Names:\")\n",
    "print(data.feature_names)\n",
    "print(\"\\nTarget Names:\", data.target_names)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(df['target'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['target'].value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class (0=Malignant, 1=Benign)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Split the data into training and testing sets, and standardize the features for better performance with linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features (important for gradient descent and regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train_scaled.shape}\")\n",
    "print(f\"Test set size: {X_test_scaled.shape}\")\n",
    "print(f\"\\nFeature scaling completed!\")\n",
    "print(f\"Mean of scaled features: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Std of scaled features: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Optimizing and Training Linear Models\n",
    "\n",
    "## 1.1 Understanding Gradient Descent Optimization\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function. It iteratively updates parameters in the direction of steepest descent.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Batch Gradient Descent**: Uses all training samples to compute gradients\n",
    "- **Stochastic Gradient Descent (SGD)**: Uses one sample at a time\n",
    "- **Mini-batch Gradient Descent**: Uses small batches of samples\n",
    "\n",
    "We'll use `SGDClassifier` which implements stochastic gradient descent with various loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model using Stochastic Gradient Descent\n",
    "sgd_clf = SGDClassifier(\n",
    "    loss='log_loss',  # Logistic regression loss\n",
    "    penalty='none',   # No regularization for now\n",
    "    max_iter=1000,\n",
    "    learning_rate='optimal',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "sgd_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_sgd = sgd_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"SGD Classifier Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_sgd):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_sgd):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_sgd):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_sgd):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    sgd = SGDClassifier(\n",
    "        loss='log_loss',\n",
    "        learning_rate=lr,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    sgd.fit(X_train_scaled, y_train)\n",
    "    y_pred = sgd.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results.append({'Learning Rate': lr, 'Accuracy': acc})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparison of Learning Rate Schedules:\")\n",
    "print(results_df)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(results_df['Learning Rate'], results_df['Accuracy'])\n",
    "plt.title('SGD Performance with Different Learning Rates')\n",
    "plt.xlabel('Learning Rate Schedule')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.9, 1.0])\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 L1 and L2/ElasticNet Regularization\n",
    "\n",
    "Regularization helps prevent overfitting by adding a penalty term to the cost function:\n",
    "\n",
    "- **L1 Regularization (Lasso)**: Adds sum of absolute values of coefficients. Promotes sparsity (feature selection).\n",
    "- **L2 Regularization (Ridge)**: Adds sum of squared coefficients. Shrinks coefficients smoothly.\n",
    "- **ElasticNet**: Combines L1 and L2 penalties.\n",
    "\n",
    "**Cost Function:**\n",
    "- L1: $J(\\theta) = MSE(\\theta) + \\alpha \\sum |\\theta_i|$\n",
    "- L2: $J(\\theta) = MSE(\\theta) + \\alpha \\sum \\theta_i^2$\n",
    "- ElasticNet: $J(\\theta) = MSE(\\theta) + r\\alpha \\sum |\\theta_i| + \\frac{1-r}{2}\\alpha \\sum \\theta_i^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models with different regularization\n",
    "models = {\n",
    "    'No Regularization': LogisticRegression(penalty=None, max_iter=1000, random_state=42),\n",
    "    'L1 (Lasso)': LogisticRegression(penalty='l1', C=1.0, solver='liblinear', max_iter=1000, random_state=42),\n",
    "    'L2 (Ridge)': LogisticRegression(penalty='l2', C=1.0, max_iter=1000, random_state=42),\n",
    "    'ElasticNet': LogisticRegression(penalty='elasticnet', C=1.0, l1_ratio=0.5, solver='saga', max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "regularization_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Count non-zero coefficients (for feature selection)\n",
    "    non_zero_coefs = np.sum(np.abs(model.coef_) > 0.01)\n",
    "    \n",
    "    regularization_results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'Non-zero Coefs': non_zero_coefs\n",
    "    })\n",
    "\n",
    "reg_df = pd.DataFrame(regularization_results)\n",
    "print(\"Regularization Comparison:\")\n",
    "print(reg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient magnitudes for different regularization methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    coefs = model.coef_[0]\n",
    "    axes[idx].bar(range(len(coefs)), np.abs(coefs))\n",
    "    axes[idx].set_title(f'{name}\\n(Non-zero: {np.sum(np.abs(coefs) > 0.01)})')\n",
    "    axes[idx].set_xlabel('Feature Index')\n",
    "    axes[idx].set_ylabel('Absolute Coefficient Value')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: L1 regularization produces sparser models (more zero coefficients) suitable for feature selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Support Vector Machines (SVM)\n",
    "\n",
    "SVMs find the optimal hyperplane that maximizes the margin between classes. They can handle non-linear decision boundaries using the kernel trick.\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **C**: Regularization parameter (smaller C = more regularization)\n",
    "- **kernel**: Linear, polynomial, RBF (Radial Basis Function), sigmoid\n",
    "- **gamma**: Kernel coefficient for RBF, poly, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with different kernels\n",
    "svm_kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "svm_results = []\n",
    "\n",
    "for kernel in svm_kernels:\n",
    "    svm = SVC(kernel=kernel, C=1.0, gamma='scale', random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    \n",
    "    svm_results.append({\n",
    "        'Kernel': kernel,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "svm_df = pd.DataFrame(svm_results)\n",
    "print(\"SVM Performance with Different Kernels:\")\n",
    "print(svm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SVM kernel comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(svm_df['Kernel'], svm_df['Accuracy'])\n",
    "axes[0].set_title('SVM Accuracy by Kernel Type')\n",
    "axes[0].set_xlabel('Kernel')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[1].bar(svm_df['Kernel'], svm_df['F1-Score'])\n",
    "axes[1].set_title('SVM F1-Score by Kernel Type')\n",
    "axes[1].set_xlabel('Kernel')\n",
    "axes[1].set_ylabel('F1-Score')\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 k-Folds Cross Validation\n",
    "\n",
    "Cross-validation provides a more robust estimate of model performance by:\n",
    "1. Splitting data into k folds\n",
    "2. Training on k-1 folds and validating on the remaining fold\n",
    "3. Repeating k times with different validation folds\n",
    "4. Averaging the results\n",
    "\n",
    "This helps detect overfitting and provides more reliable performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-fold cross-validation\n",
    "k_values = [3, 5, 10]\n",
    "model_for_cv = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, random_state=42)\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for k in k_values:\n",
    "    scores = cross_val_score(\n",
    "        model_for_cv, X_train_scaled, y_train, \n",
    "        cv=k, scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    cv_results.append({\n",
    "        'k': k,\n",
    "        'Mean Accuracy': scores.mean(),\n",
    "        'Std Accuracy': scores.std(),\n",
    "        'Min Accuracy': scores.min(),\n",
    "        'Max Accuracy': scores.max()\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models using cross-validation\n",
    "models_cv = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SGD Classifier': SGDClassifier(loss='log_loss', max_iter=1000, random_state=42),\n",
    "    'Linear SVM': SVC(kernel='linear', random_state=42),\n",
    "    'RBF SVM': SVC(kernel='rbf', random_state=42)\n",
    "}\n",
    "\n",
    "cv_comparison = []\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models_cv.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring='accuracy')\n",
    "    cv_comparison.append({\n",
    "        'Model': name,\n",
    "        'CV Mean': scores.mean(),\n",
    "        'CV Std': scores.std()\n",
    "    })\n",
    "\n",
    "cv_comp_df = pd.DataFrame(cv_comparison)\n",
    "print(\"\\n5-Fold Cross-Validation Model Comparison:\")\n",
    "print(cv_comp_df)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(cv_comp_df['Model'], cv_comp_df['CV Mean'], yerr=cv_comp_df['CV Std'], capsize=5)\n",
    "plt.title('Model Comparison using 5-Fold Cross-Validation')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([0.9, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Evaluating Models\n",
    "\n",
    "## 2.1 Metrics for Model Evaluation\n",
    "\n",
    "Different metrics provide different perspectives on model performance:\n",
    "\n",
    "- **Accuracy**: Overall correctness (can be misleading with imbalanced data)\n",
    "- **Precision**: Of predicted positives, how many are actually positive?\n",
    "- **Recall (Sensitivity)**: Of actual positives, how many did we catch?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Confusion Matrix**: Detailed breakdown of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a baseline model\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "y_pred = baseline_model.predict(X_test_scaled)\n",
    "y_pred_proba = baseline_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "print(\"Comprehensive Model Evaluation:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Interpretation:\")\n",
    "print(f\"True Negatives (TN):  {cm[0, 0]} - Correctly identified malignant\")\n",
    "print(f\"False Positives (FP): {cm[0, 1]} - Malignant predicted as benign (Type I error)\")\n",
    "print(f\"False Negatives (FN): {cm[1, 0]} - Benign predicted as malignant (Type II error)\")\n",
    "print(f\"True Positives (TP):  {cm[1, 1]} - Correctly identified benign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Hyperparameter Tuning\n",
    "\n",
    "Grid Search and Randomized Search systematically explore the hyperparameter space to find optimal configurations.\n",
    "\n",
    "**Grid Search**: Tests all combinations of specified parameter values.\n",
    "**Randomized Search**: Samples random combinations (more efficient for large spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Grid Search Results:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1-Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Test the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_best):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter tuning results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Create a pivot table for visualization\n",
    "pivot_data = results_df.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_C',\n",
    "    columns='param_penalty'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.4f', cmap='YlGnBu')\n",
    "plt.title('Grid Search Results: F1-Score by C and Penalty')\n",
    "plt.xlabel('Penalty Type')\n",
    "plt.ylabel('C (Regularization Strength)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Adjusting Thresholds for Business Needs\n",
    "\n",
    "The default classification threshold is 0.5, but we can adjust it based on business requirements:\n",
    "\n",
    "- **Higher threshold** (e.g., 0.7): More conservative, higher precision, lower recall\n",
    "- **Lower threshold** (e.g., 0.3): More aggressive, lower precision, higher recall\n",
    "\n",
    "In medical diagnosis, we might prefer high recall (catch all positive cases) even if it means more false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "y_pred_proba = baseline_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': threshold,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_threshold),\n",
    "        'Precision': precision_score(y_test, y_pred_threshold),\n",
    "        'Recall': recall_score(y_test, y_pred_threshold),\n",
    "        'F1-Score': f1_score(y_test, y_pred_threshold)\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "print(\"Performance at Different Thresholds:\")\n",
    "print(threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Precision and Recall\n",
    "axes[0].plot(threshold_df['Threshold'], threshold_df['Precision'], marker='o', label='Precision')\n",
    "axes[0].plot(threshold_df['Threshold'], threshold_df['Recall'], marker='s', label='Recall')\n",
    "axes[0].set_xlabel('Classification Threshold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Precision-Recall Trade-off vs Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy and F1\n",
    "axes[1].plot(threshold_df['Threshold'], threshold_df['Accuracy'], marker='o', label='Accuracy')\n",
    "axes[1].plot(threshold_df['Threshold'], threshold_df['F1-Score'], marker='s', label='F1-Score')\n",
    "axes[1].set_xlabel('Classification Threshold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Accuracy and F1-Score vs Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Handling Class Imbalance\n",
    "\n",
    "Class imbalance can bias models toward the majority class. Strategies include:\n",
    "\n",
    "1. **Class Weights**: Penalize misclassification of minority class more\n",
    "2. **Oversampling**: SMOTE (Synthetic Minority Oversampling Technique)\n",
    "3. **Undersampling**: Reduce majority class samples\n",
    "\n",
    "Let's create an imbalanced version of our dataset and demonstrate these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imbalanced dataset by removing some benign samples\n",
    "# Keep all malignant (0) and only 30% of benign (1)\n",
    "np.random.seed(42)\n",
    "malignant_idx = np.where(y_train == 0)[0]\n",
    "benign_idx = np.where(y_train == 1)[0]\n",
    "benign_sample_idx = np.random.choice(benign_idx, size=int(len(benign_idx) * 0.3), replace=False)\n",
    "\n",
    "imbalanced_idx = np.concatenate([malignant_idx, benign_sample_idx])\n",
    "X_train_imb = X_train_scaled[imbalanced_idx]\n",
    "y_train_imb = y_train[imbalanced_idx]\n",
    "\n",
    "print(\"Imbalanced Dataset Class Distribution:\")\n",
    "print(pd.Series(y_train_imb).value_counts())\n",
    "print(\"\\nClass Balance:\")\n",
    "print(pd.Series(y_train_imb).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: No handling (baseline)\n",
    "model_no_handling = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_no_handling.fit(X_train_imb, y_train_imb)\n",
    "y_pred_no = model_no_handling.predict(X_test_scaled)\n",
    "\n",
    "# Strategy 2: Class weights\n",
    "model_weights = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "model_weights.fit(X_train_imb, y_train_imb)\n",
    "y_pred_weights = model_weights.predict(X_test_scaled)\n",
    "\n",
    "# Strategy 3: SMOTE (Oversampling)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_imb, y_train_imb)\n",
    "model_smote = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = model_smote.predict(X_test_scaled)\n",
    "\n",
    "# Strategy 4: Undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train_imb, y_train_imb)\n",
    "model_under = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_under.fit(X_train_under, y_train_under)\n",
    "y_pred_under = model_under.predict(X_test_scaled)\n",
    "\n",
    "# Compare strategies\n",
    "imbalance_results = [\n",
    "    {'Strategy': 'No Handling', 'Accuracy': accuracy_score(y_test, y_pred_no),\n",
    "     'Precision': precision_score(y_test, y_pred_no), 'Recall': recall_score(y_test, y_pred_no),\n",
    "     'F1-Score': f1_score(y_test, y_pred_no)},\n",
    "    {'Strategy': 'Class Weights', 'Accuracy': accuracy_score(y_test, y_pred_weights),\n",
    "     'Precision': precision_score(y_test, y_pred_weights), 'Recall': recall_score(y_test, y_pred_weights),\n",
    "     'F1-Score': f1_score(y_test, y_pred_weights)},\n",
    "    {'Strategy': 'SMOTE', 'Accuracy': accuracy_score(y_test, y_pred_smote),\n",
    "     'Precision': precision_score(y_test, y_pred_smote), 'Recall': recall_score(y_test, y_pred_smote),\n",
    "     'F1-Score': f1_score(y_test, y_pred_smote)},\n",
    "    {'Strategy': 'Undersampling', 'Accuracy': accuracy_score(y_test, y_pred_under),\n",
    "     'Precision': precision_score(y_test, y_pred_under), 'Recall': recall_score(y_test, y_pred_under),\n",
    "     'F1-Score': f1_score(y_test, y_pred_under)}\n",
    "]\n",
    "\n",
    "imbalance_df = pd.DataFrame(imbalance_results)\n",
    "print(\"\\nClass Imbalance Handling Strategies Comparison:\")\n",
    "print(imbalance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize imbalance handling comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[0].bar(imbalance_df['Strategy'], imbalance_df['F1-Score'])\n",
    "axes[0].set_title('F1-Score by Imbalance Handling Strategy')\n",
    "axes[0].set_xlabel('Strategy')\n",
    "axes[0].set_ylabel('F1-Score')\n",
    "axes[0].set_xticklabels(imbalance_df['Strategy'], rotation=45, ha='right')\n",
    "\n",
    "# Precision-Recall comparison\n",
    "x = np.arange(len(imbalance_df))\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, imbalance_df['Precision'], width, label='Precision')\n",
    "axes[1].bar(x + width/2, imbalance_df['Recall'], width, label='Recall')\n",
    "axes[1].set_title('Precision and Recall by Strategy')\n",
    "axes[1].set_xlabel('Strategy')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(imbalance_df['Strategy'], rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Advanced Metrics for Model Evaluation\n",
    "\n",
    "Beyond basic metrics, we can use:\n",
    "\n",
    "- **ROC Curve**: Receiver Operating Characteristic - plots True Positive Rate vs False Positive Rate\n",
    "- **ROC-AUC Score**: Area Under the ROC Curve (1.0 = perfect, 0.5 = random)\n",
    "- **Precision-Recall Curve**: Especially useful for imbalanced datasets\n",
    "- **Average Precision (AP)**: Area under the Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve and AUC\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calculate Precision-Recall curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, y_pred_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"PR-AUC Score: {pr_auc:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC and Precision-Recall curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[0].set_xlim([0.0, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[1].plot(recall, precision, color='green', lw=2, label=f'PR curve (AP = {avg_precision:.4f})')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend(loc='lower left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this comprehensive lab, we covered:\n",
    "\n",
    "### Part 1: Model Optimization\n",
    "1. **Gradient Descent**: Compared different learning rate schedules and their impact on convergence\n",
    "2. **Regularization**: Explored L1, L2, and ElasticNet for feature selection and preventing overfitting\n",
    "3. **SVMs**: Tested various kernels (linear, RBF, polynomial, sigmoid) for classification\n",
    "4. **Cross-Validation**: Used k-fold CV for robust model evaluation\n",
    "\n",
    "### Part 2: Model Evaluation\n",
    "5. **Metrics**: Calculated accuracy, precision, recall, F1-score, and confusion matrix\n",
    "6. **Hyperparameter Tuning**: Used GridSearchCV to find optimal parameters\n",
    "7. **Threshold Tuning**: Adjusted decision thresholds for different business requirements\n",
    "8. **Class Imbalance**: Compared class weights, SMOTE, and undersampling strategies\n",
    "9. **Advanced Metrics**: Analyzed ROC curves, PR curves, and AUC scores\n",
    "\n",
    "### Best Practices\n",
    "- Always standardize features for linear models\n",
    "- Use cross-validation for reliable performance estimates\n",
    "- Consider the business context when choosing metrics and thresholds\n",
    "- Handle class imbalance appropriately for your use case\n",
    "- Visualize results to gain insights\n",
    "\n",
    "### Real-World Application\n",
    "The Breast Cancer Wisconsin dataset demonstrates real medical diagnosis challenges where:\n",
    "- False negatives (missing cancer) are more costly than false positives\n",
    "- High recall is often prioritized even at the cost of precision\n",
    "- ROC-AUC and PR-AUC provide better evaluation than accuracy alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model recommendation based on all analyses\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBest Model: {grid_search.best_estimator_}\")\n",
    "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "y_final_pred = best_model.predict(X_test_scaled)\n",
    "y_final_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_final_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_final_pred):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_final_pred):.4f}\")\n",
    "print(f\"  F1-Score:  {f1_score(y_test, y_final_pred):.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc_score(y_test, y_final_proba):.4f}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"This model balances high accuracy with good recall for medical diagnosis.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
