{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Methods in Machine Learning\n",
    "\n",
    "This hands-on lab demonstrates core concepts in probabilistic machine learning:\n",
    "\n",
    "1. **Bayesian Inference Basics** - Understanding prior beliefs, likelihood, and posterior distributions\n",
    "2. **Overview of Probabilistic Graphical Models** - Introduction to Bayesian Networks and their applications\n",
    "\n",
    "We'll use real-world datasets to illustrate these concepts and demonstrate practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries for our probabilistic methods demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\u2713 Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Bayesian Inference Basics\n",
    "\n",
    "### What is Bayesian Inference?\n",
    "\n",
    "Bayesian inference is a method of statistical inference where we update our beliefs about parameters based on observed data. The core principle is **Bayes' Theorem**:\n",
    "\n",
    "$$P(\\theta|D) = \\frac{P(D|\\theta) \\times P(\\theta)}{P(D)}$$\n",
    "\n",
    "Where:\n",
    "- **P(\u03b8|D)** is the **posterior**: our updated belief about \u03b8 after seeing data D\n",
    "- **P(D|\u03b8)** is the **likelihood**: probability of observing data D given parameter \u03b8\n",
    "- **P(\u03b8)** is the **prior**: our initial belief about \u03b8 before seeing data\n",
    "- **P(D)** is the **evidence**: probability of observing the data (normalizing constant)\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Prior Distribution**: Represents our initial beliefs before observing data\n",
    "2. **Likelihood**: How probable the observed data is for different parameter values\n",
    "3. **Posterior Distribution**: Our updated beliefs after incorporating the data\n",
    "4. **Credible Intervals**: Bayesian analog of confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1: Coin Flip Inference\n",
    "\n",
    "**Scenario**: We want to determine if a coin is fair. We flip it 100 times and observe 60 heads.\n",
    "\n",
    "**Question**: What is the probability that this coin shows heads?\n",
    "\n",
    "We'll use Bayesian inference with a **Beta distribution** as our prior (conjugate prior for Binomial likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_coin_flip(n_flips, n_heads, prior_alpha=1, prior_beta=1):\n",
    "    \"\"\"\n",
    "    Perform Bayesian inference for coin flip probability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_flips : int - Number of coin flips\n",
    "    n_heads : int - Number of heads observed\n",
    "    prior_alpha, prior_beta : int - Beta distribution parameters for prior\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Posterior statistics and visualizations\n",
    "    \"\"\"\n",
    "    n_tails = n_flips - n_heads\n",
    "    \n",
    "    # Posterior parameters (Beta distribution)\n",
    "    post_alpha = prior_alpha + n_heads\n",
    "    post_beta = prior_beta + n_tails\n",
    "    \n",
    "    # Posterior statistics\n",
    "    post_mean = post_alpha / (post_alpha + post_beta)\n",
    "    post_mode = (post_alpha - 1) / (post_alpha + post_beta - 2) if post_alpha > 1 and post_beta > 1 else None\n",
    "    post_std = np.sqrt((post_alpha * post_beta) / \n",
    "                       ((post_alpha + post_beta)**2 * (post_alpha + post_beta + 1)))\n",
    "    \n",
    "    # 95% Credible interval\n",
    "    credible_interval = stats.beta.interval(0.95, post_alpha, post_beta)\n",
    "    \n",
    "    return {\n",
    "        'posterior_mean': post_mean,\n",
    "        'posterior_mode': post_mode,\n",
    "        'posterior_std': post_std,\n",
    "        'credible_interval': credible_interval,\n",
    "        'post_alpha': post_alpha,\n",
    "        'post_beta': post_beta,\n",
    "        'prior_alpha': prior_alpha,\n",
    "        'prior_beta': prior_beta\n",
    "    }\n",
    "\n",
    "# Perform inference\n",
    "n_flips = 100\n",
    "n_heads = 60\n",
    "result = bayesian_coin_flip(n_flips, n_heads)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN COIN FLIP INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nObservations: {n_heads} heads out of {n_flips} flips\")\n",
    "print(f\"\\nPosterior Distribution: Beta({result['post_alpha']}, {result['post_beta']})\")\n",
    "print(f\"  Mean: {result['posterior_mean']:.4f}\")\n",
    "print(f\"  Std: {result['posterior_std']:.4f}\")\n",
    "print(f\"  95% Credible Interval: [{result['credible_interval'][0]:.4f}, {result['credible_interval'][1]:.4f}]\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  We are 95% confident that the true probability of heads\")\n",
    "print(f\"  lies between {result['credible_interval'][0]:.2%} and {result['credible_interval'][1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Prior, Likelihood, and Posterior\n",
    "theta_values = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Prior distribution\n",
    "prior = stats.beta.pdf(theta_values, result['prior_alpha'], result['prior_beta'])\n",
    "\n",
    "# Likelihood (proportional to binomial)\n",
    "likelihood = stats.binom.pmf(n_heads, n_flips, theta_values)\n",
    "likelihood = likelihood / likelihood.max()  # Normalize for visualization\n",
    "\n",
    "# Posterior distribution\n",
    "posterior = stats.beta.pdf(theta_values, result['post_alpha'], result['post_beta'])\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Prior\n",
    "axes[0].plot(theta_values, prior, 'b-', linewidth=2)\n",
    "axes[0].fill_between(theta_values, prior, alpha=0.3)\n",
    "axes[0].set_title('Prior Distribution\\nBeta(1, 1) - Uniform', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Probability of Heads (\u03b8)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Likelihood\n",
    "axes[1].plot(theta_values, likelihood, 'g-', linewidth=2)\n",
    "axes[1].fill_between(theta_values, likelihood, alpha=0.3, color='green')\n",
    "axes[1].set_title(f'Likelihood\\n{n_heads} heads in {n_flips} flips', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Probability of Heads (\u03b8)')\n",
    "axes[1].set_ylabel('Normalized Likelihood')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Posterior\n",
    "axes[2].plot(theta_values, posterior, 'r-', linewidth=2)\n",
    "axes[2].fill_between(theta_values, posterior, alpha=0.3, color='red')\n",
    "axes[2].axvline(result['posterior_mean'], color='darkred', linestyle='--', \n",
    "                linewidth=2, label=f\"Mean: {result['posterior_mean']:.3f}\")\n",
    "axes[2].axvline(result['credible_interval'][0], color='orange', linestyle=':', \n",
    "                linewidth=1.5, label='95% CI')\n",
    "axes[2].axvline(result['credible_interval'][1], color='orange', linestyle=':', linewidth=1.5)\n",
    "axes[2].set_title(f'Posterior Distribution\\nBeta({result[\"post_alpha\"]}, {result[\"post_beta\"]})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Probability of Heads (\u03b8)')\n",
    "axes[2].set_ylabel('Density')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca The posterior combines the prior belief with the observed data via the likelihood.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2: Bayesian Updating - Sequential Learning\n",
    "\n",
    "One powerful feature of Bayesian inference is **sequential updating**. As we collect more data, we can continuously update our beliefs.\n",
    "\n",
    "Let's see how our posterior changes as we observe more coin flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential updating with different amounts of data\n",
    "observations = [10, 20, 50, 100, 200]\n",
    "heads_ratio = 0.6  # 60% heads\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "theta_vals = np.linspace(0, 1, 1000)\n",
    "\n",
    "for idx, n_obs in enumerate(observations):\n",
    "    n_h = int(n_obs * heads_ratio)\n",
    "    result_seq = bayesian_coin_flip(n_obs, n_h)\n",
    "    \n",
    "    # Calculate posterior\n",
    "    post = stats.beta.pdf(theta_vals, result_seq['post_alpha'], result_seq['post_beta'])\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].plot(theta_vals, post, 'b-', linewidth=2)\n",
    "    axes[idx].fill_between(theta_vals, post, alpha=0.3)\n",
    "    axes[idx].axvline(result_seq['posterior_mean'], color='red', linestyle='--', \n",
    "                      linewidth=2, label=f\"Mean: {result_seq['posterior_mean']:.3f}\")\n",
    "    axes[idx].axvline(result_seq['credible_interval'][0], color='orange', \n",
    "                      linestyle=':', linewidth=1.5)\n",
    "    axes[idx].axvline(result_seq['credible_interval'][1], color='orange', \n",
    "                      linestyle=':', linewidth=1.5)\n",
    "    axes[idx].set_title(f'After {n_obs} flips ({n_h} heads)\\nCI width: {result_seq[\"credible_interval\"][1] - result_seq[\"credible_interval\"][0]:.3f}', \n",
    "                       fontsize=11)\n",
    "    axes[idx].set_xlabel('Probability of Heads (\u03b8)')\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].legend(fontsize=9)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim([0, None])\n",
    "\n",
    "# Remove the extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Sequential Bayesian Updating - Posterior Becomes More Certain with More Data', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udd0d Key Observation: As we collect more data, the posterior distribution becomes\")\n",
    "print(\"   narrower (more certain) and concentrates around the true parameter value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.3: Bayesian Classification with Real Data\n",
    "\n",
    "Now let's apply Bayesian methods to a real-world dataset: **Breast Cancer Wisconsin Dataset**.\n",
    "\n",
    "We'll use a **Naive Bayes classifier**, which applies Bayes' theorem with the \"naive\" assumption that features are conditionally independent given the class.\n",
    "\n",
    "**Dataset**: The breast cancer dataset contains features computed from digitized images of breast mass, and the task is to classify tumors as malignant or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "feature_names = cancer_data.feature_names\n",
    "target_names = cancer_data.target_names\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BREAST CANCER DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"\\nTarget classes: {target_names}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  {target_names[0]}: {np.sum(y == 0)} ({np.sum(y == 0)/len(y)*100:.1f}%)\")\n",
    "print(f\"  {target_names[1]}: {np.sum(y == 1)} ({np.sum(y == 1)/len(y)*100:.1f}%)\")\n",
    "print(f\"\\nFirst 5 features: {feature_names[:5].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_model.predict(X_test)\n",
    "y_pred_proba = nb_model.predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with probability\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "axes[0].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Prediction probabilities distribution\n",
    "benign_proba = y_pred_proba[y_test == 1, 1]  # Probability of benign for benign samples\n",
    "malignant_proba = y_pred_proba[y_test == 0, 1]  # Probability of benign for malignant samples\n",
    "\n",
    "axes[1].hist(benign_proba, bins=30, alpha=0.6, label='Benign (True)', color='green', edgecolor='black')\n",
    "axes[1].hist(malignant_proba, bins=30, alpha=0.6, label='Malignant (True)', color='red', edgecolor='black')\n",
    "axes[1].axvline(0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "axes[1].set_xlabel('Predicted Probability of Benign')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Prediction Probability Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 The Naive Bayes classifier provides probability estimates, which represent\")\n",
    "print(\"   the model's confidence in its predictions. This is valuable for decision-making!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Overview of Probabilistic Graphical Models (PGMs)\n",
    "\n",
    "### What are Probabilistic Graphical Models?\n",
    "\n",
    "Probabilistic Graphical Models (PGMs) are a powerful framework for representing and reasoning about complex probability distributions using graphs.\n",
    "\n",
    "**Key Components:**\n",
    "- **Nodes**: Represent random variables\n",
    "- **Edges**: Represent probabilistic relationships (dependencies) between variables\n",
    "\n",
    "### Types of PGMs:\n",
    "\n",
    "1. **Bayesian Networks (Directed Graphs)**:\n",
    "   - Nodes represent random variables\n",
    "   - Directed edges represent conditional dependencies\n",
    "   - Each node has a Conditional Probability Distribution (CPD)\n",
    "\n",
    "2. **Markov Random Fields (Undirected Graphs)**:\n",
    "   - Edges represent symmetric relationships\n",
    "   - Used when directionality doesn't matter\n",
    "\n",
    "### Why Use PGMs?\n",
    "- **Compact Representation**: Express complex joint distributions efficiently\n",
    "- **Modular**: Break down complex problems into manageable pieces\n",
    "- **Inference**: Answer probabilistic queries (e.g., \"What is P(Disease | Symptoms)?\")\n",
    "- **Learning**: Discover structure and parameters from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.1: Medical Diagnosis - A Simple Bayesian Network\n",
    "\n",
    "Let's create a simple Bayesian Network for medical diagnosis:\n",
    "\n",
    "```\n",
    "      Disease\n",
    "       /  \\\\\n",
    "      /    \\\\\n",
    "  Symptom1  Symptom2\n",
    "```\n",
    "\n",
    "- **Disease**: Binary (has disease or not)\n",
    "- **Symptom1**: Binary (fever present or not)\n",
    "- **Symptom2**: Binary (cough present or not)\n",
    "\n",
    "We'll define the conditional probability tables (CPTs) and perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Bayesian Network manually\n",
    "# Prior probability of disease\n",
    "P_disease = 0.01  # 1% of population has the disease\n",
    "\n",
    "# Conditional probabilities of symptoms given disease status\n",
    "# P(Fever | Disease)\n",
    "P_fever_given_disease = 0.9\n",
    "P_fever_given_no_disease = 0.1\n",
    "\n",
    "# P(Cough | Disease)\n",
    "P_cough_given_disease = 0.8\n",
    "P_cough_given_no_disease = 0.2\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN NETWORK: MEDICAL DIAGNOSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nPrior Probabilities:\")\n",
    "print(f\"  P(Disease = Yes) = {P_disease}\")\n",
    "print(f\"  P(Disease = No) = {1 - P_disease}\")\n",
    "print(\"\\nConditional Probabilities:\")\n",
    "print(f\"  P(Fever = Yes | Disease = Yes) = {P_fever_given_disease}\")\n",
    "print(f\"  P(Fever = Yes | Disease = No) = {P_fever_given_no_disease}\")\n",
    "print(f\"  P(Cough = Yes | Disease = Yes) = {P_cough_given_disease}\")\n",
    "print(f\"  P(Cough = Yes | Disease = No) = {P_cough_given_no_disease}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_network_inference(has_fever, has_cough):\n",
    "    \"\"\"\n",
    "    Compute P(Disease | Symptoms) using Bayes' theorem.\n",
    "    \n",
    "    This demonstrates inference in a simple Bayesian Network.\n",
    "    \"\"\"\n",
    "    # Calculate P(Symptoms | Disease)\n",
    "    if has_fever and has_cough:\n",
    "        P_symptoms_given_disease = P_fever_given_disease * P_cough_given_disease\n",
    "        P_symptoms_given_no_disease = P_fever_given_no_disease * P_cough_given_no_disease\n",
    "    elif has_fever and not has_cough:\n",
    "        P_symptoms_given_disease = P_fever_given_disease * (1 - P_cough_given_disease)\n",
    "        P_symptoms_given_no_disease = P_fever_given_no_disease * (1 - P_cough_given_no_disease)\n",
    "    elif not has_fever and has_cough:\n",
    "        P_symptoms_given_disease = (1 - P_fever_given_disease) * P_cough_given_disease\n",
    "        P_symptoms_given_no_disease = (1 - P_fever_given_no_disease) * P_cough_given_no_disease\n",
    "    else:\n",
    "        P_symptoms_given_disease = (1 - P_fever_given_disease) * (1 - P_cough_given_disease)\n",
    "        P_symptoms_given_no_disease = (1 - P_fever_given_no_disease) * (1 - P_cough_given_no_disease)\n",
    "    \n",
    "    # Calculate P(Symptoms) - Evidence\n",
    "    P_symptoms = (P_symptoms_given_disease * P_disease + \n",
    "                  P_symptoms_given_no_disease * (1 - P_disease))\n",
    "    \n",
    "    # Apply Bayes' Theorem: P(Disease | Symptoms)\n",
    "    P_disease_given_symptoms = (P_symptoms_given_disease * P_disease) / P_symptoms\n",
    "    \n",
    "    return P_disease_given_symptoms\n",
    "\n",
    "# Test different symptom combinations\n",
    "scenarios = [\n",
    "    (False, False, \"No symptoms\"),\n",
    "    (True, False, \"Fever only\"),\n",
    "    (False, True, \"Cough only\"),\n",
    "    (True, True, \"Both fever and cough\")\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INFERENCE: P(Disease | Symptoms)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "for fever, cough, description in scenarios:\n",
    "    prob = bayesian_network_inference(fever, cough)\n",
    "    results.append(prob)\n",
    "    print(f\"\\n{description}:\")\n",
    "    print(f\"  P(Disease | Symptoms) = {prob:.4f} ({prob*100:.2f}%)\")\n",
    "    print(f\"  Interpretation: {prob/P_disease:.1f}x more likely than prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the inference results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot of probabilities\n",
    "scenario_names = [s[2] for s in scenarios]\n",
    "colors = ['green' if r < 0.1 else 'orange' if r < 0.5 else 'red' for r in results]\n",
    "\n",
    "bars = ax1.bar(scenario_names, results, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.axhline(y=P_disease, color='blue', linestyle='--', linewidth=2, label=f'Prior: {P_disease}')\n",
    "ax1.set_ylabel('P(Disease | Symptoms)', fontsize=11)\n",
    "ax1.set_title('Posterior Probability of Disease\\nGiven Different Symptom Combinations', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.set_ylim([0, max(results) * 1.2])\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, results):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.3f}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Heatmap showing the network structure effect\n",
    "fever_vals = [0, 1]\n",
    "cough_vals = [0, 1]\n",
    "prob_matrix = np.zeros((2, 2))\n",
    "\n",
    "for i, fever in enumerate(fever_vals):\n",
    "    for j, cough in enumerate(cough_vals):\n",
    "        prob_matrix[i, j] = bayesian_network_inference(bool(fever), bool(cough))\n",
    "\n",
    "sns.heatmap(prob_matrix, annot=True, fmt='.4f', cmap='YlOrRd', ax=ax2,\n",
    "            xticklabels=['No Cough', 'Cough'], yticklabels=['No Fever', 'Fever'],\n",
    "            cbar_kws={'label': 'P(Disease | Symptoms)'})\n",
    "ax2.set_title('Probability Heatmap:\\nJoint Effect of Symptoms', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Cough Status')\n",
    "ax2.set_ylabel('Fever Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udd2c Key Insight: Having BOTH symptoms dramatically increases the probability\")\n",
    "print(\"   of disease compared to the prior or having just one symptom.\")\n",
    "print(\"   This demonstrates how Bayesian Networks combine evidence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.2: Bayesian Network Application - Iris Classification\n",
    "\n",
    "Let's apply PGM concepts to the famous **Iris dataset**. We'll use a Naive Bayes classifier, which is actually a simple Bayesian Network where:\n",
    "\n",
    "```\n",
    "                Species (Class)\n",
    "                /   |   |   \\\\\n",
    "               /    |   |    \\\\\n",
    "    Sepal_Length  Sepal_Width  Petal_Length  Petal_Width\n",
    "```\n",
    "\n",
    "The \"naive\" assumption is that all features are conditionally independent given the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names_iris = iris.feature_names\n",
    "target_names_iris = iris.target_names\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IRIS DATASET - BAYESIAN NETWORK APPLICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset shape: {X_iris.shape}\")\n",
    "print(f\"Features: {feature_names_iris}\")\n",
    "print(f\"Classes: {target_names_iris.tolist()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for i, name in enumerate(target_names_iris):\n",
    "    count = np.sum(y_iris == i)\n",
    "    print(f\"  {name}: {count} samples ({count/len(y_iris)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and train\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Train Naive Bayes (Gaussian) - This is a Bayesian Network!\n",
    "nb_iris = GaussianNB()\n",
    "nb_iris.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "# Predictions\n",
    "y_pred_iris = nb_iris.predict(X_test_iris)\n",
    "y_pred_proba_iris = nb_iris.predict_proba(X_test_iris)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_iris = accuracy_score(y_test_iris, y_pred_iris)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy_iris:.4f} ({accuracy_iris*100:.2f}%)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test_iris, y_pred_iris, target_names=target_names_iris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned parameters (means and variances)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot the learned Gaussian distributions for each feature\n",
    "for feature_idx in range(4):\n",
    "    ax = axes[feature_idx]\n",
    "    \n",
    "    # Get feature values range\n",
    "    feature_range = np.linspace(X_iris[:, feature_idx].min(), \n",
    "                                X_iris[:, feature_idx].max(), 200)\n",
    "    \n",
    "    # Plot learned distributions for each class\n",
    "    for class_idx in range(3):\n",
    "        mean = nb_iris.theta_[class_idx, feature_idx]\n",
    "        var = nb_iris.var_[class_idx, feature_idx]\n",
    "        \n",
    "        # Calculate Gaussian PDF\n",
    "        pdf = stats.norm.pdf(feature_range, mean, np.sqrt(var))\n",
    "        \n",
    "        ax.plot(feature_range, pdf, linewidth=2, \n",
    "                label=f'{target_names_iris[class_idx]}')\n",
    "    \n",
    "    ax.set_xlabel(feature_names_iris[feature_idx], fontsize=10)\n",
    "    ax.set_ylabel('Probability Density', fontsize=10)\n",
    "    ax.set_title(f'Learned Distribution: {feature_names_iris[feature_idx]}', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Gaussian Naive Bayes: Learned Feature Distributions per Class', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 These plots show the learned conditional distributions P(Feature | Class)\")\n",
    "print(\"   The model learns the mean and variance of each feature for each class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction probabilities\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_iris = confusion_matrix(y_test_iris, y_pred_iris)\n",
    "sns.heatmap(cm_iris, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=target_names_iris, yticklabels=target_names_iris)\n",
    "axes[0].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Probability distribution for predictions\n",
    "# Show max probability for each prediction\n",
    "max_probs = y_pred_proba_iris.max(axis=1)\n",
    "correct_predictions = y_pred_iris == y_test_iris\n",
    "\n",
    "axes[1].hist(max_probs[correct_predictions], bins=20, alpha=0.6, \n",
    "             label='Correct Predictions', color='green', edgecolor='black')\n",
    "axes[1].hist(max_probs[~correct_predictions], bins=20, alpha=0.6, \n",
    "             label='Incorrect Predictions', color='red', edgecolor='black')\n",
    "axes[1].set_xlabel('Maximum Predicted Probability', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].set_title('Prediction Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\u2705 Correct predictions tend to have higher probabilities (higher confidence)\")\n",
    "print(f\"\u274c Incorrect predictions often have lower probabilities (lower confidence)\")\n",
    "print(f\"\\nThis uncertainty quantification is a key advantage of probabilistic models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### Bayesian Inference Basics:\n",
    "\n",
    "1. **Bayes' Theorem** provides a principled way to update beliefs based on evidence\n",
    "2. **Prior + Likelihood \u2192 Posterior**: We combine prior knowledge with observed data\n",
    "3. **Sequential Updating**: Posteriors become more confident with more data\n",
    "4. **Credible Intervals**: Provide probabilistic statements about parameters (95% CI means \"95% probability the parameter is in this range\")\n",
    "5. **Practical Applications**: Medical diagnosis, A/B testing, classification, etc.\n",
    "\n",
    "### Probabilistic Graphical Models:\n",
    "\n",
    "1. **PGMs** represent complex probability distributions using graphs\n",
    "2. **Nodes** = Random variables, **Edges** = Dependencies\n",
    "3. **Bayesian Networks** use directed graphs to encode conditional dependencies\n",
    "4. **Inference** allows us to answer probabilistic queries given evidence\n",
    "5. **Naive Bayes** is a simple but powerful Bayesian Network for classification\n",
    "6. **Real-world Applications**: \n",
    "   - Medical diagnosis (combining symptoms)\n",
    "   - Spam filtering\n",
    "   - Recommendation systems\n",
    "   - Risk assessment\n",
    "\n",
    "### Advantages of Probabilistic Methods:\n",
    "\n",
    "\u2705 **Uncertainty Quantification**: Provides confidence in predictions  \n",
    "\u2705 **Principled Framework**: Based on probability theory  \n",
    "\u2705 **Incorporates Prior Knowledge**: Can use domain expertise  \n",
    "\u2705 **Interpretable**: Clear probabilistic interpretation  \n",
    "\u2705 **Handles Missing Data**: Natural framework for incomplete information  \n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore more complex PGMs (Hidden Markov Models, Conditional Random Fields)\n",
    "- Study Markov Chain Monte Carlo (MCMC) for complex inference\n",
    "- Learn about variational inference for scalable Bayesian methods\n",
    "- Apply to real-world problems in your domain\n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "- \"Probabilistic Graphical Models\" by Daphne Koller and Nir Friedman\n",
    "- \"Bayesian Data Analysis\" by Andrew Gelman et al.\n",
    "- PyMC3 documentation for practical Bayesian modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises (Optional)\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Different Priors\n",
    "Modify the coin flip example to use different priors:\n",
    "- Informative prior: Beta(5, 5) (belief that coin is fair)\n",
    "- Skeptical prior: Beta(10, 2) (belief that coin favors heads)\n",
    "\n",
    "How does the choice of prior affect the posterior?\n",
    "\n",
    "### Exercise 2: Expand the Medical Diagnosis Network\n",
    "Add a third symptom (e.g., headache) to the medical diagnosis Bayesian Network and perform inference.\n",
    "\n",
    "### Exercise 3: Apply to Your Own Data\n",
    "Choose a dataset relevant to your field and apply Naive Bayes classification. Analyze:\n",
    "- Which features are most discriminative?\n",
    "- How confident is the model in its predictions?\n",
    "- What are common misclassifications?\n",
    "\n",
    "### Exercise 4: A/B Testing\n",
    "Implement Bayesian A/B testing for a scenario where:\n",
    "- Variant A: 50 conversions out of 500 visitors\n",
    "- Variant B: 65 conversions out of 500 visitors\n",
    "\n",
    "What's the probability that B is better than A?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}