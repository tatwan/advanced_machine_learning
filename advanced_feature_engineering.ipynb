{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering\n",
    "\n",
    "This notebook demonstrates advanced techniques in feature engineering for machine learning projects. We'll cover:\n",
    "\n",
    "1. **Dealing with Dirty Data** - Handling inconsistencies, outliers, and data quality issues\n",
    "2. **Feature Value Imputation** - Techniques to fill missing values\n",
    "3. **Engineered Features** - Creating new features from existing ones\n",
    "4. **Denoising With ML Models** - Using machine learning to clean and denoise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dealing with Dirty Data\n",
    "\n",
    "Real-world data is often messy. Let's explore common issues and how to handle them:\n",
    "\n",
    "- Inconsistent formatting\n",
    "- Duplicate records\n",
    "- Outliers\n",
    "- Invalid values\n",
    "- Data type mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dirty dataset\n",
    "dirty_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'age': [25, 35, 35, -5, 150, 28, 42, 31, 29, 38, 45],\n",
    "    'income': [50000, 75000, 75000, 60000, 1000000, 55000, 80000, 62000, 58000, 70000, 85000],\n",
    "    'email': ['john@email.com', 'JANE@EMAIL.COM', 'jane@email.com', 'bob@email.com', \n",
    "              'alice@email.com', 'charlie@email.com', 'david@email.com', 'eve@email.com',\n",
    "              'frank@email.com', 'grace@email.com', 'henry@email.com'],\n",
    "    'zip_code': ['12345', '23456', '23456', '34567', '45678', '56789', '67890', '78901', '89012', '90123', '01234']\n",
    "})\n",
    "\n",
    "print(\"Original Dirty Data:\")\n",
    "print(dirty_data)\n",
    "print(f\"\\nShape: {dirty_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and remove duplicates\n",
    "print(f\"Duplicate rows: {dirty_data.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates based on customer_id\n",
    "clean_data = dirty_data.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "print(f\"\\nAfter removing duplicates: {clean_data.shape}\")\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Standardizing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize email addresses to lowercase\n",
    "clean_data['email'] = clean_data['email'].str.lower()\n",
    "print(\"Standardized email addresses:\")\n",
    "print(clean_data['email'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Detecting and Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize potential outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].boxplot(clean_data['age'].dropna())\n",
    "axes[0].set_title('Age Distribution')\n",
    "axes[0].set_ylabel('Age')\n",
    "\n",
    "axes[1].boxplot(clean_data['income'].dropna())\n",
    "axes[1].set_title('Income Distribution')\n",
    "axes[1].set_ylabel('Income')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAge statistics:\")\n",
    "print(clean_data['age'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Remove outliers using IQR (Interquartile Range)\n",
    "def remove_outliers_iqr(df, column, multiplier=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Method 2: Cap outliers at percentiles\n",
    "def cap_outliers(df, column, lower_percentile=5, upper_percentile=95):\n",
    "    lower_cap = df[column].quantile(lower_percentile/100)\n",
    "    upper_cap = df[column].quantile(upper_percentile/100)\n",
    "    df[column] = df[column].clip(lower=lower_cap, upper=upper_cap)\n",
    "    return df\n",
    "\n",
    "# Method 3: Isolation Forest for multivariate outliers\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "outlier_predictions = iso_forest.fit_predict(clean_data[['age', 'income']])\n",
    "\n",
    "print(f\"Outliers detected by Isolation Forest: {(outlier_predictions == -1).sum()}\")\n",
    "\n",
    "# Apply domain knowledge to fix obvious errors\n",
    "clean_data.loc[clean_data['age'] < 0, 'age'] = np.nan\n",
    "clean_data.loc[clean_data['age'] > 120, 'age'] = np.nan\n",
    "\n",
    "print(\"\\nData after handling invalid age values:\")\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Value Imputation\n",
    "\n",
    "Missing values are common in real-world datasets. Let's explore various imputation techniques:\n",
    "\n",
    "- Simple imputation (mean, median, mode)\n",
    "- K-Nearest Neighbors imputation\n",
    "- Iterative imputation\n",
    "- Forward fill / backward fill for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with missing values\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "data_with_missing = pd.DataFrame({\n",
    "    'feature_1': np.random.randn(n_samples),\n",
    "    'feature_2': np.random.randn(n_samples) * 10 + 50,\n",
    "    'feature_3': np.random.randint(1, 100, n_samples),\n",
    "    'feature_4': np.random.randn(n_samples) * 5 + 25,\n",
    "    'category': np.random.choice(['A', 'B', 'C'], n_samples)\n",
    "})\n",
    "\n",
    "# Introduce missing values randomly\n",
    "missing_mask = np.random.random(data_with_missing.shape) < 0.15\n",
    "data_with_missing_copy = data_with_missing.copy()\n",
    "data_with_missing_copy = data_with_missing_copy.mask(missing_mask)\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print(data_with_missing_copy.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {data_with_missing_copy.isnull().sum().sum()}\")\n",
    "print(f\"Percentage missing: {data_with_missing_copy.isnull().sum().sum() / data_with_missing_copy.size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simple Imputation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean imputation for numerical features\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "data_mean_imputed = data_with_missing_copy.copy()\n",
    "numeric_cols = data_mean_imputed.select_dtypes(include=[np.number]).columns\n",
    "data_mean_imputed[numeric_cols] = mean_imputer.fit_transform(data_mean_imputed[numeric_cols])\n",
    "\n",
    "print(\"Mean Imputation Results:\")\n",
    "print(f\"Remaining missing values: {data_mean_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# Median imputation (more robust to outliers)\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "data_median_imputed = data_with_missing_copy.copy()\n",
    "data_median_imputed[numeric_cols] = median_imputer.fit_transform(data_median_imputed[numeric_cols])\n",
    "\n",
    "print(\"\\nMedian Imputation Results:\")\n",
    "print(f\"Remaining missing values: {data_median_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# Most frequent (mode) imputation for categorical features\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_mode_imputed = data_with_missing_copy.copy()\n",
    "data_mode_imputed['category'] = mode_imputer.fit_transform(data_mode_imputed[['category']])\n",
    "\n",
    "print(\"\\nMode Imputation for Category:\")\n",
    "print(f\"Remaining missing values in category: {data_mode_imputed['category'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 KNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Imputation - uses k nearest neighbors to estimate missing values\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "data_knn_imputed = data_with_missing_copy.copy()\n",
    "data_knn_imputed[numeric_cols] = knn_imputer.fit_transform(data_knn_imputed[numeric_cols])\n",
    "\n",
    "print(\"KNN Imputation Results:\")\n",
    "print(f\"Remaining missing values: {data_knn_imputed[numeric_cols].isnull().sum().sum()}\")\n",
    "\n",
    "# Compare different imputation methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(data_mean_imputed['feature_2'], bins=20, alpha=0.7, label='Mean Imputed')\n",
    "axes[0].hist(data_with_missing['feature_2'], bins=20, alpha=0.5, label='Original')\n",
    "axes[0].set_title('Mean Imputation')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(data_median_imputed['feature_2'], bins=20, alpha=0.7, label='Median Imputed')\n",
    "axes[1].hist(data_with_missing['feature_2'], bins=20, alpha=0.5, label='Original')\n",
    "axes[1].set_title('Median Imputation')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].hist(data_knn_imputed['feature_2'], bins=20, alpha=0.7, label='KNN Imputed')\n",
    "axes[2].hist(data_with_missing['feature_2'], bins=20, alpha=0.5, label='Original')\n",
    "axes[2].set_title('KNN Imputation')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Advanced Imputation: Using ML Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Random Forest to predict missing values\n",
    "def ml_imputation(df, target_col, feature_cols):\n",
    "    \"\"\"Impute missing values using a Random Forest model\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Split into rows with and without missing values\n",
    "    train_data = df_copy[df_copy[target_col].notna()]\n",
    "    predict_data = df_copy[df_copy[target_col].isna()]\n",
    "    \n",
    "    if len(predict_data) == 0:\n",
    "        return df_copy\n",
    "    \n",
    "    # Train Random Forest on complete cases\n",
    "    X_train = train_data[feature_cols].fillna(train_data[feature_cols].mean())\n",
    "    y_train = train_data[target_col]\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict missing values\n",
    "    X_predict = predict_data[feature_cols].fillna(train_data[feature_cols].mean())\n",
    "    predictions = model.predict(X_predict)\n",
    "    \n",
    "    # Fill in the predictions\n",
    "    df_copy.loc[df_copy[target_col].isna(), target_col] = predictions\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Apply ML imputation to feature_2\n",
    "data_ml_imputed = ml_imputation(\n",
    "    data_with_missing_copy, \n",
    "    target_col='feature_2', \n",
    "    feature_cols=['feature_1', 'feature_3', 'feature_4']\n",
    ")\n",
    "\n",
    "print(\"ML-based Imputation Results:\")\n",
    "print(f\"Remaining missing values in feature_2: {data_ml_imputed['feature_2'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Engineered Features\n",
    "\n",
    "Feature engineering is the process of creating new features from existing ones to improve model performance:\n",
    "\n",
    "- Polynomial features\n",
    "- Interaction features\n",
    "- Aggregation features\n",
    "- Domain-specific features\n",
    "- Time-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset for feature engineering\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "engineering_data = pd.DataFrame({\n",
    "    'temperature': np.random.uniform(15, 35, n),\n",
    "    'humidity': np.random.uniform(30, 90, n),\n",
    "    'wind_speed': np.random.uniform(0, 20, n),\n",
    "    'pressure': np.random.uniform(980, 1040, n),\n",
    "    'hour_of_day': np.random.randint(0, 24, n),\n",
    "    'day_of_week': np.random.randint(0, 7, n)\n",
    "})\n",
    "\n",
    "print(\"Original Dataset:\")\n",
    "print(engineering_data.head(10))\n",
    "print(f\"\\nShape: {engineering_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "numerical_features = ['temperature', 'humidity', 'wind_speed']\n",
    "poly_features = poly.fit_transform(engineering_data[numerical_features])\n",
    "\n",
    "# Get feature names\n",
    "poly_feature_names = poly.get_feature_names_out(numerical_features)\n",
    "\n",
    "print(f\"Original features: {len(numerical_features)}\")\n",
    "print(f\"Polynomial features (degree 2): {len(poly_feature_names)}\")\n",
    "print(f\"\\nNew feature names: {poly_feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features manually\n",
    "engineering_data['temp_humidity_interaction'] = engineering_data['temperature'] * engineering_data['humidity']\n",
    "engineering_data['temp_wind_interaction'] = engineering_data['temperature'] * engineering_data['wind_speed']\n",
    "engineering_data['humidity_wind_interaction'] = engineering_data['humidity'] * engineering_data['wind_speed']\n",
    "\n",
    "print(\"Interaction Features:\")\n",
    "print(engineering_data[['temperature', 'humidity', 'temp_humidity_interaction']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ratio and Difference Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ratio features\n",
    "engineering_data['temp_humidity_ratio'] = engineering_data['temperature'] / (engineering_data['humidity'] + 1e-5)\n",
    "engineering_data['wind_pressure_ratio'] = engineering_data['wind_speed'] / engineering_data['pressure']\n",
    "\n",
    "# Create difference features\n",
    "engineering_data['temp_deviation'] = engineering_data['temperature'] - engineering_data['temperature'].mean()\n",
    "\n",
    "print(\"Ratio and Difference Features:\")\n",
    "print(engineering_data[['temperature', 'humidity', 'temp_humidity_ratio', 'temp_deviation']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cyclical features for time (using sine and cosine transformations)\n",
    "engineering_data['hour_sin'] = np.sin(2 * np.pi * engineering_data['hour_of_day'] / 24)\n",
    "engineering_data['hour_cos'] = np.cos(2 * np.pi * engineering_data['hour_of_day'] / 24)\n",
    "engineering_data['day_sin'] = np.sin(2 * np.pi * engineering_data['day_of_week'] / 7)\n",
    "engineering_data['day_cos'] = np.cos(2 * np.pi * engineering_data['day_of_week'] / 7)\n",
    "\n",
    "# Create time-based categories\n",
    "engineering_data['is_rush_hour'] = engineering_data['hour_of_day'].apply(\n",
    "    lambda x: 1 if (7 <= x <= 9) or (17 <= x <= 19) else 0\n",
    ")\n",
    "engineering_data['is_weekend'] = engineering_data['day_of_week'].apply(\n",
    "    lambda x: 1 if x >= 5 else 0\n",
    ")\n",
    "\n",
    "print(\"Time-Based Features:\")\n",
    "print(engineering_data[['hour_of_day', 'hour_sin', 'hour_cos', 'is_rush_hour']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Aggregation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregation features (rolling statistics)\n",
    "engineering_data_sorted = engineering_data.sort_values('hour_of_day').reset_index(drop=True)\n",
    "\n",
    "# Rolling mean and standard deviation\n",
    "window_size = 5\n",
    "engineering_data_sorted['temp_rolling_mean'] = engineering_data_sorted['temperature'].rolling(\n",
    "    window=window_size, min_periods=1\n",
    ").mean()\n",
    "engineering_data_sorted['temp_rolling_std'] = engineering_data_sorted['temperature'].rolling(\n",
    "    window=window_size, min_periods=1\n",
    ").std()\n",
    "\n",
    "print(\"Aggregation Features:\")\n",
    "print(engineering_data_sorted[['temperature', 'temp_rolling_mean', 'temp_rolling_std']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Binning and Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binned features\n",
    "engineering_data['temp_category'] = pd.cut(\n",
    "    engineering_data['temperature'],\n",
    "    bins=[0, 20, 25, 30, 100],\n",
    "    labels=['Cold', 'Mild', 'Warm', 'Hot']\n",
    ")\n",
    "\n",
    "engineering_data['humidity_category'] = pd.cut(\n",
    "    engineering_data['humidity'],\n",
    "    bins=[0, 40, 60, 80, 100],\n",
    "    labels=['Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "\n",
    "print(\"Binned Features:\")\n",
    "print(engineering_data[['temperature', 'temp_category', 'humidity', 'humidity_category']].head(10))\n",
    "print(f\"\\nFinal dataset shape: {engineering_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Denoising With ML Models\n",
    "\n",
    "Machine learning models can be used to denoise data by:\n",
    "\n",
    "- Autoencoders for dimensionality reduction and denoising\n",
    "- PCA for noise reduction\n",
    "- Ensemble methods for robust predictions\n",
    "- Anomaly detection to identify and remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a noisy dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "n_features = 10\n",
    "\n",
    "# Generate clean signal\n",
    "X_clean = np.random.randn(n_samples, n_features)\n",
    "true_signal = X_clean[:, 0] * 2 + X_clean[:, 1] * 1.5 + X_clean[:, 2] * 0.5\n",
    "\n",
    "# Add noise\n",
    "noise_level = 0.5\n",
    "X_noisy = X_clean + np.random.randn(n_samples, n_features) * noise_level\n",
    "y_noisy = true_signal + np.random.randn(n_samples) * noise_level\n",
    "\n",
    "print(f\"Dataset shape: {X_noisy.shape}\")\n",
    "print(f\"Signal-to-Noise Ratio: {np.var(true_signal) / np.var(y_noisy - true_signal):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 PCA for Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce noise\n",
    "# Scale the data first\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_noisy)\n",
    "\n",
    "# Apply PCA keeping components that explain 95% of variance\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_denoised_pca = pca.inverse_transform(X_pca)\n",
    "X_denoised_pca = scaler.inverse_transform(X_denoised_pca)\n",
    "\n",
    "print(f\"Original features: {n_features}\")\n",
    "print(f\"PCA components: {pca.n_components_}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Visualize explained variance\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Ensemble Methods for Robust Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_noisy, y_noisy, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Random Forest (inherently denoises through ensemble averaging)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate true signal for test set\n",
    "X_test_indices = X_test[:, :3]  # We know the first 3 features are important\n",
    "y_true_signal = X_test_indices[:, 0] * 2 + X_test_indices[:, 1] * 1.5 + X_test_indices[:, 2] * 0.5\n",
    "\n",
    "# Compare performance\n",
    "mse_noisy = mean_squared_error(y_true_signal, y_test)\n",
    "mse_rf = mean_squared_error(y_true_signal, y_pred_rf)\n",
    "\n",
    "print(f\"MSE (noisy data): {mse_noisy:.4f}\")\n",
    "print(f\"MSE (Random Forest predictions): {mse_rf:.4f}\")\n",
    "print(f\"Improvement: {(1 - mse_rf/mse_noisy) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Anomaly Detection for Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Isolation Forest to detect and remove noisy samples\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "anomaly_labels = iso_forest.fit_predict(X_train)\n",
    "\n",
    "# Remove anomalies\n",
    "X_train_clean = X_train[anomaly_labels == 1]\n",
    "y_train_clean = y_train[anomaly_labels == 1]\n",
    "\n",
    "print(f\"Original training samples: {len(X_train)}\")\n",
    "print(f\"Anomalies detected: {(anomaly_labels == -1).sum()}\")\n",
    "print(f\"Clean training samples: {len(X_train_clean)}\")\n",
    "\n",
    "# Train model on cleaned data\n",
    "rf_clean = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_clean.fit(X_train_clean, y_train_clean)\n",
    "y_pred_clean = rf_clean.predict(X_test)\n",
    "\n",
    "mse_clean = mean_squared_error(y_true_signal, y_pred_clean)\n",
    "print(f\"\\nMSE (after anomaly removal): {mse_clean:.4f}\")\n",
    "print(f\"Additional improvement: {(1 - mse_clean/mse_rf) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize denoising results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Noisy vs True Signal\n",
    "plt.subplot(1, 3, 1)\n",
    "sample_indices = range(50)\n",
    "plt.scatter(sample_indices, y_test[sample_indices], alpha=0.5, label='Noisy Data', s=30)\n",
    "plt.plot(sample_indices, y_true_signal[sample_indices], 'r-', linewidth=2, label='True Signal')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Noisy Data vs True Signal')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Random Forest Predictions\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(sample_indices, y_pred_rf[sample_indices], alpha=0.5, label='RF Predictions', s=30)\n",
    "plt.plot(sample_indices, y_true_signal[sample_indices], 'r-', linewidth=2, label='True Signal')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Random Forest Denoising')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Cleaned Model Predictions\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(sample_indices, y_pred_clean[sample_indices], alpha=0.5, label='Cleaned Predictions', s=30)\n",
    "plt.plot(sample_indices, y_true_signal[sample_indices], 'r-', linewidth=2, label='True Signal')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('After Anomaly Removal')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered advanced feature engineering techniques:\n",
    "\n",
    "1. **Dealing with Dirty Data**: We learned how to handle duplicates, standardize text data, and detect/handle outliers using IQR, capping, and Isolation Forest.\n",
    "\n",
    "2. **Feature Value Imputation**: We explored various imputation strategies including simple imputation (mean, median, mode), KNN imputation, and ML-based imputation using Random Forest.\n",
    "\n",
    "3. **Engineered Features**: We created new features through:\n",
    "   - Polynomial features\n",
    "   - Interaction features\n",
    "   - Ratio and difference features\n",
    "   - Time-based features (cyclical encoding)\n",
    "   - Aggregation features (rolling statistics)\n",
    "   - Binning and discretization\n",
    "\n",
    "4. **Denoising With ML Models**: We demonstrated denoising techniques using:\n",
    "   - PCA for dimensionality reduction and noise filtering\n",
    "   - Ensemble methods (Random Forest) for robust predictions\n",
    "   - Anomaly detection (Isolation Forest) for noise removal\n",
    "\n",
    "These techniques are essential for improving model performance and should be applied based on your specific dataset and problem domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
